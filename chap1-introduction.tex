\begin{quote}
  Testing shows the presence, not the absence of bugs
\end{quote}
\begin{flushright}
  --- Edsger Wybe Dijkstra, 1969~\cite{naur1969software}
\end{flushright}

\begin{quote}
  If you blindly optimize without profiling, you will likely waste your time on the 99\% of code that isn't actually a performance bottleneck and miss the 1\% that is.
\end{quote}
\begin{flushright}
  --- Charles E.~Leiserson~\cite{Profiling2020Leiserson}
\end{flushright}

%\begin{quote}
%  [P]remature optimization is the root of all evil
%\end{quote}
%\begin{flushright}
%  --- Donald E.~Knuth~\cite[p.~671]{KnuthPrematureOptimization}
%\end{flushright}

\chapter{Introduction}\label{ch:intro}

\section{Introduction}\label{sec:intro:intro}




In critical software systems, like cryptography, there are opposing pressures to innovate and to let things be as they are.
Innovation can help create more performant systems with higher mathematical security.
However, if the new code has any bugs, it could leave the system vulnerable to attacks costing billions of dollars.

Testing of various kinds is the de facto standard for finding bugs, but it is both expensive and does not guarantee a lack of bugs.
For example, some bugs in cryptographic code only occur in as few as 19 out of $2^{255}$ cases~\cite{curve25519-donna-commit-correct-bounds}.
This means that if we want to catch this bug in a ``modest'' twenty years of continuous random testing, then we need to have over a thousand times as many computers as there are atoms in the solar system!
This is not an accident.
If our computers become fast enough to complete this testing in reasonable time, then attackers can use these faster computers to launch brute force attacks.
As a result, however fast our computers get, ensuring security will require scaling up the size of the cryptographic problem proportionally, and testing will continue to be inadequate at finding all bugs.
So, in adversarial contexts where even a single vulnerability in the system could cost us enormously, implementing new, innovative algorithms is a slow and risky process.


% https://www.wolframalpha.com/input/?i=%282%5E255%2F19+*+529812+%2F+%285+ghz%29+%2F+%2820+years%29+%2F+%28atoms+in+the+solar+system%29%29+*+1+atom = 533260
% (2^255/19 * 529812 / (5 ghz) / (20 years) / (atoms in the solar system)) * 1 atom

An appealing solution to this problem is to \emph{prove} critical software correct.
We do this by specifying in formal mathematics the intended behavior of our software, and showing a correspondence between our math and our code.
Ideally, the specification is relatively simple, and easier to trust than the millions of lines of code.

While proofs of algorithms tend to be done with pen-and-paper (consider the ubiquitous proofs that various sorting algorithms are correct found in introductory algorithms classes), proofs of actual code are much harder.
Code in the real-world tends to be much more complicated than code in an algorithms textbook.
This difference could be due to reasons of performance, historical accident, or interfacing with real users, among others.
Proofs of code-correctness thus tend to be filled with tedious case-analysis with only sparse mathematical insights, and attempts to create and check these proofs by hand are subject to the same issues of human fallibility as writing the programs in the first place.

Enter formal verification via machine-checked proofs, a partial solution to the problem of human fallibility.
Foundational tools allow users to write down theorems and proofs that are then checked by machine.
These tools free the proof author from error-prone case analysis, as the computer can check that no cases were missed.

Tools for machine-checked proofs have had many successes, in both software verification and traditional mathematics proof formalization.
Examples abound, from compilers~\cite{Compcert} to microkernels~\cite{seL4SOSP09} to web-servers~\cite{Network2015Chlipala} to cryptography code~\cite{FiatCryptoSP19}, from the Four-Color Theorem~\cite{gonthier2008formal} to the Odd-Order Theorem~\cite{gonthier2013machine} to Homotopy Type Theory~\cite{HoTTBook}.

However, in all examples of software verification successes, there is an enormous overhead in the lines of proof written over the lines of code being verified.
The proof is so exorbitant and arduous that it typically requires multiple PhDs worth of work to verify one piece of software.
\todo{Insert graph}

We may automate the proof generation so we no longer need to replicate proof writing effort for iterations of code with the same (or similar) mathematical specification.
Through this, we are able to decrease the marginal overhead of proof writing, and utilise formal verification to implement innovative code in critical systems.


In this thesis, drawing experience from the generation of verified low-level cryptographic code in the Fiat Cryptography project~\cite{FiatCryptoSP19} as well as auxiliary case studies in category theory~\cite{category-coq-experience} and parsers~\cite{jgross-masters-thesis}, we present difficulties in automation of proof generation.
In the quest to enable automated proof generation, we encountered several performance bottlenecks in proof-checking.
We present in \autoref{part:rewriting} a research prototype of a tool and methodology for achieving acceptable performance at scale in the domain of term transformation and rewriting.
We present design principles in \autoref{part:design} to avoid the performance bottlenecks we encountered, and insights about the proof-checking system that these performance bottlenecks reveal.


Unlike many other performance domains, in our experience, the time it takes to check proofs as we scale the size of the input is almost always super-linear---quadratic at best, commonly cubic or exponential, occasionally even worse.
Empirically, this might look like a proof script that checks in tens of seconds on the smallest of toy examples; takes about a minute on the smallest real-world example, which might be twice the size of the toy example; takes twenty hours to generate and check the proof of an example only twice that size; and, on a (perhaps not-quite-realistic) example twice the size of the twenty-hour example, the script might not finish within a year, or even a thousand years---see \autoref{sec:fiat-crypto-codegen-numbers} for more details.
In just three doublings of input size, we might go from tens of seconds to thousands of years.
Moreover, in proof assistants, this is not an isolated experience.
This sort of performance behavior is \emph{typical}.
\minortodo{Nickolai says: Seems fine ... but is it really unique to proofs?  C++ compilers in the 90s had the same problems.  Jason: Maybe see if I can find references.  Or just ignore?}


The story that this thesis aims to tell is that in order for proof assistants to scale to industrial uses, \emph{we must get the basics of asymptotic performance of proof-checking right}.

While compiler performance---both the time it takes to compile code and the time it takes to run the generated code---has long been an active field of study~\cite{CC++Performance2017,georges2007statistically,mytkowicz-wrong-data}\minortodo{find more citations here}, to our knowledge there is no existing body of work systematically investigating the performance of proof assistants, nor even any work primarily focused on the problem of proof assistant performance.
We distinguish the question of interactive proof assistant performance from that of performance of fully automated reasoning tools such as SAT and SMT solvers, on which there has been a great deal of research~\cite{Efficiency1994Boulton}.
As we discuss in \autoref{sec:intro:history}, interactive proof assistants utilize human creativity to handle a greater breadth and depth of problems than fully automated tools, which succeed only on more restricted domains.

This thesis argues that the problem of \emph{compile-time performance} or \emph{proof-checking performance} is both significantly different from the problem of performance of typical programs and that it is nontrivial.
While many papers mention performance, obliquely or otherwise, and some are even driven by performance concerns of a particular algorithm or part of the system,~\cites[p.~1382]{gonthier2008formal}{Efficiency1994Boulton}{Proving2005Benjamin}{Idris2Faster2020Brady}{Recognizing1989Benanav}{mechanical1990Pierce}{CelikETAL17iCoq}{PalmskogETAL18piCoq}{vmcompute}{thesis-nogin}{Idris2Faster2020Brady}, we have not found any that investigate the performance problems that arise \emph{asymptotically} when proof assistants are used to verify programs at large scale.

Through investigation of this problem, we hope to utilise verification in the software innovation pipeline. We expect that solutions in this domain will enable independence from inadequate testing, and faster iteration and deployment of software.


\subsection{What are proof assistants?}\label{sec:intro:history}
\minortodo{is it Nuprl or NuPRL? make sure to be consistent}
%{The structure of this section:}
%\begin{itemize}
%\item {how the first proof assistants worked}
%\item {subsubsection on LCF approach, and how that lets you prove things}
%\item {paragraph on descendents of LCF, dependently typed and not; foreshadow how proof checking as typechecking works with a single sentence or so}
%\item {very brief paragraph on other proof assistants, also mention survey papers}
%\item {paragraph on successes of proof assistants}
%\end{itemize}
%
Before diving into the details of performance bottlenecks and solutions, we review the history of formal verification and proof assistants to bring the reader up to speed on the context of our work and investigation.





%\todo{Maybe say something about the purpose of this section: simultaneously let people know that there's been a lot of thought and effort, let people know about success stories (because it possibly hasn't touched them, and hasn't been adopted mainstream), (and also a bit about how the proof assistants might actually check the proofs)}
\minortodo{cite the following in chronological order, maybe}
While we intend to cover a wide swath of the history and development in this subsection, more detailed descriptions can be found in the literature~\cites{ringer2020qed,Proof2009Geuvers,History2014Harrison,CoqArtForward2013Huet,Brief2019Darbari,davis2001early,Matuszewski05mizar:the,Automath2002Kamareddine,Milestones2019Moore,Automation2013Moore,LCF2000Gordon,LCF2019Paulson,Logical2002Pfenning}[Related Work]{nuprl}.
\textcite[ch.~4]{ringer2020qed} has a particularly clear presentation which was invaluable in assembling this section.

Formal verification can be traced back to the early 1950s~\cite{Brief2019Darbari}.
The first formally verified proof, in some sense, achieved in 1954, was of the theorem that the sum of two even numbers is even~\cite{davis2001early}.
The ``proof'' was an implementation in a vacuum-tube computer of the algorithm of \textcite{Uber1929Presburger}, which could decide, for any first order formula of natural number arithmetic, whether the formula represented a true theorem or a false one;
by implementing this algorithm and verifying that it returns ``true'' on a formula such as $\forall a\ b\ x\ y,\ \exists z,\ a = x + x \to b = y + y \to a + b = z + z$, the machine can be said to prove that this formula is true.

While complete decision procedures exist for arithmetic, propositional logic (the fragment of logic without quantifiers, i.e., consisting only of $\to$, $\wedge$, $\vee$, $\neg$, and $\iff$), and elementary geometry, there is no complete decision procedure for first-order logic, which allows predicates, as well as universal and existential quantification over objects~\cite{davis2001early}.
In fact, first-order logic is sufficiently expressive to encode systems that reason about themselves, such as Peano arithmetic, and Gödel's incompleteness theorem proves that there must be some statements which are neither provably true nor provably false.
In fact, we cannot even decide which statements are undecidable~\cite{Are2011Makholm}!

This incompleteness, however, does not sink the project of automated proof search.
Consider, for example, the very simple program that merely lists out all possible proofs in a given logical system, halting only when it has found either a proof or a disproof of a given statement.
While this procedure will run forever on statements which are neither provably true nor provably false, it will in fact be able to output proofs for all provable statements.
This procedure, however, is uselessly slow.

More efficient procedures for proof search exist, however.
\minortodo{Should I cite \textcite{Computing1960Davis}, which was an earlier form of the resolution rule that proved infeasible?}
Early systems such as the Stanford Pascal Verifier~\cite{luckham1979stanford} and Stanford Resolution Prover were based on what is now known as Robinson's resolution rule~\cite{Machine1965Robinson}, which, when coupled with syntactic unification, resulted in tolerable performance on sufficiently simple problems~\cite{davis2001early,Brief2019Darbari}.
A particularly clear description of the resolution method can be found in \textcite[pp.~17--18]{Metamathematics1994Shankar}.
In the 1960s, all 400-or-so theorems of Whitehead and Russell's \emph{Principia Mathematica} were automatically proven by the same program~\cite[p.~9]{davis2001early}.
However, as the author himself notes, this was only feasible because all of the theorems could be expressed in a way where all universal quantifiers came first, followed by all existential quantifiers, followed by a formula without any quantifiers.

In the early 1970s, Boyer and Moore began work on theorem provers which could work with higher-order principles such as mathematical induction~\cite[p.~6]{Automation2013Moore}.
This work resulted in a family of theorem provers, collectively known as the Boyer--Moore theorem provers, which includes the first seriously successful automated theorem provers~\cites[p.~8]{Automation2013Moore}{Brief2019Darbari}.
They developed the Edinburgh Pure LISP Theorem Prover, Thm, and later Nqthm~\cites[p.~8]{Automation2013Moore}{Nqthm,wiki:Nqthm}, the last of which came to be known as \emph{the} Boyer--Moore theorem prover.
Nqthm has been used to formalize and verify Gödel's first incompleteness theorem in 1986~\cites{Metamathematics1994Shankar}[p.~29]{Milestones2019Moore}, to verify the implementation of an assembler and linker~\cite{moore2007piton} as well as a number of FORTRAN programs, and to formally prove the invertibility of RSA encryption, the undecidability of the halting problem, Gauss' law of quadratic reciprocity~\cite[pp.~28--29]{Milestones2019Moore}.
Nqthm later evolved into ACL2~\cite{Milestones2019Moore,ACL2Applications,ACL2}, which has been used, among other things, to verify a Motorola digital signal processor, the floating-point arithmetic unit in AMD chips, and some x86 machine code programs~\cite[p.~2]{Milestones2019Moore}.
% More details on the assembler and linker (Piton) at \url{https://www.cs.utexas.edu/~moore/best-ideas/piton/index.html}
% Also Computational Logic Inc.~which used Nqthm, \url{http://www.computationallogic.com/}

In 1967, at around the same time that Robinson published his resolution principle, N.~G.~de Bruijn developed the Automath system~\cite{Automath2002Kamareddine,Survey1994deBruijn,mathematical1970Bruijn,wiki:AutoMath}.
Unlike the Boyer--Moore theorem provers, Automath checked the validity of sequences of human-generated proof steps, and hence was more of a proof checker or proof assistant than an automated theorem prover~\cite{ringer2020qed}.
Automath is notable for being the first system to represent both theorems and proofs in the same formal system, reducing the problem of proof checking to that of type checking~\cite{ringer2020qed} by exploiting what came to be known as the Curry--Howard correspondence~\cite{Automath2002Kamareddine}; we will discuss this more in \autoref{sec:why-how-dependent-types}.
The legacy of Automath also includes de Bruijn indices, a method for encoding function arguments which we describe in \autoref{sec:binders:de-bruijn}, dependent types, which we explain in \autoref{sec:why-how-dependent-types}, and the \emph{de Bruijn principle}---stating that proof checkers should be as small and as simple as possible---which we discuss in \autoref{sec:debruijn-criterion}~\cite{ringer2020qed,Automath2002Kamareddine}.
We are deferring the explanation of these important concepts for the time being because, unlike the methods of theorem proving described above, these methods are at the heart of Coq, the primary theorem prover used in this thesis, as well as proof assistants like it.
One notable accomplishment in the Automath system was the translation and checking of the entirety of Edmund Landau's \emph{Foundations of Analysis} in the early 1970s~\cite{Automath2002Kamareddine}.

Almost at the same time as Boyer and Moore were working on their theorem provers in Edinburgh, Scotland, Milner developed the LCF theorem prover at Stanford in 1972~\cite[p.~1]{LCF2000Gordon}.
Written as an interactive proof checker based on Dana Scott's 1969 logic for computable functions (which LCF abbreviates), LCF was designed to allow users to interactively reason about functional programs~\cite[p.~1]{LCF2000Gordon}.
In 1973, Milner moved to Edinburgh and designed Edinburgh LCF, the successor to Stanford LCF.
This new version of LCF was designed to work around two deficiencies of its predecessor: theorem proving was limited by available memory for storing proof objects, and the fixed set of functions for building proofs could not be easily extended.
The first of these was solved by what is now called ``the LCF approach'': by representing proofs with an abstract \texttt{thm} type, whose API only permitted valid rules of inference, proofs did not have to be carried around in memory~\cites[pp.~1--2]{LCF2000Gordon}{harrison2001lcf}.
In order to support abstract data types, Milner et al.~invented the language ML (short for ``Meta Language'')~\cite[p.~2]{LCF2000Gordon}, the precursor to Caml and later OCaml.
The second issue---ease of extensibility---was also addressed by the design of ML~\cite[p.~2]{LCF2000Gordon}.
By combining an abstract, opaque, trusted API for building terms with a functional programming language, users were granted the ability to combine the basic proof steps into ``tactics''.
Tactics were functions that took in a goal, that is, a formula to be proven, and returned a list of remaining subgoals, together with a function that would take in proofs of those subgoals and turn them into a proof of the overall theorem.
An example: a tactic for proving conjunctions might, upon being asked to prove $A \wedge B$, return the two-element list of subgoals $[A, B]$ together with a function that, when given a proof of $A$ and a proof of $B$ (i.e., when given two \texttt{thm} objects, the first of which proves $A$ and the second of which proves $B$), combines them with a primitive conjunction rule to produce a proof object justifying $A \wedge B$.
\minortodo{Do I need to say more about LCF?}
\minortodo{Do I need to say more about tactics?}

\begin{comment}
\todo{How should I summarize this into relevant paragraphs???}
\begin{quote}
Gérard Huet started working on automatic theorem proving in 1970, using LISP to implement the SAM prover for first-order logic with equality.
At the time, the state of the art was to translate all logical propositions into lists (conjunctions) of lists (disjunctions) of literals (signed atomic formulas), quantification being replaced by Skolem functions.
In this representation deduction was reduced to a principle of pairing complementary atomic formulas modulo instantiation (so-called resolution with principal unifiers).
Equalities gave rise to unidirectional rewritings, again modulo unification.
Rewriting order was determined in an ad hoc way and there was no insurance that the process would converge, or whether it was complete.
Provers were black boxes that generated scores of unreadable logical consequences.
The standard working technique was to enter your conjecture and wait until the computer's memory was full.
Only in exceptionally trivial cases was there an answer worth anything.
This catastrophic situation was not recognized as such, it was understood as a necessary evil, blamed on the incompleteness theorems.
Nevertheless, complexity studies would soon show that even in decidable areas, such as propositional logic, automatic theorem proving was doomed to run into a combinatorial wall.

A decisive breakthrough came in the 1970s with the implementation of a  systematic methodology to use termination orders to guide rewriting, starting from the founding paper of Knuth and Bendix.
The KB software, implemented in 1980 by Jean-Marie Hullot and Gérard Huet, could be used to automate in a natural way decision and semi-decision procedures for algebraic structures.
At the same time, the domain of proofs by induction was also making steady progress, most notably with the NQTHM/ACL of Boyer and Moore.
Another significant step had been the generalization of the resolution technique to higher-order logic, using a unification algorithm for the theory of simple types, designed by Gérard Huet back in 1972.
This algorithm was consistent with a general approach to unification in an equational theory, worked out independently by Gordon Plotkin.

At the same time, logicians (Dana Scott) and theoretical computer scientists (Gordon Plotkin, Gilles Kahn, Gérard Berry) were charting a logical theory of computable functions (computational domains) together with an effectively usable axiomatization (computational induction) to define the semantics of programming languages.
There was hope of using this theory to address rigorously the problem of designing trustworthy software using formal methods.
The validity of a program with respect to its logical specifications could be expressed as a theorem in a mathematical theory that described the data and control structures used by the algorithm.
These ideas were set to work most notably by Robin Milner's team at Edinburgh University, who implemented the LCF system around 1980.
The salient feature of this system was its use of proof tactics that could be programmed in a meta-language (ML).
The formulas were not reduced to undecipherable clauses and users could use their intuition and knowledge of the subject matter to guide the system within proofs that mixed automatic steps (combining predefined and specific \emph{tactics} that users could program in the ML language) and easily understandable manual steps.

Another line of investigation was explored by the philosopher Per Martin-Löf, starting from the constructive foundations of mathematics initially proposed by Brouwer and extended notably by Bishop's development of constructive analysis.
Martin-Löf's Intuitionistic Theory of Types, designed at the beginning of the 1980s, provided an elegant and general framework for the constructive axiomatization of mathematical structures, well suited to serve as a foundation for functional programming.
This direction was seriously pursued by Bob Constable at Cornell University who undertook the implementation of the NuPRL software for the design of software from formal proofs, as well as by the ``Programming methodology'' team headed by Bengt Nordström at Chalmers University in Gothenburg.

All this research relied on the λ-calculus notation, initially designed by the logician Alonzo Church, in its pure version as a language to define recursive functionals, and in its typed version as a higher-order predicate calculus (the theory of simple types, a simpler alternative for meta-mathematics to the system originally used by Whitehead and Russell in \emph{Principia Mathematica}).
Furthermore, the λ-calculus could also be used to represent proofs in a natural deduction format, thus yielding the famous ``Curry-Howard correspondence,'' which expresses an isomorphism between proof structures and functional spaces.
These two aspects of the λ-calculus were actually used in the Automath system for the representation of mathematics, designed by Niklaus de Bruijn in Eindhoven during the 1970s.
In this system, the types of λ-expressions were no longer simple hierarchical layers of functional spaces.
Instead they were actually λ-expressions that could express the dependence of a functional term's result type on the value of its argument---in analogy with the extension of propositional calculus to first-order predicate calculus, where predicates take as arguments terms that represent elements of the carrier domain.

λ-calculus was indeed the main tool in proof theory.
In 1970, Jean-Yves Girard proved the consistency of Analysis through a proof of termination for a polymorphic λ-calculus called system $F$.
This system could be generalized to a calculus $Fω$ with polymorphic functionals, thus making it possible to encode a class of algorithms that transcended the traditional ordinal hierarchies.
The same system was to be rediscovered in 1974 by John Reynolds, as a proposal for a generic programming language that would generalize the restricted form of polymorphism that was present in ML.

In the early 1980s, research was in full swing at the frontier between logic and computer science, in a field that came to be known as Type Theory.
In 1982 Gérard Huet started the Formel project at INRIA's Rocquencourt laboratory, jointly with Guy Cousineau and Pierre-Louis Curien from the computer science laboratory at École Normale Supérieure.
This team set the objective of designing and developing a proof system extending the ideas of the LCF system, in particular by adopting the ML language not only as the meta-language used to define tactics but also as the implementation language of the whole proof system.
This research and development effort on functional programming would lead over the years to the Caml language family and, ultimately, to its latest offspring Objective Caml, still used to this day as the implementation language for the \emph{Coq} proof assistant.

At the international conference on types organized by Gilles Kahn in Sophia Antipolis in 1984, Thierry Coquand and Gérard Huet presented a synthesis of dependent types and polymorphism that made it possible to adapt Martin-Löf's constructive theory to an extension of the Automath system called the \emph{Calculus of Constructions}.
In his doctoral thesis, Thierry Coquand provided a meta-theoretical analysis of the underlying λ-calculus.
By proving the termination of this calculus, he also provided a proof of its logical soundness.
This calculus was adopted as the logical basis for the Formel project's proof system and Gérard Huet proposed a first verifier for this calculus (CoC) using as a virtual machine his \emph{Constructive Engine}.
This verifier made it possible to present a few formal mathematical developments at the Eurocal congress in April 1985.

This was the first stage of what was to become the \emph{Coq} system: a type verifier for λ-expressions that represent either proof terms in a logical system or the definition of mathematical objects.
This proof assistant kernel was completely independent from the proof synthesis tool that was used to construct the terms to be verified---the interpreter for the constructive engine is a deterministic program.
Thierry Coquand implemented a sequent-style proof synthesis algorithm that made it possible to build proof terms by progressive refinement, using a set of tactics that were inspired from the LCF system.
The second stage would soon be completed by Christine Mohring, with the initial implementation of a proof-search algorithm in the style of \emph{Prolog}, the famous \texttt{Auto} tactic.
This was practically the birth of the \emph{Coq} system as we know it today.
In the current version, the kernel still rechecks the proof term that is synthesized by the tactics that are called by the user.
This architecture has the extra advantage of making it possible to simplify the proof-search machinery, which actually ignores some of the constraints imposed by stratification in the type system.

The Formel team soon considered that the Calculus of Constructions could be used to synthesize certified programs, in the spirit of the NuPRL system.
A key point was to take advantage of polymorphism, whose power may be used to express as a type of system $F$ an algebraic structure, such as the integers, making systematic use of a method proposed by Bӧhm and Berarducci.
Christine Mohring concentrated on this issue and implemented a complex tactic to synthesize induction principles in the Calculus of Constructions.
This allowed her to present a method for the formal development of certified algorithms at the conference ``Logic in Computer Science (LICS)'' in June 1986.
However, when completing this study in her doctoral thesis, she realized that the ``impredicative'' encodings she was using did not respect the tradition where the terms of an inductive type are restricted to compositions of the type constructors.
Encodings in the polymorphic λ-calculus introduced parasitic terms and made it impossible to express the appropriate inductive principles.
This partial failure actually gave Christine Mohring and Thierry Coquand the motivation to design in 1988 the ``Calculus of Inductive Constructions,'' an extension of the formalism, endowed with good properties for the axiomatization of algorithms on inductive data structures.

The Formel team was always careful to balance theoretical research and experimentation with models to assert the feasibility of the proposed ideas, prototypes to verify the scalability to real-size proofs, and more complete systems, distributed as free software, with a well-maintained library, documentation, and a conscious effort to ensure the compatibility between successive versions.
The team's in-house prototype CoC became the \emph{Coq} system, made available to a community of users through an electronic forum.
Nevertheless, fundamental issues were not neglected: for instance, Gilles Dowek developed a systematic theory of unification and proof search in Type Theory that was to provide the foundation for future versions of \emph{Coq}.

In 1989, \emph{Coq} version 4.10 was distributed with a first mechanism for extracting functional programs (in Caml syntax) from proofs, as designed by Benjamin Werner.
There was also a set of tactics that provided a certain degree of automatization and a small library of developments about mathematics and computer science---the dawn of a new era.
Thierry Coquand took a teaching position in Gothenburg, Christine Paulin-Mohring joined the École Normale Supérieure in Lyon, and the \emph{Coq} team carried on its research between the two sites of Lyon and Rocquencourt.
At the same time, a new project called Cristal took over the research around functional programming and the ML language.
In Rocquencourt, Chet Murthy, who had just finished his PhD in the NuPRL team on the constructive interpretation of proofs in classical logic, brought his own contribution to the development of a more complex architecture for \emph{Coq} version 5.8.
An international effort was organized within the European funded Basic Research Action ``Logical Frameworks,'' followed three years later by its successor ``Types.''
Several teams were combining their efforts around the design of proof assistants in a stimulating emulation: \emph{Coq} was one of them of course, but so were LEGO, developed by Randy Pollack in Edinburgh, Isabelle, developed by Larry Paulson in Cambridge and later by Tobias Nipkow in Munich, Alf, developed by the Gothenburg team, and so on.

In 1991, \emph{Coq} V5.6 provided a uniform language for describing mathematics (the Gallina ``vernacular''), primitive inductive types, program extraction from proofs, and a graphical user interface.
\emph{Coq} was then an effectively usable system, thus making it possible to start fruitful industrial collaborations, most notably with CNET and Dassault-Aviation.
This first generation of users outside academia was an incentive to develop a tutorial and reference manual, even if the art of \emph{Coq} was still rather mysterious to newcomers.
For \emph{Coq} remained a vehicle for research ideas and a playground for experiments.

In Sophia Antipolis, Yves Bertot reconverted the Centaur effort to provide structure manipulation in an interface CTCoq that supported the interactive construction of proofs using an original methodology of ``proof-by-pointing,'' where the user runs a collection of tactics by invoking relevant ones through mouse clicks.
In Lyon, Catherine Parent showed in her thesis how the problem of extracting programs from proofs could be inverted into the problem of using invariant-decorated programs as skeletons of their own correctness proof.
In Bordeaux, Pierre Castéran showed that this technology could be used to construct certified libraries of algorithms in the continuation semantics style.
Back in Lyon, Eduardo Gimenez showed in his thesis how the framework of inductive types that defined hereditarily finite structures could be extended to a framework of co-inductive types that could be used to axiomatize potentially infinite structures.
As a corollary, he could develop proofs about protocols operating on data streams, thus opening the way to applications in telecommunications.

In Rocquencourt, Samuel Boutin showed in his thesis how to implement reflective reasoning in \emph{Coq}, with a notable application in the automatization of tedious proofs based on algebraic rewriting.
His \emph{Ring} tactic can be used to simplify polynomial expressions and thus to make implicit the usual algebraic manipulations of arithmetic expressions.
Other decision procedures contributed to improving the extent of automatic reasoning in \emph{Coq} significantly: \emph{Omega} in the domain of Presburger arithmetic (Pierre Crégut at CNET-Lannion), \emph{Tauto} and \emph{Intuition} in the propositional domain (César Muñoz in Rocquencourt), \emph{Linear} for the predicate calculus without contraction (Jean-Christophe Filliâtre in Lyon).
Amokrane Saïbi showed that a notion of subtype with inheritance and implicit coercions could be used to develop modular proofs in universal algebra and, most notably, to express elegantly the main notions in category theory.

In November 1996, \emph{Coq} V6.1 was released with all the theoretical advances mentioned above, but also with a number of technical innovations that were crucial for improving its efficiency, notably with the reduction machinery contributed by Bruno Barras, and with advanced tactics for the manipulation of inductive definitions contributed by Christina Cornes.
A proof translator to natural language (English and French) contributed by Yann Coscoy could be used to write in a readable manner the proof terms that had been constructed by the tactics.
This was an important advantage against competitor proof systems that did not construct explicit proofs, since it allowed auditing of the formal certifications.

In the domain of program certification, J.-C.~Filliâtre showed in his thesis in 1999 how to implement proofs on imperative programs in \emph{Coq}.
He proposed to renew the approach based on Floyd--Hoare--Dijkstra assertions on imperative programs, by regarding these programs as notation for the functional expressions obtained through their denotational semantics.
The relevance of \emph{Coq}'s two-level architecture was confirmed by the certification of the CoC verifier that could be extracted from a \emph{Coq} formalization of the meta-theory of the Calculus of Constructions, which was contributed by Bruno Barras--a technical \emph{tour de force} but also quite a leap forward for the safety of formal methods.
Taking his inspiration from Objective Caml's module system, Judicaël Courant outlined the foundations of a modular language for developing mathematics, paving the way for the reuse of libraries and the development of large-scale certified software.

The creation of the company Trusted Logic, specialized in the certification of smart-card-based system using technologies adapted from the Caml and Coq teams, confirmed the relevance of their research.
A variety of applicative projects were started.

The \emph{Coq} system was then completely redesigned, resulting in version 7 based on a functional kernel, the main architects being Jean-Christophe Filliâtre, Hugo Herbelin, and Bruno Barras.
A new language for tactics was designed by David Delahaye, thus providing a high-level language to program complex proof strategies.
Micaela Mayero addressed the axiomatization of real numbers, with the goal of supporting the certification of numerical algorithms.
Meanwhile, Yves Bertot recast the ideas of CtCoq in a sophisticated graphical interface PCoq, developed in Java.

In 2002, four years after Judicaël Courant's thesis, Jacek Chrząszcz managed to integrate a module and functor system analogous to that of Caml.

With its smooth integration in the theory development environment, this extension considerably improved the genericity of libraries.
Pierre Letouzey proposed a new algorithm for the extraction of programs from proofs that took into account the whole \emph{Coq} language, modules included.

On the application side, \emph{Coq} had become robust enough to be usable as a low-level language for specific tools dedicated to program proofs.
This is the case for the CALIFE platform for the modeling and verification of timed automata, the \emph{Why} tool for the proof of imperative programs, or the \emph{Krakatoa} tool for the certification of Java applets, which was developed in the VERIFICARD European project.
These tools use the \emph{Coq} language to establish properties of the models and whenever the proof obligations are too complex for automatic tools.

After a three-year effort, Trusted Logic succeeded in the formal modeling of the whole execution environment for the JavaCard language.
This work on security was awarded the EAL7 certification level (the highest level in the so-called common criteria).
This formal development required 121000 lines of \emph{Coq} development in 278 modules.

\emph{Coq} is also used to develop libraries of advanced mathematical theorems in both constructive and classical form.
The domain of classical mathematics required restrictions to the logical language of \emph{Coq} in order to remain consistent with some of the axioms that are naturally used by mathematicians.
\end{quote}
\begin{flushright}
  --- Gérard Huet and Christine Paulin-Mohring, November 2003~\cite{CoqArtForward2013Huet}
\end{flushright}
\end{comment}

In the mid 1980s, Coq~\cite{Coq}, the proof assistant which we focus most on in this thesis, was born from an integration of features and ideas from a number of the proof assistants we've discussed in this subsection.
Notably, it was based on the Calculus of Constructions (CoC), a synthesis of Martin-Löf's type theory~\cite{Intuitionistic1975MartinLoef,Constructive1982MartinLoef} with dependent types and polymorphism, which grew out of Dana Scott's logic of computable functions~\cite{Type1993Scott} together with de Bruijn's work on Automath~\cite{CoqArtForward2013Huet}.
In the late 1980s, some problems were found with the way datatypes were encoded using functions, which lead to the introduction of inductive types, and an extension of CoC called the Calculus of Inductive Constructions (CIC)~\cite{CoqArtForward2013Huet}.
\textcite{CoqArtForward2013Huet} contains an illuminating and comprehensive discussion of how the threads of Coq's development arose in tandem with the history discussed in the preceding paragraphs, and we highly recommend this read for those interested in Coq's history.

Major accomplishments of verification in Coq include the fully verified optimizing C compiler CompCert~\cite{Compcert}, the proof of the Four Color Theorem~\cite{gonthier2008formal}, and the complete formalization of the Odd Order Theorem, also known as the Feit--Thompson Theorem~\cite{gonthier2013machine}.
This last development was the of about six years of work formalizing a proof that every finite group of odd order is solvable; the original proof, published in the early 1960s, is about 225 pages long.
\minortodo{Should I mention the following? ``Computer-verified proofs of versions of the first incompleteness theorem were announced [...] by Russell O'Connor in 2003 using Coq (O'Connor 2005)'' \url{https://en.wikipedia.org/wiki/Gödel's_incompleteness_theorems}}

\minortodo{Do I need to include anything more from \textcite{Proof2009Geuvers,History2014Harrison}?}

We now briefly mention a number of other proof assistants, calling out some particularly significant accomplishments of verification.
Undoubtedly we will miss some proof assistants and accomplishments, for which we refer the reader to the rich existing literature, some of which is cited in the first paragraph of this subsection, as well as scattered among other papers which describe a variety of proof assistants~\cite{Formalizing2009Wiedijk}.

Inspired by Automath, the Mizar~\cite{harrison-mizar,Rudnicki92anoverview,Matuszewski05mizar:the} proof checker was designed to assist mathematicians in preparing mathematical papers~\cite{Rudnicki92anoverview}.
The Mizar Mathematical Library had already 55 thousand formally verified lemmas in 2009 and was at the time (and might still be \minortodoask{is this accurate?}) the largest library of formal mathematics~\cite{Formalizing2009Wiedijk}.
LCF~\cite{LCF2000Gordon,gordon1979edinburgh,gordon1978metalanguage} spawned a number of other closely related proof assistants, such as HOL~\cite{Programming2000Barras,LCF2000Gordon}, Isabelle/HOL~\cite{LCF2019Paulson,Isabelle/Isar2002Wenzel,Isabelle,paulson1994isabelle}, HOL4~\cite{slind2008brief}, HOL Light~\cite{harrison1996hol}.
Among other accomplishments, a complete OS microkernel, seL4, was fully verified in Isabelle/HOL by 2009~\cite{seL4SOSP09}.
In 2014, a complete proof of the Kepler conjecture on optimal packing of spheres was formalized in a combination of Isabelle and HOL Light~\cite{flyspeck,flyspeck2014Hales}.
\minortodo{should I say anything about the Kepler conjecture? ``In 1998 Thomas Hales, following an approach suggested by Fejes Tóth (1953), announced that he had a proof of the Kepler conjecture. Hales' proof is a proof by exhaustion involving the checking of many individual cases using complex computer calculations. Referees said that they were ``99\% certain'' of the correctness of Hales' proof, and the Kepler conjecture was accepted as a theorem.'' \url{https://en.wikipedia.org/wiki/Kepler_conjecture}}
The functional programming language CakeML includes a self-bootstrapping optimizing compiler which is fully verified in HOL~\cite{CakeML}.
\minortodo{should I say something about the year?}
\minortodo{should I mention the following? ``Computer-verified proofs of versions of the first incompleteness theorem were announced [...] by John Harrison in 2009 using HOL Light (Harrison 2009). A computer-verified proof of both incompleteness theorems was announced by Lawrence Paulson in 2013 using Isabelle (Paulson 2014). `` \url{https://en.wikipedia.org/wiki/Gödel's_incompleteness_theorems}}
The Nqthm Boyer--Moore theorem prover eventually evolved into ACL2~\cite{ACL2,ACL2Applications}.
Other proof assistants include LF~\cite{pfenning1991logic,harper93,Logical2002Pfenning}, Twelf~\cite{pfenning1999system}, Matita~\cite{asperti2007user,Matita2011Asperti}, PVS~\cite{owre1996pvs,PVS1992Owre}, LEGO~\cite{LEGO1994Pollack}, and \NuPRL~\cite{nuprl}.
\minortodo{What should I say abound \NuPRL?}
\minortodoask{Which, if any, of the projects on http://www.nuprl.org/html/Projects.html are worth mentioning?  What about http://www.nuprl.org/Publications/?}
%\todo{Paragraph on \NuPRL.  Innovation: type theory and proof checking is type checking (but type checking is undecidable).  Poke around \NuPRL\space project website for 10 minutes to see what they highlight.  Talk about Curry--Howard, Check Martin-Löf.  Maybe email Bob Constable for historical context of \NuPRL\space}
\minortodo{should I mention Agda? Idris? Lean?}
\minortodo{Do I need to email Randy Pollack(sp?) creator of LEGO, to get historical context?}

\minortodo{How should I conclude this section?}

\section{Basic Design Choices}\label{sec:intro:proof-assistant-design-choices}
Although the design-space of proof assistants is quite large, as we've touched on in \autoref{sec:intro:history}, there are only two main design decisions which we want to assume for the investigations of this thesis.
The first is the use of dependent type theory as a basis for formal proofs, as is done in Automath~\cite{wiki:AutoMath,mathematical1970Bruijn,Survey1994deBruijn}, Coq~\cite{Coq}, Agda~\cite{Dependently2009Norell}, Idris~\cite{Idris2013Brady}, Lean~\cite{Lean2015Moura}, \NuPRL~\cite{nuprl}, Matita~\cite{Matita2011Asperti}, and others, rather than on some other logic, as is done in LCF~\cite{LCF2000Gordon,gordon1979edinburgh,gordon1978metalanguage}, Isabelle/HOL~\cite{LCF2019Paulson,Isabelle/Isar2002Wenzel,Isabelle,paulson1994isabelle}, HOL4~\cite{slind2008brief}, HOL Light~\cite{harrison1996hol}, LF~\cite{pfenning1991logic,harper93}, and Twelf~\cite{pfenning1999system}, among others.
The second is the \emph{de Bruijn criterion}, mandating independent checking of proofs by a small trusted kernel~\cite{challenge2005Barendregt}.
We have found that many of the performance bottlenecks are fundamentally a result of one or the other of these two design decisions.
Readers are advised to consult \textcite[ch.~4]{ringer2020qed} for a more thorough mapping of the design axes.

In this section, we will explain these two design choices in detail;
by the end of this section, the reader should understand what each design choice entails, and, we hope, why these are reasonable choices to make.

\minortodo{do I need to say any more before the subsections?}

\subsection{Dependent Types: What? Why? How?}\label{sec:why-how-dependent-types}
%Note: target audience: Saman (or POPL/PLDI)
\minortodo{subsubsection structure of this section}
There are, broadly, three schools of thought on what is a \emph{proof}.
\textcite{Proof2009Geuvers} describe two roles that a proof plays:
\begin{quote}
\begin{enumerate}[(i)]
\item A proof \emph{convinces} the reader that the statement is correct.
\item A proof \emph{explains} why the statement is correct.
\end{enumerate}
\end{quote}
A third conception of proof is that a proof is itself a mathematical object or construction which corresponds to the content of a particular theorem~\cite{Rigour2013Bauer}.
This third conception dates back to the school of intuitionism of Brouwer in the early 1900s and of constructive mathematics of Bishop in the 1960s; see \textcite[Related Works]{nuprl} for a tracing of the history from Brouwer to Martin-L\"of, whose type theory is at the heart of Coq and similar proof assistants.
\minortodo{should I rehash how proofs are done in other proof assistants?}

\minortodo{Work in justification of dependent types ``Randy Pollack, On extensibility of proof checkers, Types for Proofs
and Programs (LNCS 996) (Baastad, Sweden), June 1994, WWW:
\url{http://homepages.inf.ed.ac.uk/rpollack/export/extensibility.ps.gz}'' from ``However, Pollack [21] offers reasons why a dependently-typed
programming language may be preferable'' from \textcite{TrustedSlind}}

This third conception of proof admits formal frameworks where proof and computation are unified as the same activity.
As we'll see shortly, this allows for drastically smaller proofs.

The foundation of unifying computation and proving is, in some sense, the \emph{Curry--Howard--de Bruijn correspondence}, more commonly known as the Curry--Howard correspondence or the Curry--Howard isomorphism.
This correspondence establishes the relationship between types and propositions, between proofs and computational objects.

The reader may be familiar with types from programming languages such as C/C++, Java, and Python, all of which have types for strings, integers, and lists, among others.
A \emph{type} denotes a particular collection of objects, called its \emph{members}, \emph{inhabitants}, or \emph{terms}.
For example, \texttt{0} is a term of type \texttt{int}, \verb|"abc"| is a term of type \texttt{string}, and \texttt{true} and \texttt{false} are terms of type \texttt{bool}.
Types define how terms can be built and how they can be used.
New natural numbers, for example, can be built only as zero or as the successor of another natural number; these two ways of building natural numbers are called the type's \emph{constructors}.
Similarly, the only ways to get a new boolean are by giving either \texttt{true} or \texttt{false}; these are the two constructors of the type \texttt{bool}.
Note that there are other ways to get a boolean, such as by calling a function that returns booleans, or by having been given a boolean as a function argument.
The constructors define the only booleans that exist at the end of the day, after all computation has been run.
This uniqueness is formally encoded by the \emph{eliminator} of a type, which describes how to use it.
The eliminator on \texttt{bool} is the \texttt{if}-statement; to use a boolean, one must say what to do if it is \texttt{true}, and what to do if it is \texttt{false}.
Some eliminators encode recursion: to use a natural number, one must say what to do if it is zero, and also, one must say what to do if it is a successor.
In the case where the given number is the successor of $n$, however, one is allowed to call the function recursively on $n$.
For example, we might define the factorial function as
\begin{minted}{haskell}
fact m =
  case m of
    zero   -> succ zero
    succ n -> m * fact n
\end{minted}

Eliminators in programming correspond to \emph{induction} and case analysis in mathematics.
To prove a property of all natural numbers, one must prove it of zero, and also prove that if it holds for any number $n$, then it holds for the successor of $n$.
Here we see the first glimmer of the the Curry--Howard isomorphism, which identifies types with the set of terms of that type, identifiers propositions with the set of proofs of that proposition, and thereby identifies terms with proofs.

\begin{table}[t]
\centering
\begin{tabular}{c|c|c}
computation & set theory & logic \\ \hline
type & set & proposition \\
term / program & element of a set & proof \\
eliminator / recursion & & case analysis / induction \\
type of pairs & cartesian product ($\times$) & conjunction ($\wedge$) \\
sum type ($+$) & disjoint union ($\sqcup$) & disjunction ($\vee$) \\
function type & set of functions & implication ($\to$) \\
unit type & singleton set & trivial truth \\
empty type & empty set ($\emptyset$) & falsehood \\
dependent function type ($\Pi$) & & universal quantification ($\forall$) \\
dependent pair type ($\Sigma$) & & existential quantification ($\exists$)
\end{tabular}
\minortodo{Fix overflow of table}
\caption{The Curry--Howard Correspondence}
\label{tab:curry-howard}
\end{table}

\autoref{tab:curry-howard} shows the correspondence between programs and proofs.
We have already seen how recursion lines up with induction in the case of natural numbers; let us look now at how some of the other proof rules correspond.
%Readers familiar with the Curry--Howard correspondence may safely skip the next few paragraphs.

To prove a conjunction $A \wedge B$, one must prove $A$ and also prove $B$; if one has a proof of the conjunction $A \wedge B$, one may assume both $A$ and $B$ have been proven.
This lines up exactly with the type of pairs: to inhabit the type of pairs $A \times B$, one must give an object of type $A$ paired with an object of type $B$; given an object of the pair type $A \times B$, one can \emph{project} out the components of types $A$ and $B$.

To prove the implication $A \to B$, one must prove $B$ under the assumption that $A$ holds, i.e., that a proof of $A$ has been given.
The rule of \emph{modus ponens} describes how to use a proof of $A \to B$: if also a proof of $A$ is given, then $B$ may be concluded.
These correspond exactly to the construction and application of functions in programming languages:
to define a function of type $A \to B$, the programmer gets an argument of type $A$ and must return a value of type $B$.
To use a function of type $A \to B$, the programmer must \emph{apply} the function to an argument of type $A$; this is also known as \emph{calling} the function.

Here we begin to see how type-checking and proof-checking can be seen as the same task.
The process of \emph{type-checking} a program consists of ensuring that every variable is given a type, that every expression assigned to a variable has the type of that variable, that every argument to a function has the correct type, etc.
If we write the boolean negation function which sends \texttt{true} to \texttt{false} and \texttt{false} to \texttt{true} by case analysis (i.e., by an \texttt{if}-statement), the type-checker will reject our program if we try to apply it to, say, an argument of type \texttt{string} such as \verb|"foo"|.
Similarly, if we try to use \emph{modus ponens} to combine a proof that $x = 1 \to 2x = 2$ with a proof that $x = 2$ to obtain a proof that $2x = 2$, the proof checker should complain that $x = 1$ and $x = 2$ are not the same type.

While the correspondence of the unit type to tautologies is relatively trivial, the correspondence of the empty type to falsehood encodes non-trivial principles.
By encoding falsehood as the empty type, the principle of explosion---that from a contradiction, everything follows---can be encoded as case analysis on the empty type.
\minortodo{do I need to explain this bit more?}

The last two rows of \autoref{tab:curry-howard} are especially interesting cases which we will now cover.

Some programming languages allow functions to return values whose type depends on the \emph{value} of the function's arguments.
In these languages, the types of arguments are generally also allowed to depend on the values of previous arguments.
Such languages are said to support dependent types.
For example, we might have a function that takes in a boolean, and returns a string if the boolean is \texttt{true}, but an integer if the boolean is \texttt{false}.
More interestingly, we might have a function that takes in two booleans, and additionally takes in a third argument which is of type \texttt{unit} whenever the two booleans are either both \texttt{true} or both \texttt{false}, but is of type \texttt{empty} when they are not equal.
This third argument serves as a kind of proof that the first two arguments are equal.
By checking that the third argument is well-typed, that is, that the single inhabitant of the \texttt{unit} type is passed only when in fact the first two arguments are equal, the type-checker is in fact doing proof checking.
While compilers of languages like C++, which supports dependent types via templates, can be made to do rudimentary proof-checking in this way, proof assistants such as Coq are built around such dependently-typed proof checking.

The last two lines of \autoref{tab:curry-howard} can now be understood.

A dependent function type is just one whose return value depends on its arguments.
For example, we may write the non-dependently-typed function type
\[
\texttt{bool} \to \texttt{bool} \to \texttt{unit} \to \texttt{unit}
\]
which takes in three arguments of types \texttt{bool}, \texttt{bool}, and \texttt{unit}, and returns a value of type \texttt{unit}.
Note that we write this function in curried style, with $\to$ associating to the right (i.e., $A \to B \to C$ is $A \to (B \to C)$), where functions take in one argument at a time, and return a function awaiting the next argument.
This function is not very interesting, since it can only return the single element of type \texttt{unit}.

However, if we define $E(b_1, b_2)$ to be the type \mintinline{coq}{if b₁ then (if b₂ then unit else empty) else (if b₂ then empty else unit)}, i.e., the type which is \texttt{unit} when both are \texttt{true} or both are \texttt{false}, and is \texttt{empty} otherwise, then we may write the dependent type
\[
(b_1 : \texttt{bool}) \to (b_2 : \texttt{bool}) \to E(b_1, b_2) \to E(b_2, b_1)
\]
Alternate notations include
\[
\Pi_{b_1 : \texttt{bool}} \Pi_{b_2 : \texttt{bool}} E(b_1, b_2) \to E(b_2, b_1)
\]
and
\[
\forall (b_1 : \texttt{bool}) (b_2 : \texttt{bool}), E(b_1, b_2) \to E(b_2, b_1).
\]
A function of this type witnesses a proof that equality of booleans is symmetric.
\minortodo{should I say more?  How to organize this paragraph?}

Similarly, dependent pair types witness existentially quantified proofs.
Suppose we have a type $T(n)$ which encodes the statement ``$n$ is prime and even''.
To prove $\exists n, T(n)$, we must provide an explicit $n$ together with a proof that it satisfies $T$.
This is exactly what a dependent pair is: $\Sigma_n T(n)$ is the type of pairs of numbers $n$ paired with proofs that that particular $n$ satisfies $T$.

\minortodo{subsection split here?}

As we mentioned above, one feature of basing a proof assistant on dependent type theory is that computation can be done at the type-level, without leaving a trace in the proof term.
Many proofs require intermediate arguments based solely on the computation of functions.
For example, a proof in number theory or cryptography might depend on the fact that a particularly large number, raised to some large power, is congruent to 1 modulo some prime.
As argued by \textcite{stampoulis2013veriml}, if we are required to record all intermediate computation steps in the proof term, they can become prohibitively large.
The \emph{Poincaré principle} asserts that such arguments should not need to be recorded in formal proofs, but should instead be automatically verified by appeal to computation~\cite[p.~1167]{barendregt2001proof}.
The ability to appeal to computation without blowing up the size of the proof term is quite important for so-called reflective (or reflexive) methods of proof, described in great detail in \autoref{ch:reflection}.

\minortodo{I think the original phrasing of \textcite{stampoulis2013veriml} is much clearer:
``Another variation of the basic architecture that is part of the Coq proof assistant is the inclusion of a conversion rule in its logical core.
Arguments that are based solely on computation of functions defined in the logic, such as the fact that $1+1=2$, are ubiquitous throughout formal proofs.
If we record all the steps required to show that such arguments are valid as part of the proof objects,  their size soon becomes prohibitively large.
It has been argued that such arguments should not be recorded in formal proofs but rather should be automatically decided through computation – a principle that has been called the Poincaré principle'' \cite[p.~1167]{barendregt2001proof}
What should be done?}

\minortodo{where should I mention that the LCF approach, in particular the ability to never manifest proof objects, is somewhat incompatible with dependent types, as there is no distinction between proofs and terms, and hence proofs must be manifested in general so that we can inspect them~\cite{NewCoqTactics2016Pedrot}.}

\minortodo{do I need to do more to explain the idea of reducing proof checking to typechecking (with examples)?}

\minortodo{do I need to explain the idea of structuring programming so that some properties are true by construction?  where do I fit it?  what do I say?}

\minortodo{do I need to explain extensional vs intensional?}

\minortodo{is there anything more to say in this section?}

Readers interested in a more comprehensive explanation of dependent type theory are advised to consult Chapter 1 (Type theory) and Appendix A (Formal type theory) of \textcite{HoTTBook}.
Readers interested in perspectives on how dependent types may be disadvantageous are invited to consult literature such as \textcite{Should1999Lamport} and \textcite{Formalising2018Paulson}.

\subsection{The de Bruijn Criterion}\label{sec:debruijn-criterion}
\begin{quote}
  A Mathematical Assistant satisfying the possibility of independent checking by a small program is said to satisfy the \emph{de Bruijn} criterion.
\end{quote}
\begin{flushright}
  --- Henk Barendregt~\cite{challenge2005Barendregt}
\end{flushright}

As described in the beginning of this chapter, the purpose of proving our software correct is that we want to be able to trust that it has no bugs.
Having a proof checker reduces the problem of software correctness to the problem of the correctness of the specification, together with the correctness of the proof checker.
If the proof checker is complicated and impenetrable, it might be quite unreasonable to trust it.

\minortodo{is there a clearer way to express this?}
Proof assistants satisfying the de Bruijn criterion are, in general, more easily trustable than those which violate it.
The ability to check proofs with a small program, divorced from any heuristic programs and search procedures which generate the proof, allows trust in the proof to be reduced to trust in that small program.
Sufficiently small and well-written programs can more easily be inspected and verified.

%\todo{explain why it's worth following this, and what goes wrong if you don't}

The proof assistant Coq, which is the primary proof assistant we consider in this thesis, is a decent example of satisfying the de Bruijn criterion.
There is a large untrusted codebase which includes the proof scripting language \Ltac, used for generating proofs and doing type inference.
There's a much smaller kernel which checks the proofs, and Coq is even shipped with a separate checker program, \texttt{coqchk}, for checking proof objects saved to disk.
Moreover, in the past year, a checker for Coq's proof objects has been implemented in Coq itself and formally verified with respect to the type theory underlying Coq~\cite{Coq2019Sozeau}.

Note that the LCF approach to theorem proving, where proofs have an abstract type and type-safety of the tactics guarantees validity of the proof object, forms a sort-of complementary approach to trust.
\minortodo{what should I say about LCF here?}
\minortodo{should I mention formal verification of \NuPRL\space in Coq?}

\minortodo{is there anything else to say in this subsection?}

\section{Look ahead: Layout and contributions of the thesis}\label{sec:intro:layout}
In the remainder of \autoref{part:introduction}, we will finish laying out the landscape of performance bottlenecks we encountered in dependently-typed proof assistants;
\fullnameref{ch:perf-failures} gives a more in-depth investigation into what makes performance optimization in dependent type theory hard, different, and unique, followed by describing major axes of super-linear performance bottlenecks in \fullnameref{sec:perf-axes}.

\fullnameref{part:rewriting} is devoted, in some sense, to performance bottlenecks that arise from the de Bruijn criterion of \autoref{sec:debruijn-criterion}.
We investigate one particular method for avoiding these performance bottlenecks.
We introduce this method, variously called \emph{proof by reflection} or \emph{reflective automation}, in \fullnameref{ch:reflection}, with a special emphasis on a particularly common use case---transformation of syntax trees.
\fullnameref{ch:rewriting} describes our original contribution of a framework for leveraging reflection to perform rewriting and program transformation at scale, driven by our need to synthesize efficient, proven-correct, low-level cryptographic primitives~\cite{FiatCryptoSP19}.
\minortodo{cite also the rewriter paper }%
Where \autoref{ch:rewriting} addresses the performance challenges of verified or proof-producing program transformation, \fullnameref{ch:rewriting-more} is a deep-dive into the performance challenges of engineering the tool itself, and serves as a sort-of microcosm of the performance bottlenecks previously discussed and the solutions we've proposed to them.
Unlike the other chapters of this thesis, \autoref{ch:rewriting-more} at times assumes a great deal of familiarity with the details of the Coq proof assistant.
Finally, \fullnameref{ch:reification-by-parametricity} presents a way to efficiently, elegantly, and easily perform \emph{reification}, the first step of proof by reflection, which is often a bottleneck in its own right.
We discovered---or invented---this trick in the course of working on our library for synthesis of low-level cryptographic primitives~\cite{FiatCryptoSP19,reification-by-parametricity}.

\fullnameref{part:design} is devoted, by and large, to the performance bottlenecks that arise from the use of dependent types as the basis of a proof assistant as introduced in \autoref{sec:why-how-dependent-types};
in \fullnameref{ch:design}, we discuss lessons on engineering libraries at scale drawn from our case study in formalizing category theory %~\cite{category-coq-experience}
and augmented by our other experience.
Many of the lessons presented here are generalizations of examples described in \fullnameref{ch:rewriting-more}.
The category theory library formalized as part of this doctoral work, available at \textcite{HoTT/HoTT-categories}, is described briefly in this chapter; a more thorough description can be found in the paper we published on our experience formalizing this library~\cite{category-coq-experience}.

\fullnameref{part:conclusion} is in some sense the mirror image of \autoref{part:introduction}:
Where \autoref{ch:perf-failures} is a broad look at what is currently lacking and where performance bottlenecks arise, \fullnameref{ch:coq-tooling-fixes} takes a historical perspective on what advancements have already been made in the performance of proof assistants, and Coq in particular.
Finally, while the present chapter which we are now concluding has looked back on the present state and history of formal verification, \fullnameref{ch:conclusion} looks forward to what we believe are the most important next steps in the perhaps-nascent field of proof-assistant performance at scale.


\begin{subappendices}

\section{deleted paragraphs}


Even with machine-checked proofs, it is impossible to gain complete confidence in your software; the layer underneath may be untrustworthy in ways you cannot see~\cite{Reflections1984Thompson}, you can never prove that the mathematical system you're working in is correct~\cite{sep-goedel-incompleteness}, and most proof-checkers are not yet themselves proven correct.
However, machine-checked proofs allow a drastic reduction in how much code needs to be trusted.
Rather than trusting the millions of lines of code of the software being verified, you only need to trust the specifications of the software, and the likely-much-smaller codebase of the proof checker.
Furthermore, it's very unlikely that bugs in the proof checker will overlap with bugs in the proof in just the right way to allow errors to slip through only in one particular piece of software.
Said another way, we no longer need to write tests for every piece of software separately; writing tests for the proof-checker helps eliminate bugs in \emph{all} software proven by that checker.

\begin{minorcomment}
\section{TODOs from \autoref{sec:intro:intro}}
\end{minorcomment}
%\todo{read / look into \textcite{georges2007statistically}}
%\todo{maybe read \textcite{mytkowicz-wrong-data}}
%\todo{Karl Palmskog: @Jason Gross I saw your post on performance optimization/measurements for proof assistants on Coq-Club. We summarize a lot of work on parallelization/selection for proof assistants in our regression proving papers (ASE 17 \& ISSTA 18): \textcite{CelikETAL17iCoq} \textcite{PalmskogETAL18piCoq}}
\minortodo{read and cite \textcite{lean-tactic-language}}
\minortodo{read \textcite{Implementing1998Shao}}
\begin{minorcomment}
  ``It's not directly related to proof assistants, but the techniques described can be applicable to proof assistants and the experience \emph{may} be applicable to some extent.''
\end{minorcomment}
\minortodo{read \textcite{Inductive2003Brady}}
\minortodo{read \textcite{thesis-nogin}}
\minortodo{read Grégoire and Leroy's paper from ICFP 2002 \textcite{vmcompute}}
\minortodo{read Dirk Kleeblatt's PhD thesis \textcite{Strongly2011Kleeblatt}.}
\begin{minorcomment}
``Both of these are about using compiled code in dependent type checkers instead of interpreters.''
\end{minorcomment}
\minortodo{read András Kovács has smalltt at \textcite{smallttKovacs}``, which I don't think has been written up anywhere but has nonetheless been influential, both on Idris 2 and on Olle Fredriksson's reimplementation of Sixten at \url{https://github.com/ollef/sixty}''}
\minortodo{look at \url{https://github.com/AndrasKovacs/normalization-bench}}
\minortodo{look at \url{https://github.com/AndrasKovacs/smalltt}}
\minortodo{``I haven't yet updated the smalltt repo, but there's a simplified (\url{https://gist.github.com/AndrasKovacs/a0e0938113b193d6b9c1c0620d853784}) implementation of its evaluator, which seems to have roughly the same performance but which is much simpler to implement.''}
%\todo{\cite{Idris2Faster2020Brady}}
\minortodo{\cite{NewCoqTactics2016Pedrot} about why not LCF tactics in dependently typed setting}

\begin{minorcomment}

András Kovács wrote:
\begin{quotation}
The basic idea is that in elaboration there are two primary computational tasks, one is conversion checking and the other is generating solutions for metavariables. Clearly, we should use NbE/environment machines for evaluation, and implement conversion checking in the semantic domain. However, when we want to generate meta solutions, we need to compute syntactic terms, and vanilla NbE domain only supports quote/readback to normal forms. Normal forms are way too big and terrible for this purpose. Hence, we extend vanilla NbE domain with lazy non-deterministic choice between two or more evaluation strategies. In the simplest case, the point of divergence is whether we unfold some class of definitions or not. Then, the conversion checking algorithm can choose to take the full-unfolding branch, and the quoting operation can choose to take the non-unfolding branch. At the same time, we have a great deal of shared computation between the two branches; we avoid recomputing many things if we choose to look at both branches.

I believe that a feature like this is absolutely necessary for robust performance. Otherwise, we choke on bad asymptotics, which is surprisingly common in dependent settings. In Agda and Coq, even something as trivial as elaborating a length-indexed vector expression has quadratic complexity in the length of the vector.

It is also extremely important to stick to the spirit of Coquand's semantic checking algorithm as much as possible. In summary: core syntax should support *no* expensive computation: no substitution, shifting, renaming, or other ad-hoc term massaging. Core syntax should be viewed as immutable machine code, which supports evaluation into various semantic domains, from which sometimes we can read syntax back; this also leaves it open to swap out the representation of core syntax to efficient alternatives such as bytecode or machine code.

Only after we get  the above two basic points right, can we start to think about more specific and esoteric optimizations. I am skeptical of proposed solutions which do not include these. Hash consing has been brought up many times, but it is very unsatisfying compared to non-deterministic NbE, because of its large constant costs, implementation complexity, and the failure to handle sharing loss from beta-redexes in any meaningful way (which is the most important source of sharing loss!). I am also skeptical of exotic evaluators such as interaction nets and optimal beta reducers; there is a good reason that all modern functional languages run on environment machines instead of interaction nets.

If we want to support type classes, then \href{https://arxiv.org/pdf/2001.04301.pdf}{tabled instance resolution} \textcite{Tabled2020Selsam} is also a must, otherwise we are again smothered by bad asymptotics even in modestly complex class hierarchies. This can be viewed as a specific instance of hash-consing (or rather ``memoization''), so while I think ubiquitous hash-consing is bad, some focused usage can do good.

Injectivity analysis is another thing which I believe has large potential impact. By this I mean checking whether functions are injective up to definitional equality, which is decidable, and can be used to more precisely optimize unfolding in conversion checking.

I'd be very interested in your findings about proof assistant performance. This has been a topic that I've been working on on-and-off for several years. I've recently started to implement a system which I intend to be eventually ``production strength'' and also as fast as possible, and naturally I want to incorporate existing performance know-how.
\end{quotation}
\end{minorcomment}
\minortodo{look into ``So technically, the lost sharing is the second-order sharing that is preserved
    in ``optimal reduction'' of lambda calculi [Levy-1980, Lamping-1990, Asperti-Laneve-1992],
    while hash consing normally is directly usable only for first-order sharing.''}
\minortodo{look at \url{https://math.stackexchange.com/questions/3466976/online-reference-book-for-implementing-concepts-in-type-theory}}
\minortodo{look at \url{https://github.com/AndrasKovacs/elaboration-zoo/blob/0c7f8a676c0964cc05c247879393e97729f59e5b/AIMprez/AIMprez.pdf} or \url{https://eutypes.cs.ru.nl/eutypes\_pmwiki/uploads/Meetings/Kovacs\_slides.pdf}}
\begin{minorcomment}
  Konrad Slind wrote:
\end{minorcomment}
\minortodo{read \textcite{Programming2000Barras}}
\minortodo{\textcite{Efficiency1994Boulton}}

\minortodo{look into \textcite{Sealing2020Selsam} Sealing Pointer-Based Optimizations Behind Pure Functions}

\begin{minorcomment}
\section{Coq's design}
\end{minorcomment}
\minortodo{where does this description of Coq's design go?}
\begin{minorcomment}
Coq is split into two parts.

There's the part of the system that is called the kernel or the trusted code base.
Once you get a proof this part will be like ``yup, I believe the proof'' or like ``nope your proof is bad.''
And then there's the other part that is like ``here's magic and it will make proof for you.''
You're like ``I have an arithmetic expression please prove that it's true'' and there's a bit in this other part that's like ``I know how to prove arithmetic expressions'' and it gets the arithmetic expression and then it generates a certificate or proof that this other trusted part checks.
If the part generating the arithmetic proof is wrong then the users come complaining to you that you have a bug.
If the part checking the proofs is wrong, then you don't see the bug and now suddenly your users can prove anything they want.
And the system is no longer trustworthy right and so for that bit of it you need to be very careful with any changes you make.
\end{minorcomment}
\end{subappendices}
%\begin{itemize}
%  \item Mention domains: math/CT and program transformation
%  \item Case studies: parsers, fiat-crypto, CT library
%\end{itemize}
