\chapter{The Performance Landscape in Type-Theoretic Proof Assistants} \label{ch:perf-failures}

The purpose of this chapter is to convince the reader that the issue of performance in proof assistants is non-trivial in ways that differ from performance bottlenecks in non-dependently-typed languages.
I intend to do this by first sketching out what I see as the main difference between performance issues in dependently-typed proof assistants vs performance issues in other languages, and then supporting this claim with a palette of real performance issues that have arisen in Coq.

The widespread commonsense in performance engineering\cite{commonsense-perf-engineering-order-of-operations} is that good performance optimization happens in a particular order:
there is no use micro-optimizing code if you are implementing an algorithm with unacceptable performance characteristics; imagine trying to optimize the pseudorandom number generator used in bogosort~\cite{bogosort}, for example.\footnote{Bogosort, whose name is a portmanteau of the words bogus and sort~\cite{bogosort-name}, sorts a list by randomly permuting the list over and over until it is sorted.}
Similarly, there is no use trying to find or create a better algorithm if the problem you're solving is more complicated than it needs to be; consider, for example, the difference between ray tracers and physics simulators.
Ray tracers determine what objects can be seen from a given point essentially by drawing lines from the viewpoint to the object and seeing if it passes through any other object ``in front of'' it.
Alternatively, one could provide a source of light waves and simulate the physical interaction of light with the various objects, to determine what images remain when the light arrives at a particular point.
There's no use trying to find an efficient algorithm for simulating quantum electrodynamics, though, if all you need to know is ``which parts of which objects need to be drawn on the screen?''

One essential ingredient to allowing this division of concerns---between specifying the problem, picking an efficient algorithm, and optimizing the implementation of the algorithm---is knowledge of what a typical set of input looks like, and what the scope looks like.
In Coq, and other dependently-typed proof assistants, this ingredient is missing.
When sorting a list, we know that the length of the list and the initial ordering matter; it generally doesn't matter, though, whether we're sorting a list of integers or colors or names.
Furthermore, randomized datasets tend to be reasonably representative for list ordering, though we may also care about some special cases, such as already-sorted lists, nearly sorted lists, and lists in reverse-sorted order.
We can say that sorting is always possible in $\mathcal O(n\log n)$ time, and that's a pretty good starting point.

In proof assistants, the domain is much larger: in theory, we want to be able to check any proof anyone might write.
Furthermore, in dependently typed proof assistants, the worst-case behavior is effectively unbounded, because any provably terminating computation can be run at typechecking time.

In fact, this issue already arises for compilers of mainstream programming languages.
\todo{literature search on perf of compiletime in mainstream compilers?}
The C++ language, for example, has \texttt{constexpr} constructions that allow running arbitrary computation at compile-time, and it's well-known that C++ templates can incur a large compile-time performance overhead.\todo{cite?}
However, I claim that, in most languages, even as you scale your program, these performance issues are the exception rather than the rule.
Most code written in C or C++ does not hit unbounded compile-time performance bottlenecks.
Generally if you write code that compiles in a reasonable amount of time, as you scale up your codebase, your compile time will slowly creep up as well.

In Coq, however, the scaling story is very different.
Frequently, users will cobble together code that works to prove a toy version of some theorem, or to verify a toy version of some program.
By virtue of the fact that humans are impatient, the code will execute in reasonable time on the toy version.
The user will then apply the same proof technique on a slightly larger example, and the proof-checking time will often be pretty similar.
After scaling the input size a bit more, the proof-checking time will be noticeably slow---maybe it now takes a couple of minutes.
Scaling the input just a tiny bit more, though, will result in the compiler not finishing even if you let it run for a day or more.
This is what working in an exponential performance domain is like.

To put numbers on this, a project I was working on\todo{cite fiat-crypto} involved generating C code to do arithmetic on very large numbers.
The code generation was parameterized on the number of machine words needed to represent a single big integer.
Our smallest toy example used two machine words; our largest---slightly unrealistic---example used 17.
The smallest toy example---two machine words---took about 14 seconds.
Based on the the compile-time performance of about a hundred examples, we expect the largest example---17 machine words---would have taken over four thousand \emph{millenia}!
(Our primary non-toy test example used four machine words and took just under a minute; the biggest realistic example we were targeting was twice that size, at eight machine words, and took about 20 hours.)

Maybe, you might ask, were we generating unreasonable amounts of code?
Each example using $n$ machine words generated $3n$ lines of code.
Furthermore, the actual code generation took less than 0.002\% of the total time on the largest examples we tested (just 14 seconds out of about 211 hours).
How can this be?

Our method involved two steps: first generate the code, then check that the generated code matches with what comes out of the verified code generator.
This may seem a bit silly, but this is actually somewhat common; if you have a theorem that says ``any code that comes out of this code generator satisfies this property'', you need a proof that the code you feed into the theorem actually came out of the specified code generator, and the easiest way to prove this is, roughly, to tell the proof assistant to just check that fact for you.
(It's possible to be more careful and not do the work twice, but this often makes the code a bit harder to read and understand, and is oftentimes pointless; premature optimization is the root of all evil, as they say.)
Furthermore, because you often don't want to fully compute results when checking that two things are equal---just imagine having to compute the factorial of $1000$ just to check that $1000!$ is equal to itself---the default method for checking that the code came out of the code generator is different from the method we used to compute the code in the first place.

The fix itself is quite simple, only 21 characters long.\footnote{\texttt{Strategy 1 [Let\_In].} for those who are curious.}
However, tracking down this solution was quite involved, requiring the following pieces:
\begin{enumerate}
  \item
    A good profiling tool for proof scripts (see \autoref{sec:ltac-prof}).
    This is a standard component of a performance engineer's toolkit, but when I started my PhD, there was no adequate profiling infrastructure for Coq.
    While such a tool is essential for performance engineering in all domains, what's unusual about dependently-typed proof assistants, I claim, is that essentially \emph{every} codebase that needs to scale runs into performance issues, and furthermore these issues are frequently total blockers for development because so many of them are exponential in nature.
  \item
    Understanding the details of how Coq works under-the-hood.
    Conversion, the ability to check if two types or terms are the same, is one of the core components of any dependently-typed proof assistant.
    Understanding the details of how conversion works is generally not something users of a proof assistant want to worry about; it's like asking C programmers to keep in mind the size of \texttt{gcc}'s maximum nesting level for \texttt{\#include}'d files\footnote{It's 200, for those who are curious.\todo{cite \url{https://gcc.gnu.org/onlinedocs/gcc-7.5.0/cpp/Implementation-limits.html}}} when writing basic programs.
    It's certainly something that advanced users need to be aware of, but it's not something that comes up frequently.
  \item
    Being able to run the proof assistant in your head.
    When I looked at the conversion problem, I knew immediately what the most likely cause of the performance issue was.
    But this is because I've managed to internalize most of how Coq runs in my head.
    This might seem reasonable at a glance; one expects to have to understand the system being optimized in order to optimize it.
    But I've managed to learn the details of what Coq is doing---including performance characteristics---basically without having to read the source code at all!
    This is akin to, say, being able to learn how \texttt{gcc} represents various bits of C code, what transformations it does in what order, and what performance characteristics these transformations have, just from using \texttt{gcc} to compile C code and reading the error messages it gives you.
    These are details that should not need to be exposed to the user, but because dependent type theory is so complicated---complicated enough that it's generally assumed that users will get \emph{line-by-line interactive feedback from the compiler} while developing, the numerous design decisions and seemingly reasonable defaults and heuristics lead to subtle performance issues.
  \item
    Knowing how to tweak the built-in defaults for parts of the system which most users expect to be able to treat as black-boxes.
\end{enumerate}

\todo{Some sort of summary of argument-so-far here}

To finish off the argument about slowness in dependently-typed proof assistants, I want to present a couple more cases of performance bottlenecks.

\section{Quadratic Behavior in \texorpdfstring{\texttt{cbv}}{cbv}}
\todo{explain how cbv is quadratic}

\section{Quadratic Behavior in \texorpdfstring{\texttt{vm\_compute}}{vm\_compute}}
\todo{explain how vm\_compute is quadratic?}

\section{How Can We Prove Anything Big?}
Consider the problem of proving a large conjunction, such as \texttt{True $\wedge$ True $\wedge \cdots\wedge$ True}.
Because we want a separation between 
\begin{itemize}
  \item \todo{intros is cubic}
  \item \todo{Application is quadratic \url{https://github.com/andres-erbsen/coq-experiments/blob/master/experiments/bench/app10000.v}}
  \item \todo{Quadratic proof term for many ands}: \url{https://github.com/andres-erbsen/coq-experiments/blob/master/experiments/bench/big\_and\_10000\_true.v}
  \begin{itemize}
      \item -> try let-binding things (but intros n is $n^3$ time)
      \item -> try reflection
      \item -> application is also quadratic (maybe now fixed)
      \end{itemize}
  
\end{itemize}

\todo{figure out examples}
\todo{figure out story for this section}

\section{The Four Axes}
\todo{introduce this section}

\subsection{The Size of the Type}

We start with one of the simplest axes.

Suppose we want to prove a conjunction of $n$ things, say, $\texttt{True} \wedge \texttt{True} \wedge \cdots \wedge \texttt{True}$.
For such a simple theorem, we want the size of the proof, and the time- and memory- complexity of checking it, to be linear in $n$.

Recall from \autoref{sec:debruijn-criterion} that we want a separation between the small trusted part of the proof assistant and the larger untrusted part.
The untrusted part generates certificates, which in dependently typed proof assistants are called terms, which the trusted part, the kernel, checks.

The obvious certificate to prove a conjunction $A \wedge B$ is to hold a certificate $a$ proving $A$ and a certificate $b$ proving $B$.
In Coq, this certificate is called \texttt{conj} and it takes four parameters: $A$, $B$, $a : A$, and $b : B$.
Perhaps you can already spot the problem.

To prove a conjunction of $n$ things, we end up repeating the type $n$ times in the certificate, resulting in a term that is quadratic in the size of the type.
\todo{perf test conj term}

Note that for small, and even medium-sized examples, it's pretty reasonable to do duplicative work.
It's only when we reach very large examples that we start hitting non-linear behavior.

There are two obvious solutions for this problem:
\begin{enumerate}
    \item
    We can drop the type parameters from the \texttt{conj} certificates.
    \item
    We can implement some sort of sharing, where common subterms of the type only exist once in the representation.
\end{enumerate}

\subsubsection{Dropping Type Parameters: Nominal vs.~Structural Typing}
The first option requires that the proof assistant implement structural typing~\cite{structural-typing} rather than nominal typing~\cite{nominal-typing}.
\todo{explain structural and nominal typing more}
Morally, the reason for this is that if we have an inductive record type \todo{have we explained inductive types yet?} \todo{have we explained records yet?} whose fields do not constrain the parameters of the inductive type family \todo{have we explained parameters vs indices and inductive type families yet?}, then we need to consider different instantiations of the same inductive type family to be convertible.
That is, if we have a phantom record such as\todo{mention where the name ``phantom'' comes from?}
\begin{minted}{coq}
Record Phantom (A : Type) := phantom {}.
\end{minted}
and our implementation does not include \texttt{A} as an argument to \texttt{phantom}, then we must consider \texttt{phantom} to be both of type \texttt{Phantom nat} and \texttt{Phantom bool}, even though \texttt{nat} and \texttt{bool} are not the same.
I have requested this feature in \cite{https://github.com/coq/coq/issues/5293}.
Note, however, that sometimes it is important for such phantom types to be considered distinct when doing type-level programming.
\todo{Come up with better justification for having nominal typing available?}

\subsubsection{Sharing}
The alternative to eliminating the duplicative arguments is to ensure that the duplication is at-most constant sized.
There are two ways to do this: either the user can explicitly share subterms so that the size of the term is in fact linear in the size of the goal, or the proof assistant can ensure maximal sharing of subterms \todo{explain this better}.

There are two ways for the user to share subterms: using let-binders, and using function abstraction.
For example, rather than writing
\begin{minted}{coq}
@conj True (and True (and True True)) I (@conj True (and True True) I (@conj True True I I))
\end{minted}
and having roughly $n^2$ occurrences\footnote{The exact count is $n(n+1)/2 - 1$.} of \mintinline{coq}{True} when we are trying to prove a conjunction of $n$ \mintinline{coq}{True}s, the user can instead write
\begin{minted}{coq}
let T0 : Prop := True in
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
which has only $n$ occurrences of \mintinline{coq}{True}.
Alternatively, the user can write
\begin{minted}{coq}
(λ (T0 : Prop) (v0 : T0),
  (λ (T1 : Prop) (v1 : T1),
    (λ (T2 : Prop) (v2 : T2), @conj True T2 I v2)
      (and True T1) (@conj True T1 I v1))
    (and True T0) (@conj True T0 I v0))
  True I
\end{minted}

Unfortunately, both of these incur quadratic typechecking cost, even though the size of the term is linear.

Recall that the typing rules for $\lambda$ and \texttt{let} are as follows:\todo{cite appendix with typing rules of Coq?}
\todoask{What's the suggested way of pretty-printing typing rules?}
\todoask{Which way do substitution brackets go?}
\todoask{What convention should we use for typing rules with regard to things being types?  Maybe just copy the HoTT book?}
\begin{verbatim}
Γ ⊢ A type      Γ, x:A ⊢ B type
Γ, x:A ⊢ f:B
-------------------------------
Γ ⊢ (λ (x:A), f) : ∀ x:A, B


Γ ⊢ A type      Γ, x:A ⊢ B type
Γ ⊢ f : ∀ x:A, B
Γ ⊢ y : A
-------------------------------
Γ ⊢ f y : B[x/y]
\end{verbatim}
    
\begin{verbatim}
Γ ⊢ A type
Γ ⊢ y : A
Γ, x:A:=y ⊢ B type
Γ, x:A:=y ⊢ f : B
-------------------------------
Γ ⊢ (let x : A := y in f) : B[x/y]
\end{verbatim}

Let us consider the inferred types for the intermediate terms when typechecking the \mintinline{coq}{let} expression:
\begin{itemize}
  \item
  We infer the type \mintinline{coq}{and True T2} for the expression
\begin{minted}{coq}
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v2} into that type to type the expression
\begin{minted}{coq}
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We substitute \mintinline{coq}{T2 := and True T1} into this type to get the type \mintinline{coq}{and True (and True T1)} for the expression
\begin{minted}{coq}
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v1} into this type to get the type for the expression
\begin{minted}{coq}
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We substitute \mintinline{coq}{T1 := and True T0} into this type to get the type \mintinline{coq}{and True (and True (and True T0))} for the expression
\begin{minted}{coq}
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v0} into this type to get the type for the expression
\begin{minted}{coq}
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  Finally, we substitute \mintinline{coq}{T0 := True} into this type to get the type \mintinline{coq}{and True (and True (and True True))} for the expression
\begin{minted}{coq}
let T0 : Prop := True in
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
\end{itemize}
Note that we have performed linearly many substitutions into linearly-sized types, so unless substitution is constant time in size of the term being substituted, we incur quadratic overhead here.
The story for function abstraction is similar.

\todo{Should we run though typechecking in more detail here?}

We again have two choices to fix this:
either we can change the typechecking rules (which work just fine for small-to-medium-sized terms), or we can adjust typechecking to deal with some sort of pending substitution data, so that we only do substitution once.

\todo{maybe cite https://github.com/coq/coq/issues/11838?}

\todo{reference quadratic cbv here, which had a similar issue?}

\todo{some sort of section division marker here?}

The proof assistant can also try to maximally share subterms for us.
Many proof assistants do some version of this, called \emph{hashconsing}.
\todo{explain and cite hashconsing?}

However, hashconsing looses a lot of its benefit if terms are not maximally shared, and can lead to very unpredictable performance when transformations unexpectedly cause a loss of sharing.
\todo{cite hashconsing needing to be full to get perf benefit}
Furthermore, it's an open problem how to efficiently persist full hashconsing to disk in a way that allows for diamond dependencies.
\todo{explain this more, find citation for hashconsing being hard with disk}
\todo{flesh out hashconsing section more}

\subsection{The Size of the Term}

Recall that Coq (and dependently typed proof assistants in general) have \emph{terms} which serve as both programs and proofs.
The essential function of a proof checker is to verify that a given term has a given type.
We obviously cannot type-check a term in better than linear time in the size of the representation of the term.

Recall that we cannot place any hard bounds on complexity of typechecking a term, as terms as simple as \texttt{@eq\_refl bool true} proving that the boolean \texttt{true} is equal to itself can also be typechecked as proofs of arbitrarily complex decision procedures returning success.

We might reasonably hope that typechecking problems which require no interesting computation can be completed in time linear in the size of the term and its type.

However, some seemingly reasonable decisions can result in typechecking taking quadratic time in the size of the term.

Suppose we want 


\subsection{The Number of Binders}

\subsection{The Number of Abstraction Barriers}

\begin{subappendices}
    
    \section{Notes from call with Adam}
    start with a compelling example of a problem in Coq, including code
    
    end with a $\approx$1 page description of the quantitative axes of measuring performance in Coq, and effect of these axes on time and space (backsolve from here to example at beginning of chapter)
    \begin{itemize}
        \item \# of binders
        \item size of (input/output) term
        \item size of type
        \item \# of abstraction barriers
    \end{itemize}
    
    goal is to generalize over experience in multiple projects to build a performance model against which our tools can be independently verified to be improvements and to be useful
    
    the model should be independently verifiable as a theory that makes predictions about time and space usage of Coq
    
    \section{transcript from Rajee}
    
    
    Okay, it's recording. 
    
    You were saying.
    
    Was I?
    
    You were saying you don't know how to structure this section.
    
    Okay, so the the purpose of this section is to convince the reader that performance issues in proof assistants are nontrivial.  They're nontrivial in different ways----the differences between performance issues in proof assistants and performance issues in other programs is non-trivial. The way that I want to do this is by giving a bunch of examples of what performance issues in Coq look like.
    
    Do you also have to supplement with a taxonomy of issues in general? Like in other programs and then compare? Or like, how are you making the distinction? 
    
    Also is there a taxonomy of different types of performance issues, are you inventing one?
    
    I hope not? For the latter. And I don't know for the former.
    
    Okay, so my claim is that in most other programs, you pick the algorithm that you want for the job. And you have some sense of how it's going to be used and so you can pick an algorithm that. Has reasonable performance characteristics and you do a back with the envelope calculation and you're like yep if I implement the algorithm well then it will be good enough for my job. 
    
    And. Then you implement the algorithm. And then separately you don't need to understand the algorithm at all in order to performance optimize the implementation, very like you only need to understand it very superficially. You can like divide concerns between understanding and the implementation. And. To test the performance you need even less understanding. 
    
    Right and you just throw. The you need to know what sort of data gets thrown at it and you throw the data at it and you like profile the hot spots to three things getting recap them, okay, so. What? Like. Okay, so at all levels you need to know what? 
    
    Sort of tasks your program is doing what the like typical data looks like maybe that's the difference between performance and other programs and performance and call that. In proof assistance. It's quite hard to know like there isn't to standard for what the standard workload it is. Um, And then in these other programs there is the high level optimizations where you pick an algorithm and you do a back of the envelope calculation and you're like yep could the data that I use my I picked now rhythm that has the right asymptotics. 
    
    And then now you can implement it and then you can throw away all your knowledge the algorithm itself. And you can just focus on performance optimizing the implementation. Look up what the hotspots are and you optimize them. Yeah, how did the latter two play into how you do things of cock so the problem with pop is that? 
    
    You're typical data. Is proofs and math all proofs and math. And also other kinds of proofs that are like not commonly seen in the math there, isn't. Like. Okay, there's sort of a typical workload. Um, but. It's closer to the typical workload of like program compilation, but it's it's even bigger than that. 
    
    And. Usually with program compilation you like you want your compiler to be fast, but you care more about the speed of the compiled program and so. You only really need to check the asymptotics, usually. Whereas. 
    
    The. 
    
    So. In most do maintenance, you can actually. 
    
    Like you can get a meaningful sense of what the asymptotics are. In that you like look the compiler is at worst like you can get a worse overskis asymptotics. 
    
    Caulk, that is actually impossible. Because. Ah. Or like I can prove to you that the worst case running time and talk. Is exponential like no matter what you do no matter how good you make it and worse case running time is always going to be exponential as the no matter how good you make it worse is like oh because worst case because in cock yeah, oh you can. 
    
    Include you can take any. Turing machine and you can run it for a number of steps that is exponential in the length of your input. Very this is just how logarithmic like decimal encodings of numbers work and so. Unless you can solve the halting problem and like do better than just solving the whole thing problem unless you can like solve all acetic problems. 
    
    If you're going to have to run algorithms that are exponential in the size of your input. 
    
    I don't follow say that part again the beginning okay, so in most languages yeah you you're like, okay sorting a list is analog n in the length of the list. You're like good. I'm done. I don't need to worry about like sorting a list is never going to be exponential in the. 
    
    In the length of the list like this you mean constant time comparison and like you can express it in terms of how long comparison stay oh and you're like okay doing this database operation you like can actually put down a running time on it and like compiling code is a little bit harder to put a running time on but you could probably write a compiler where you're like look file or guarantees. 
    
    Like linear time quadratic time something and the one for the code. Like you're compiler for CH should not be exponential in the length of your C code. I think in fact. Well okay, so then interesting case but can you explain more about how the compiler? C should not be more than quadratic or it's not quite true and see because you can. 
    
    In the pre-processor you can code fancy things but like. For the standard use case. Um, 
    
    Like if. If you have like two unrelated blocks of code. Compiling them like shouldn't be that much harder than like compiling them separately. So compilation should be roughly linear in the length of your code. 
    
    And like okay, they're exceptions to this but they're rare. My claim is that in Coq the exceptions dominate every like when you scale things the exceptions dominate, what is what is the reason? Like how to be exceptions arise and yeah, so the the standard exception is you're like, um, So in C, you can say these computations when I compile time and now you have to run a C program. 
    
    I compile time. So you have a short C program you're like, please compute 20,000 factorial. Have compiled time. And this is like five lines of code. And then you're like, okay now please compute 40,000 factorial and you haven't changed the number of characters, but now it takes much more than twice as long. 
    
    So my claim is that in Coq. Um, this is sort of what goes on in lots of places. Why? Oh because. So much computation happens at file time. Why? 
    
    So when you run simple and call that's computation. When it creates your proof when it checks your proof when it runs your proof, that's all compile time. 
    
    It's like sort of the de facto standard is to do computation. I compile time so if you're proof becomes more and more complex. Where I don't understand how I'm using the word complex, you're encountering the exception that you're encounter where things yeah. No so you. Even when you're code is not complex, you're like hitting the exceptions but the factors are small enough that your exponential behavior or whatever doesn't doesn't show how are you using the word complex? 
    
    Informally, okay. 
    
    So what what's like a marker of like when things become when things scale weirdly and when they don't. Oh. 
    
    So you make your you run simple and it works and then you increase the size of the thing that you're running simple on and now maybe it finishes after 10 hours. 
    
    Maybe. Okay. Increasing the size, okay? What why is simple like right so you you were dealing with things with like successory of end times them yeah and like okay, so now. You like. Have a like cage long arithmetic problem and you write that down you run simple and it's fast. 
    
    And then you add two more terms to it. And now it doesn't finish after five hours what a simple actually running what's the? Magic magic. They would take me. Like two pages of text to describe what's simple as theory. Okay is that part of so is it a very complex program or what's up there like it's it's a complex algorithm with lots of heuristics, okay a philosophers. 
    
    Ah, it's like one complicated heuristic in particular okay, but it like takes the page or three to build up enough backgrounds even understand what the heuristic is for in the first place and how much of you analyze the complexities like is that analyzable or not or ah, 
    
    The problem with analyzing. The complexity of anything Coq is that everything calls everything else? 
    
    Like. 
    
    You're like, okay, I want to know how long it takes to to run simple on something and you're like okay what's simple does is it uses the definitions of things that sounds simple right and then you're like, okay, but like the definition of plus is that it's this recursive function that's like defined as an anonymous recursive function. 
    
    But when you simplify plus you get back the name plus. Right you get back plus symbol and you don't get back anonymous recursive function applied to arguments no this is what the heuristic and simplest for I see okay, so you're like, okay, so when I after I so simple unfolds things sometimes sometimes it's like nope actually. 
    
    I don't unfold this the heuristics about when it unfolds stuff when it uses definitions and when it doesn't you're complicated but then even after it use definitions now it needs to go back and it needs to use definitions and the other direction and be like, oh this anonymous function is bound to this name. 
    
    Okay, that's still fun but some of the anonymous functions take extra arguments before they get to the recursive part and so now it needs to pull those arguments out here anonymous recursive function. And in order to know whether or not. Like they can be pulled out because they can be arbitrarily complicated terms it invokes this entirely different procedure. 
    
    Which is how do you tell how do you tell whether one two things are the same when you might not know everything about the two things there might be some bits that you're like fill it in later. And this is an entirely different procedure that also doesn't have well understood complexity. 
    
    Because this one also can do arbitrary amounts of computation in the middle of of running it can use definitions arbitrarily much. And. Here even here when you fill in the holes. I think sometimes it goes back to this question of can you like is this thing down to a name or not? 
    
    And so you got this back and forth between them. I'm not actually might have only been an older versions of cock that there was this back and forth but. You'd still get this back and forth and so it gets quite tricky town allies asymptotics. It's not even clear what the what meaningful variable you can talk about asymptotics and right you're like but it's like the asymptotics are clearly. 
    
    Exponential or worse they just have to be because there's no way like when you code up programs that are exponential or worse or like when you could have programs that are that are exponential. It has to run the whole program and there's no way around that. So the worst case behavior is is just inherits it from the program. 
    
    Oh. And so it's not clear how to analyze the performance of this thing. Separately from the performance of the data that you're running and normally you don't think about performance of data meaningful characteristic, but it's sort of. It's like entwined in almost all of how cock works. 
    
    Something like oh you first point at this thing that you can do for other programs and you point at why you can't do it for cock and then you make a case for why that's okay or like there's like a couple questions about like why this would even be the case or like like do it you could like punton be like well, this is what we have. 
    
    Adam said I should not do that, okay. What should you do then? Um, 
    
    I should read the responses the email that I sent to call Club that are like, here's why we have dependent types, okay, like it's kind of hard to do math in some sense without all this power. Oh. 
    
    But this isn't exactly what I want this section to look like what do you like the first section? I will already have introduced some decent trunk of this. I think this is chapter 2. There. 
    
    Yeah. I feel like the structure is a little bit mixed up or something. Maybe chapter one is introducing the different design issues in chapter two is about here's performance things that go up with them. But design issues, so I've been focusing on the spit that is like, Ah. All of the. 
    
    Like worst case exponential behavior, there's no way around that also it's hard to analyze the performance. 
    
    That is. Sort of one large class of things but there's another class of things that I haven't talked about at all yet. That originates from a different. Difficulty source. 
    
    In one way of looking at where the other class of problems originates from is that. You want to have only a small. Chunk of program that you trust. And you want to have a larger part that even untrust. What is trust me here, oh. That if there is a bug in it. 
    
    Then other things happen, okay, right so there's a lot of stuff that you write and if there's bug enough. And you're like, oh there's an error message. There's a smaller chunk of the program that if there is a bug in it then you're like, Well, I guess I proved that one. 
    
    = 0. And so you won't only a small chunk of code where if you have bugs in it and you get proofs that one = 0. And. Sort of the standard way to do this is that you. Dress the language like gives sort of trace of what it's of the like smaller steps that you know how to do in the small language. 
    
    That the that's the like smaller kernel can check. 
    
    The problem is that. 
    
    The sort of obvious mathematical way of encoding this is frequently quadratic. 
    
    Oh coding what is quadratic? The steps steps off. The primitive steps that say that this is a valid beer. Okay. And that's quadratic. Oh, it's quadratic. Sort of because. 
    
    There isn't very much support for. Sharing. For being like reuse these steps. These sort of need to. Give the steps from scratch each time or jump through a lot of hoops. 
    
    Like. If you're not careful then there's no sharing. No sharing. Oh. So like if you just write the function that computes the Fibonacci numbers recursively. You. Duplicate a lot of work. Because like to compute like 5 you're like 5 plus 5 3, but to compute 4, you're like 5 3 plus 5 2, and now you can keep 3 twice. 
    
    Use of don't you use like tricks like memoization. Great, but if you want the, 
    
    Like most programming languages don't have. Of memorization. Yeah. Like. It might be interesting to have a programming language that auto memoizes everything. I expect there you would run into other performance issues. Okay. But in Coq again, you don't you don't have I mean, there is some attempts at doing something that's like auto memorization but. 
    
    Um, It's sort of not built in or like it's a how does the autopark matter can't you have? Introduced memoization yourself. Yeah, you can yeah, um, but it's. It's not the way that most people wait right proofs because you write the simplest thing that works first. Sure. Okay, so let's say you go introduce memorization problem is that. 
    
    That is still not good enough because. Ah. There are various bits of the cocky ego system that are something like cubic in the number of things that you've memorized. What do you mean by that? Like you want to memorize and things like. If you do this interactively. 
    
    Like if you. Want a system that lets you do this you need to make sure that all of your primitives behave nicely like have predictable performance with respect to this variable. I've already told you how hard it is to get ridicule performance of all the various things and the sort of naive obvious ways implement some of the bits of call. 
    
    Give you cubic performance and. How many things you've memorized? 
    
    And like quite possibly, maybe you could design a proof of system. Carefully if you had a like theoretical foundation. Or would like a global picture that doesn't have this issue. But otherwise it's sort of feels like you're playing guacamole. We're like you end up doing various trade officers something. 
    
    So then. 
    
    Ah. 
    
    Well, that's that's sort of where this chapter ends. The well okay not quite there's okay, so then the rest of the pieces. Are way too, maybe I don't remember if I put this at the end of this chapter. Maybe I'll put it at the end of it but no outline ish but not very well. 
    
    So then. The upcoming parts of the thesis are going to be about how to. How to work around these issues or how to solve them, so it's not. So one path you could take is like redesign your proof assistant like have a picture of how to do this memorization correctly that's still only solves half the problem. 
    
    I'm going to talk about first how to solve the other half of the problem why. What do you mean why why are you know, first talk about that? I mean, I could talk about it second no. I meant like is there like a utility doing that? It's a solution to the problem, okay? 
    
    You're like, why are you writing this thesis? Jason the the question doesn't feel like that he feels like I don't understand. Is the, The fact that you hit this worst case exponential behavior everywhere accidentally. How did how did I want to say how to not how to design your programs so that you stop hitting it everywhere accidentally, okay? 
    
    And so you're chunking the problem to it yes, okay, that's one of the bits yeah the other bit is this. Um, 
    
    This like. You're like okay, we want separation between trusted and untrusted yeah, but if you don't the like mathematically sound obvious way of doing this gives you quadratic. Blow up in various places and you're like, okay but memoization and. Then you try to do that and you're like, oh the things that made sense to do in various parts of the system are now real bad. 
    
    Along. 
    
    And. So. You could redesign your whole purpose system to like support this sort of analyzation thing. I'll come back to that in the conclusion as future work, but I'm instead of going for a different solution. The solution is that you? Um, 
    
    So this came about because we were trying to do things in the untrusted part and get the trusted part to check them. You could just put everything in the trusted card. Because you don't trust it well, but we can prove it. Like what do you mean, so we use the untrust part to like build? 
    
    New things that we trust and the reason we trust them is because we've proved them and the trusted part checked our group, okay, so now we have these new things that we built and now they live entirely like the parts of the system we need to interact with to deal with them as we scale the input is only the trusted part we don't need to do anything also the entrusted part as we scale input. 
    
    I don't follow why they were previously in the untrusted part what have you not done, but they had to be mental support and what have you introduced okay, so say you want to prove. A conjunction with like. A hundred different propositions, okay? There's. Sort of three ways to do this you can be like well I know how to prove conjunctions. 
    
    I proved conjunctions by proving the two parts of them and you interface through the untrusted part and you're like untrusted part, please let me prove two parts of the conjunction you do this and if you're not careful about my ovulation, you now have a hundred squared costs oops, okay? 
    
    So then you're like, Oh. So you could instead do some sort of fancy numberization then. But then you run into the other problem. Or you could say well. I'm going to prove that I'm gonna like write a description language of all the different kinds of propositions that I could write down and then I'm going to write a thing that just computes whether the proposition is true or false and then I'm going to prove that like the thing that computes whether it's true or false behaves correctly, but if it's as true in the proposition can in fact be proven, And then you're like, okay, here's my description of the conjunction. 
    
    Check true false. Look etc. 
    
    And the like check true false. Is all done in the like the trusted part knows how to run things. So it can run your check through false. 
    
    Can you distinguish between the trusted and untrusted part and what you're introduced again. Oh so the trusted part knows how to run knows how to like make use of definitions and run things. And it knows how to like. 
    
    Check everything is good, yeah. I didn't notice how to apply the like basic rules of types. Cool and the untrusted part, let's you like build proofs it by bit. Do arbitrarily fancy things. What what what were you using how's the trust important unpressed support like what was what were what was the structure serving as before and why are you making everything trusted and what what so the reason you want? 
    
    A small you want a small trusted perk yeah because. Um, You want it to be the case that as users add more features. They any bugs they introduce don't let you prove when equals zero. Oh. So the untrusted part is sort of convenient. So if you make things trust to when you're. 
    
    And the untrusted part you it's also like faster to write things in it because you don't need to prove that the thing that you're doing is right you're just like do this check that it's good. Check that as good as the same as private tray no truck that it's good is the thing that happens when you write QED and it's like make sure my proof is actually valid. 
    
    So why are you suddenly comfortable with making more things trusted whatever I'm not no. I'm not it's not that I'm making more things trust them yeah basically what I'm doing is I'm saying things that you were doing untrusted lands and that you were just later saying once I'm done. 
    
    I've generated the thing to check instead I'm saying, That what gets checked is the code implementing the untrusted bit. 
    
    Say that again. So different words no same word, so previously the untrusted bit generates a thing that gets checked but the trusted bit yeah instead I'm saying. I'm going to write code for the untrusted bit and that's what gets checked. And then once it's checked then it's good. I've shown that it's always correct and so I can just use it and the thing that it generates doesn't need to be checked. 
    
    This is how this is how gender they works. What do you mean? Like how does the trusted bit generally interact with the trust of the untrusted bit? The untrusted bit generates a certificate or the proof object or something and it hands it to the trusted bit and says check that it's good. 
    
    Okay. Where's now it's? Just getting checked with the whole things. So instead what I do is I say I managed to convince the trusted bit that any certificate that this bit of code could possibly generate. Is good. How do you do that? The first proof so what? Ah. There will be a whole chapter devoted to this. 
    
    Okay. But. Ah. 
    
    Okay, okay, here's an example. Suppose you want to prove that one number is less than or equal to another number natural numbers. Here my rules for less than or equal. X is less than equal X. If A is less than equal B then. A is less than equal success or P. 
    
    Okay, so now I want to prove that 0 is less than equal 100. And to have a hundred of the successor objects. Yep followed by one of the zero less than equals zero. Yeah, okay. What do you want to do instead? And then the current will check that this large object is correct. 
    
    Yeah. Here's the thing. I can do inside. 
    
    I define subtraction where I say that subtract. A minus B as long as both of them have a successor and you keep peeling the successors off. If A becomes zero, then the difference is zero, you just don't emit negative numbers negatives just go to zero and otherwise if the right thing becomes zero then you just submit first thing anything minus zero is itself. 
    
    And then I say, I can prove that. If. Be line to say sorry if a minus b is zero then a is less than equal b. Okay, now all I need to do is I need to compute. A minus b or sorry, yeah a minus b my check is a minus b zero. 
    
    And the trusted part knows how to compute a minus b. And the trusted partners how to check if it's zero. And I've written a proof that if a minus b is zero then a less than equal b. And so the trusted person doesn't need to expand out that certificate it doesn't need to know the certificate that a less than equal b yeah because it has a proof that it will always have a certificate. 
    
    And that's that's how this bit works. Well. So you want to introduce more? Than that. Yeah, I write functions that are like this function generates certificates for this kind of problem. Yeah. And then the kernel checks that it in fact always generates certificates. Yeah. And then I never need to run it. 
    
    Cool. And I can just run something simpler that doesn't generate quadratically sized objects. And also doesn't need to reificate or memoization. 
    
    Okay. What was this chapter again? The where I'm solving the second half of the problem or the entire thing we're talking about. The current chapter or whatever. I mean, the the chapter that I wanted to outline was just the second introduction chapter. That's like here the performance here's what performance issues look like. 
    
    Yes, but the chapter what I've been talking about is the chapters on how we solve the performance issue of that arises from having a separation between. Trusted and untrusted vets. 
    
    So going back to the chapter that you wanted to outline go. 
    
    Yeah. You said you want to have explanations for why cop has been designed in the specific way that a generates a performance issue has different room performance issues that are programmed. You have those explanations? 
    
    I. I feel like So there is another thing that I said at all yet, which is that. So before I was talking about how in a lot of languages. You can like separate understanding the algorithm from performance optimizing it. Uh-huh. And. Also separate that from. 
    
    Well and maybe the reason you can do this is because you have a good representative set of data unlike a small number of things that you need awesometotic behavior with respect to Magen say that again, I got lost. Oh, okay, so standard performance optimization. Same words is fine. Oh. 
    
    Unless you think it's not. I want to try different way and soon food. Okay. Yeah, so standard performance optimization you like generate your good set of data that exercises various axes that you care about. And then you're like, let me pick a good algorithm. And then you're like, let me write a good implementation and these all three steps are separate. 
    
    And my claim is that in Coq. The first step is an open research question, what good representative said of data, okay? I mean, whatever food representative said of data is this so like for sorting lists, you're like, let me pick a bunch of lists that if I can sort these. 
    
    In if I like can sort everything in the set of data that like I'm like can maybe it's an infinite set, maybe it's finite set but it's like a well-modeled set. And you're like if I can if I perform well on this set then I'll basically perform well on everything. 
    
    I'm like for sorting this is totally reasonable you're like okay what matters is the ordering of the list and the length of the list and whether I'm sorting lists of strings or lists of integers or lists of colors that doesn't matter at all. And also it's like, Like it's a not only is it a well described set you also and like not only to know which variations you don't care about you're like look if I can sort of vary the length of the list mostly independently from what how I vary the ordering within the list, oh. 
    
    And like this is a good exercise of sorting algorithms. And then Coq it's an open research question. I think it's like so big that it's it's hard to find this. Like it's not clear what the relevant set of data is. 
    
    For any of the proofs that you want to run. I know the problem is that the dataset is the proofs that you want to run. Okay and you're trying to sell elements of the data set are the proofs that you want to run. And you're like, what's a representative set of all proofs that you might want to run? 
    
    It's hard yeah. And like obviously they're going and like I can prove that you can't do well you could try to just say all sets in the same way you're like all lists and like when you say all this you're like great I can give you and login and then I say all proofs and you're like great I can give you exponential behavior you're like, no, please no. 
    
    And so it's hard to see how to do better right, oh. And so because you don't have this that means that your other two steps are coupled and require a lot of like nuanced understanding like you need to understand the algorithm understand what the relevant what the possible relevant axes are understand the implementation of the algorithm and you need to do all of this simultaneously. 
    
    That makes it hard. 
    
    And I want to give a couple of other examples where? Like that show this sort of coupling. Yeah, that seems good. Do you have those examples? Who do I talk about? Oh. Later. Okay. Here we go. Is this been transcribing? Yes that cool. Forty-five minutes. Sweet. No, redone. Okay.
    
    
    \section{transcript from Adam}
    You're like performance issues and look like this is what makes them hard and I want to like, It'll probably it's I should probably like say the things that I'm thinking of including in this section because you'll probably tell me that some of them are not good to include. So I'm thinking of including like both deep and mundane issues and one of the things that I want to point out is that it's hard to figure out like what the performance roots are. 
    
    And. One example that I want to pull that seems like a good example to give here is like a like one of the examples in the palette of performance. Troubles is the quadratic behavior of CBV on the number of finders. And. The ultimate issue here is the cock uses NDE to do to implement CVB and the way that they move closures under binders is they add a thunk that says bump all the point indices by one. 
    
    And so if you move it under and binders you add unthanks that add one to all the different indices you mentioned one of these design choices in Coq. I think you will be good to do your best to explain why it's not terrible any that you should finish ever to try. 
    
    Oh interesting. I feel like. 
    
    So. Leaning on premature premature optimization is the root of all evil it is this seems like a fairly easy way to implement bumping the brain indices. And if you expect most functions to have order 10-ish binders, then you don't run into any problems and you can see this by the fact that CPV is not usually very slow, okay, yeah, I'm not necessarily trying to pull out these explanations from you now to apply things to see that we really want to try in this paper and this document and then say the next version we submit of our rewriter paper not to just present it as usual and into us by the gods we have to deal with that we want to explain why this is. 
    
    The tool we're connected to is somewhat representative of the community's knowledge about how to build a good privilege system sure. Oh. Yeah. So like this to to finish up the CBV example, like the algorithm that they have tends to work, it's tends to be fast enough and even catch this. 
    
    Performance issue either you need to be the one who wrote the code in the first place or you need to be actually playing with hundreds of binders. To notice it. To diagnose it. You need to understand the algorithm in part because all of Coq is like a big mutual recursive lock and soon call traces turned always give you relevant data. 
    
    And also because you're implementing mathematical algorithms that are non-trivial. And because it's I think CBV is not. In the trusted code base. But. You could imagine it's possible that lazy has the same issue. I'm not sure if it does it might. And if it does then it is part of the trusted code base. 
    
    And so then you can you need to be careful when you're optimizing because you still need to trust the resulting code. And so you have this. Confluence of a bunch of different restrictions that they hit hard to catch and diagnose and fix the performance issues, yeah. Even when they're Monday and like this, that sounds okay. 
    
    Yeah. 
    
    And so another another example from this is that oh. Building proof terms step by step every time you need to create a new. Evar for the goal and. When you create the new Eve already you need to relate it to the you need to relate it's context the only verse context. 
    
    And it turns out that this is something like linear in the number of things in the context. And this is usually fine because your context are usually not more than a couple hundred things big and you're it's usually the case that the individual steps in your approved take more time than the like transition between steps in the proof. 
    
    But this means that if you want to introduce say a couple thousand variables and you do it one of the time you're now something like. Ah, like quadratic. In the republic. I can't remember if it's quadratic or cubic there might be another linear factor somewhere in there and call okay, but you have like that performance in the number of variables you're trying to introduce for this like very simple tactic and this one is sort of less obvious how to solve it. 
    
    Sure. And say like that's that's another thing that's hard. Oh and so I think in this section now probably go digging up a couple other performance issues that unlike pain this picture of like widely ranging performance issues in terms of how hard they are to solve but the commonality is that it's. 
    
    Complicated to tell ahead of time like. It's like hard to find the performance issue and know how hard it'll be to solve, okay? Oh so that's how I want to sort of end the introduction. Makes sense to me. It does sound like you've been pretty clear you would go there so I figured out remaining time that's more useful to talk about earlier parts that are fuzzier yep sounds good. 
    
    
    \section{transcript from Rajee}
The first bit is about what makes performance in proof assistants and Coq challenging. In most languages, or in most most areas, the performance story is that you do a thing and you make it fast on your toy examples and you do larger examples and hopefully it's still fast and then you do much larger examples and maybe it gets a bit slow and maybe for the largest of examples you need to like let it run overnight or something. 

The experience in Coq frequently is that you do something and you get it to work on your toy examples and it's fast and you do slightly larger examples and it's still kind of fast. But it's noticeable. And then you do somewhat larger examples and now it's slow and annoying but still okay.  And then you make your examples a little bit longer and \ldots{} maybe it won't finish for a week. Or longer.  I ran into a case where I had a thing and it worked and for the larger examples it would maybe take an hour to finish, and then there were some examples that did not finish after about 900 hours.  Not for any deep reason, though.  Just because we weren't careful enough in setting up conversion checks, and so it would take a really long time to just run \texttt{abstract reflexivity}.

And this is sort of common in Coq where you'll get something and it'll work and if it works then you just leave it and then you try to scale your thing and now suddenly you've hit one of many instances of quadratic or exponential behavior and now it's unclear if it'll finish in a year.

This is performance of compile time by the way, this is like performance of how long it takes to check proofs.  If you're writing programs and you want to prove things about them, how your programs perform is subject to the same optimization techniques as other languages, but the issue with using those techniques for how your proofs perform is that usually, for most languages either a library is slow, and you know it's slow and it has known performance characteristics; or the code that you wrote was slow. Here, there are many many bits of the compiler that usually just work and most the time you don't need to care about how they work; they just work \ldots{} until they don't.

So why don't they work?  There's various reasons.  One of them is that the Coq dev team is understaffed. Here's a performance issue that has not yet been solved. I went to the Coq dev team and I was like ``this thing is quadratic in the number of arguments to the function; what you're doing underneath the function should not be quadratic and how many arguments there are to the function.''  There were actually two different parts and I was like ``both of these things are quadratic in the number of arguments the function''.  The Coq dev team looked at it and was like, ``oh there's different sources of quadraticness in the two different things and the reason for the worse source of quadraticness was that when you refer to variables, you do it by number. And so when you have a closure, when you have a function object that is like waiting for extra arguments, but it refers to some of the arguments that exist, and you move it beneath more binders, you need to update all the numbers.  So you're doing it a second time. But the way that they update all the numbers is every time they move beneath one binder they have a thunk that says ``when you go to look at this function bump all the numbers by one.'' And then you move it under another binder and you add another thunk that says ``when you go to look at this function bump all the numbers by one'' so when you put it under binders $n$ times, you say plus one plus on plus one plus one and then it's quadratic. 

And no one realizes this because this part of the system is pretty fast and no one was dealing with functions that had a hundred or a thousand arguments. And so this part of the system went unstressed.  In order to find that this is the issue you need to know the slowness, you need to know what it's slow in, you need to know what the algorithm is doing, and for complicated software that relies on complicated math, this is not always trivial. If it's in a part of the code base that needs to be trusted then if you make any changes, you need to get them correct or else your trust story is bad. 

So there's this like cornucopia issues that makes like improving the performance in these sorts of systems a bit tricky. 

And this part of my thesis. I will like, point to a couple of others, a sort of palette of examples of slowness issues that come in come up, and talk that are sort of hard to tell where exactly they come from.

Okay, so that's the like introduction palate of slowness.

\todo{this chapter}

\begin{itemize}
\item Descriptions of the order in which things fail?
\end{itemize}
\end{subappendices}