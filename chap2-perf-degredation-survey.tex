\chapter{The Performance Landscape in Type-Theoretic Proof Assistants} \label{ch:perf-failures}
\section{The Story}\label{sec:perf-failures:story}
\todo{come up with a better name for this section}
The purpose of this chapter is to convince the reader that the issue of performance in proof assistants is non-trivial in ways that differ from performance bottlenecks in non-dependently-typed languages.
I intend to do this by first sketching out what I see as the main difference between performance issues in dependently-typed proof assistants vs performance issues in other languages, and then supporting this claim with a palette of real performance issues that have arisen in Coq.

The widespread commonsense in performance engineering\cite{commonsense-perf-engineering-order-of-operations} is that good performance optimization happens in a particular order:
there is no use micro-optimizing code if you are implementing an algorithm with unacceptable performance characteristics; imagine trying to optimize the pseudorandom number generator used in bogosort~\cite{Sorting2007Gruber}, for example.\footnote{Bogosort, whose name is a portmanteau of the words bogus and sort~\cite{bogosort-name}, sorts a list by randomly permuting the list over and over until it is sorted.}
Similarly, there is no use trying to find or create a better algorithm if the problem you're solving is more complicated than it needs to be; consider, for example, the difference between ray tracers and physics simulators.
Ray tracers determine what objects can be seen from a given point essentially by drawing lines from the viewpoint to the object and seeing if it passes through any other object ``in front of'' it.
Alternatively, one could provide a source of light waves and simulate the physical interaction of light with the various objects, to determine what images remain when the light arrives at a particular point.
There's no use trying to find an efficient algorithm for simulating quantum electrodynamics, though, if all you need to know is ``which parts of which objects need to be drawn on the screen?''

One essential ingredient to allowing this division of concerns---between specifying the problem, picking an efficient algorithm, and optimizing the implementation of the algorithm---is knowledge of what a typical set of input looks like, and what the scope looks like.
In Coq, and other dependently-typed proof assistants, this ingredient is missing.
When sorting a list, we know that the length of the list and the initial ordering matter; for sorting algorithms that work for sorting lists with any type of elements, it generally doesn't matter, though, whether we're sorting a list of integers or colors or names.
Furthermore, randomized datasets tend to be reasonably representative for list ordering, though we may also care about some special cases, such as already-sorted lists, nearly sorted lists, and lists in reverse-sorted order.
We can say that sorting is always possible in $\mathcal O(n\log n)$ time, and that's a pretty good starting point.

In proof assistants, the domain is much larger: in theory, we want to be able to check any proof anyone might write.
Furthermore, in dependently typed proof assistants, the worst-case behavior is effectively unbounded, because any provably terminating computation can be run at typechecking time.
\todo{cite https://github.com/coq/coq/issues/12200}

In fact, this issue already arises for compilers of mainstream programming languages.
\todo{literature search on perf of compile time in mainstream compilers?}
The C++ language, for example, has \texttt{constexpr} constructions that allow running arbitrary computation at compile-time, and it's well-known that C++ templates can incur a large compile-time performance overhead.\todo{cite?}
However, I claim that, in most languages, even as you scale your program, these performance issues are the exception rather than the rule.
Most code written in C or C++ does not hit unbounded compile-time performance bottlenecks.
Generally if you write code that compiles in a reasonable amount of time, as you scale up your codebase, your compile time will slowly creep up as well.

In Coq, however, the scaling story is very different.
Frequently, users will cobble together code that works to prove a toy version of some theorem, or to verify a toy version of some program.
By virtue of the fact that humans are impatient, the code will execute in reasonable time on the toy version.
The user will then apply the same proof technique on a slightly larger example, and the proof-checking time will often be pretty similar.
After scaling the input size a bit more, the proof-checking time will be noticeably slow---maybe it now takes a couple of minutes.
Scaling the input just a tiny bit more, though, will result in the compiler not finishing even if you let it run for a day or more.
This is what working in an exponential performance domain is like.

\label{sec:fiat-crypto-codegen-numbers}%
To put numbers on this, a project I was working on\todo{cite fiat-crypto} involved generating C code to do arithmetic on very large numbers.
The code generation was parameterized on the number of machine words needed to represent a single big integer.
Our smallest toy example used two machine words; our largest---slightly unrealistic---example used 17.
The smallest toy example---two machine words---took about 14 seconds.
Based on the the compile-time performance of about a hundred examples, we expect the largest example---17 machine words---would have taken over four thousand \emph{millennia}!
See \autoref{fig:timing-montgomery-fesub-before-user} and \autoref{fig:timing-montgomery-fesub-before-user-log}.
(Our primary non-toy test example used four machine words and took just under a minute; the biggest realistic example we were targeting was twice that size, at eight machine words, and took about 20 hours.)
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{fiat-crypto-perf-logs/montgomery-fesub-before.txt}
    }
    \begin{axis}[xlabel=\# limbs,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        xtick distance=1,
        ylabel style={
            yshift = {width("10")}
        }]
        \addplot[only marks,color=black] table[x=nlimbs,y=user]{fiat-crypto-perf-logs/montgomery-fesub-before.txt};
        \addplotexponentialregression[no markers, black][x=nlimbs,y=user,a=0.0001,b=2.5]{fiat-crypto-perf-logs/montgomery-fesub-before.txt};
        \addlegendentry{synthesis}
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona} e^{\pgfmathprintnumber{\pgfplotstableregressionb} (\text{\# limbs})} $
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of synthesizing subtraction} \label{fig:timing-montgomery-fesub-before-user}
\end{figure*}
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{fiat-crypto-perf-logs/montgomery-fesub-before.txt}
    }
    \begin{axis}[xlabel=\# limbs,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        ymode=log,
        xtick distance=1]
        \addplot[only marks,color=black] table[x=nlimbs,y=user]{fiat-crypto-perf-logs/montgomery-fesub-before.txt};
        \addplotexponentialregression[no markers, black][x=nlimbs,y=user,a=0.0001,b=2.5]{fiat-crypto-perf-logs/montgomery-fesub-before.txt};
        \addlegendentry{synthesis}
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona} e^{\pgfmathprintnumber{\pgfplotstableregressionb} x} $
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of synthesizing subtraction ($\log$-scale)} \label{fig:timing-montgomery-fesub-before-user-log}
\end{figure*}

Maybe, you might ask, were we generating unreasonable amounts of code?
Each example using $n$ machine words generated $3n$ lines of code.
Furthermore, the actual code generation took less than 0.002\% of the total time on the largest examples we tested (just 14 seconds out of about 211 hours).
How can this be?

Our method involved two steps: first generate the code, then check that the generated code matches with what comes out of the verified code generator.
This may seem a bit silly, but this is actually somewhat common; if you have a theorem that says ``any code that comes out of this code generator satisfies this property'', you need a proof that the code you feed into the theorem actually came out of the specified code generator, and the easiest way to prove this is, roughly, to tell the proof assistant to just check that fact for you.
(It's possible to be more careful and not do the work twice, but this often makes the code a bit harder to read and understand, and is oftentimes pointless; premature optimization is the root of all evil, as they say.)
Furthermore, because you often don't want to fully compute results when checking that two things are equal---just imagine having to compute the factorial of $1000$ just to check that $1000!$ is equal to itself---the default method for checking that the code came out of the code generator is different from the method we used to compute the code in the first place.

The fix itself is quite simple, only 21 characters long.\footnote{\texttt{Strategy 1 [Let\_In].} for those who are curious.}
However, tracking down this solution was quite involved, requiring the following pieces:
\begin{enumerate}
  \item
    A good profiling tool for proof scripts (see \autoref{sec:ltac-prof}).
    This is a standard component of a performance engineer's toolkit, but when I started my PhD, there was no adequate profiling infrastructure for Coq.
    While such a tool is essential for performance engineering in all domains, what's unusual about dependently-typed proof assistants, I claim, is that essentially \emph{every} codebase that needs to scale runs into performance issues, and furthermore these issues are frequently total blockers for development because so many of them are exponential in nature.
  \item
    Understanding the details of how Coq works under-the-hood.
    Conversion, the ability to check if two types or terms are the same, is one of the core components of any dependently-typed proof assistant.
    Understanding the details of how conversion works is generally not something users of a proof assistant want to worry about; it's like asking C programmers to keep in mind the size of \texttt{gcc}'s maximum nesting level for \texttt{\#include}'d files\footnote{It's 200, for those who are curious.\todo{cite \url{https://gcc.gnu.org/onlinedocs/gcc-7.5.0/cpp/Implementation-limits.html}}} when writing basic programs.
    It's certainly something that advanced users need to be aware of, but it's not something that comes up frequently.
  \item
    Being able to run the proof assistant in your head.
    When I looked at the conversion problem, I knew immediately what the most likely cause of the performance issue was.
    But this is because I've managed to internalize most of how Coq runs in my head.
    This might seem reasonable at a glance; one expects to have to understand the system being optimized in order to optimize it.
    But I've managed to learn the details of what Coq is doing---including performance characteristics---basically without having to read the source code at all!
    This is akin to, say, being able to learn how \texttt{gcc} represents various bits of C code, what transformations it does in what order, and what performance characteristics these transformations have, just from using \texttt{gcc} to compile C code and reading the error messages it gives you.
    These are details that should not need to be exposed to the user, but because dependent type theory is so complicated---complicated enough that it's generally assumed that users will get \emph{line-by-line interactive feedback from the compiler} while developing, the numerous design decisions and seemingly reasonable defaults and heuristics lead to subtle performance issues.
    Note, furthermore, that this performance issue is essentially about the algorithm used to implement conversion, and is not even sensible when only talking about only the spec of what it means for two terms to be convertible.
    \todo{incorporate Andres' suggestions:}
    you running the typechecker in your head is essentially the statement that if the entire implementation is part of the spec, it is possible to engineer better, and something close to this has been necessary in practice.
    the research direction you are advocating is finding a simpler performance-aware spec (perhaps by moving around interfaces or etc; my thought is that maybe we just want to get rid of the kernel and trust the proof engine).
  \item
    Knowing how to tweak the built-in defaults for parts of the system which most users expect to be able to treat as black-boxes.
\end{enumerate}

Note that even after this fix, the performance is \emph{still} exponential!
However, the performance is good enough that we deemed it not currently worth digging into the profile to understand the remaining bottlenecks.
See \autoref{fig:timing-montgomery-fesub-after-user} and \autoref{fig:timing-montgomery-fesub-after-user-log}.
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{fiat-crypto-perf-logs/montgomery-fesub-after.txt}
    }
    \begin{axis}[xlabel=\# limbs,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        xtick distance=1,
        ylabel style={
            yshift = {width("0")}
        }]
        \addplot[only marks,color=black] table[x=nlimbs,y=user]{fiat-crypto-perf-logs/montgomery-fesub-after.txt};
        \addplotexponentialregression[no markers, black][x=nlimbs,y=user,a=10,b=0.2]{fiat-crypto-perf-logs/montgomery-fesub-after.txt};
        \addlegendentry{synthesis}
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona} e^{\pgfmathprintnumber{\pgfplotstableregressionb} (\text{\# limbs})} $
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of synthesizing subtraction after fixing the bottleneck} \label{fig:timing-montgomery-fesub-after-user}
\end{figure*}
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{fiat-crypto-perf-logs/montgomery-fesub-after.txt}
    }
    \begin{axis}[xlabel=\# limbs,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        ymode=log,
        xtick distance=1,
        ylabel style={
            yshift = {width("0")}
        }]
        \addplot[only marks,color=black] table[x=nlimbs,y=user]{fiat-crypto-perf-logs/montgomery-fesub-after.txt};
        \addplotexponentialregression[no markers, black][x=nlimbs,y=user,a=10,b=0.2]{fiat-crypto-perf-logs/montgomery-fesub-after.txt};
        \addlegendentry{synthesis}
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona} e^{\pgfmathprintnumber{\pgfplotstableregressionb} x} $
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of synthesizing subtraction after fixing the bottleneck ($\log$-scale)} \label{fig:timing-montgomery-fesub-after-user-log}
\end{figure*}

\todo{Some sort of summary of argument-so-far here}

To finish off the argument about slowness in dependently-typed proof assistants, I want to present four axes of performance bottlenecks.
These axes are by no means exhaustive, but, in my experience, most interesting performance bottlenecks scale as a super-linear factor of one or more of these axes.

\section*{Misc Fragments}
\todo{Find a place for this (h/t conversation with Andres)}:  because we have a kernel and a proof engine on top of it, you need to simultaneously optimize the kernel and the proof engine to see performance improvements; if the kernel API doesn't give you good enough performance on primitives, then there's no hope to optimizing the proof engine, but at the same time if the proof engine is not optimized right, improvements in the performance of the kernel API don't have noticeable impact.

\todo{find a place for this:}
In many domains, good performance optimization can be done locally.
It's rarely the case that disparate parts of the codebase must be simultaneously optimized to see any performance improvement.
However, in proof assistants satisfying the de Bruijn criterion, there are many seemingly reasonable implementation choices that can be made for the kernel which make performance-optimizing the proof engine next to impossible.
Worse, if performance optimization is done incrementally, to avoid needless premature optimization, then it can be the case that performance-optimizing the kernel has effectively no visible impact; the most efficient proof engine design for the slower kernel might be inefficient in ways that prevent optimizations in the kernel from showing up in actual use cases, because simple proof engine implementations tend to avoid the performance bottlenecks of the kernel while simultaneously shadowing them with bottlenecks with similar performance characteristics.

\todo{incorporate Andres' suggestions}
I like the last two sentences.
I would instead lead with something along the lines of ``in many domains, the performance challenges have been studied and understood, resulting in useful decompositions of the problem into subtasks that can be optimized independently.''
``in proof assistants, it doesn't look like anyone has even tried'' :P.
but e g signal processing was a huge mess too before the fast Fourier transform.
coq abstractions are mostly accidents of history.
no other system has a clear performance-conscious story for how these interfaces should be designed either.

\section{The Four Axes}\label{sec:perf-axes}
I now present four major axes of performance.
These are not comprehensive, but after extensive experience with Coq, most performance bottlenecks scaled super-linearly as a function of at least one of these axes.
\todo{introduce this section better}

\subsection{The Size of the Type} \label{sec:perf-axis:size-of-type}  \label{sec:quadratic-conj-certificate}

We start with one of the simplest axes.

Suppose we want to prove a conjunction of $n$ things, say, $\texttt{True} \wedge \texttt{True} \wedge \cdots \wedge \texttt{True}$.
For such a simple theorem, we want the size of the proof, and the time- and memory- complexity of checking it, to be linear in $n$.

Recall from \autoref{sec:debruijn-criterion} that we want a separation between the small trusted part of the proof assistant and the larger untrusted part.
The untrusted part generates certificates, which in dependently typed proof assistants are called terms, which the trusted part, the kernel, checks.

\todo{Mention possibility of not building proof terms at all somewhere}
Andrew Appel said via private correspondence on May 7, 2020, 7:39 PM:
\begin{quotation}
There's some work on typechecking LF, in the Twelf system, where there can be performance bottlenecks if you're not careful.

The most glaringly obvious performance bottleneck in Coq is that it builds proof terms, when one should really use the futuristic technique of using data abstraction, in the type system, to distinguish ``proposition'' from ``theorem''; as done in that state-of-the-art system, Edinburgh LCF.  And presumably HOL, HOL light, Isabelle/HOL, etc.
\end{quotation}
\todo{mention the issue that LCF approach does not fully satisfy de Bruijn criterion, does not protect against plugins and Obj.magic}
\todo{share reasoning of ppedrot from \textcite{NewCoqTactics2016Pedrot}}

The obvious certificate to prove a conjunction $A \wedge B$ is to hold a certificate $a$ proving $A$ and a certificate $b$ proving $B$.
In Coq, this certificate is called \texttt{conj} and it takes four parameters: $A$, $B$, $a : A$, and $b : B$.
Perhaps you can already spot the problem.

To prove a conjunction of $n$ things, we end up repeating the type $n$ times in the certificate, resulting in a term that is quadratic in the size of the type.
We see in \autoref{fig:timing-conj-True-repeat-constructor} the time it takes to do this in Coq's tactic mode via \mintinline{coq}{repeat constructor}.
If we are careful to construct the certificate manually without duplicating work, we see that it takes linear time for Coq to build the certificate and quadratic time for Coq to check the certificate; see \autoref{fig:timing-conj-True-ltac2}.
\todo{improve data collection on perf test}
\todo{make a note about more complicated types causing scaling factors to be worse, and not just impacting the leaves}

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/conj-True-repeat-constructor.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[x=param-n,y=repeat-constructor-user]{performance-experiments/conj-True-repeat-constructor.txt};
        \addlegendentry{\mintinline{coq}{repeat constructor}}
        \addplotquadraticregression[no markers, black][x=param-n,y=repeat-constructor-user]{performance-experiments/conj-True-repeat-constructor.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}n^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}n
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of \mintinline{coq}{repeat constructor} to prove a conjunction of $n$ \mintinline{coq}{True}s} \label{fig:timing-conj-True-repeat-constructor}
\end{figure*}

\begin{figure*}
\beginTikzpictureStamped{
    \einput{performance-experiments/conj-True-ltac2.txt}
}
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=blue] table[x=param-n,y=typecheck-user]{performance-experiments/conj-True-ltac2.txt};
        \addlegendentry{typecheck}
        \addplotquadraticregression[no markers, blue][x=param-n,y=typecheck-user]{performance-experiments/conj-True-ltac2.txt};
        \edef\tca{\pgfplotstableregressiona}
        \edef\tcb{\pgfplotstableregressionb}
        \edef\tcc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\tca}n^2
            \pgfmathprintnumber[print sign]{\tcb}n
            \pgfmathprintnumber[print sign]{\tcc}$
        }
        \addplot[only marks,mark=o,color=black] table[x=param-n,y=build-user]{performance-experiments/conj-True-ltac2.txt};
        \addlegendentry{build}
        \addplot [thick,black] table[x=param-n,
            y={create col/linear regression={y=build-user}}
            ] % compute a linear regression from the input table
            {performance-experiments/conj-True-ltac2.txt};
        \edef\builda{\pgfplotstableregressiona}
        \edef\buildb{\pgfplotstableregressionb}
        \addlegendentry{
            $\pgfmathprintnumber{\builda}n
            \pgfmathprintnumber[print sign]{\buildb}$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of manually building and typechecking a certificate to prove a conjunction of $n$ \mintinline{coq}{True}s using \LtacTwo} \label{fig:timing-conj-True-ltac2}
\end{figure*}

Note that for small, and even medium-sized examples, it's pretty reasonable to do duplicative work.
It's only when we reach very large examples that we start hitting non-linear behavior.

There are two obvious solutions for this problem:
\begin{enumerate}
    \item
    We can drop the type parameters from the \texttt{conj} certificates.
    \item
    We can implement some sort of sharing, where common subterms of the type only exist once in the representation.
\end{enumerate}

\subsubsection{Dropping Type Parameters: Nominal vs.~Structural Typing} \label{sec:nominal-vs-structural} \label{sec:dropping-constructor-parameters}
The first option requires that the proof assistant implement structural typing rather than nominal typing~\cite[19.3 Nominal and Structural Type Systems]{tapl}.
\todo{Find a place for this note:} Note that it doesn't actually require structural; we can do it with nominal typing if we enforce everywhere that we can only compare terms who are known to be the same type, because not having structural typing results in having a single kernel term with multiple non-unifiable types.
\todo{maybe look into TAPL}
\todo{explain structural and nominal typing more}
Morally, the reason for this is that if we have an inductive record type \todo{have we explained inductive types yet?} \todo{have we explained records yet?} whose fields do not constrain the parameters of the inductive type family \todo{have we explained parameters vs indices and inductive type families yet?}, then we need to consider different instantiations of the same inductive type family to be convertible.
That is, if we have a phantom record such as\todo{mention where the name ``phantom'' comes from?}
\begin{minted}{coq}
Record Phantom (A : Type) := phantom {}.
\end{minted}
and our implementation does not include \texttt{A} as an argument to \texttt{phantom}, then we must consider \texttt{phantom} to be both of type \texttt{Phantom nat} and \texttt{Phantom bool}, even though \texttt{nat} and \texttt{bool} are not the same.
I have requested this feature in \cite{https://github.com/coq/coq/issues/5293}.
Note, however, that sometimes it is important for such phantom types to be considered distinct when doing type-level programming.
\todo{Come up with better justification for having nominal typing available?}

\subsubsection{Sharing} \label{sec:sharing}
The alternative to eliminating the duplicative arguments is to ensure that the duplication is at-most constant sized.
There are two ways to do this: either the user can explicitly share subterms so that the size of the term is in fact linear in the size of the goal, or the proof assistant can ensure maximal sharing of subterms \todo{explain this better}.

There are two ways for the user to share subterms: using let-binders, and using function abstraction.
For example, rather than writing
\begin{minted}{coq}
@conj True (and True (and True True)) I (@conj True (and True True) I (@conj True True I I))
\end{minted}
and having roughly $n^2$ occurrences\footnote{The exact count is $n(n+1)/2 - 1$.} of \mintinline{coq}{True} when we are trying to prove a conjunction of $n$ \mintinline{coq}{True}s, the user can instead write
\begin{minted}{coq}
let T0 : Prop := True in
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
which has only $n$ occurrences of \mintinline{coq}{True}.
Alternatively, the user can write
\begin{minted}{coq}
(λ (T0 : Prop) (v0 : T0),
  (λ (T1 : Prop) (v1 : T1),
    (λ (T2 : Prop) (v2 : T2), @conj True T2 I v2)
      (and True T1) (@conj True T1 I v1))
    (and True T0) (@conj True T0 I v0))
  True I
\end{minted}

Unfortunately, both of these incur quadratic typechecking cost, even though the size of the term is linear.
See \autoref{fig:timing-conj-True-let-ltac2} and \autoref{fig:timing-conj-True-app-ltac2}.
\todo{improve data collection on perf test}

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/conj-True-let-ltac2.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=blue] table[x=param-n,y=typecheck-user]{performance-experiments/conj-True-let-ltac2.txt};
        \addlegendentry{typecheck}
        \addplotquadraticregression[no markers, blue][x=param-n,y=typecheck-user]{performance-experiments/conj-True-let-ltac2.txt};
        \edef\tca{\pgfplotstableregressiona}
        \edef\tcb{\pgfplotstableregressionb}
        \edef\tcc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\tca}n^2
            \pgfmathprintnumber[print sign]{\tcb}n
            \pgfmathprintnumber[print sign]{\tcc}$
        }
        \addplot[only marks,mark=o,color=black] table[x=param-n,y=build-user]{performance-experiments/conj-True-let-ltac2.txt};
        \addlegendentry{build}
        \addplot [thick,black] table[x=param-n,
        y={create col/linear regression={y=build-user}}
        ] % compute a linear regression from the input table
        {performance-experiments/conj-True-let-ltac2.txt};
        \edef\builda{\pgfplotstableregressiona}
        \edef\buildb{\pgfplotstableregressionb}
        \addlegendentry{
            $\pgfmathprintnumber{\builda}n
            \pgfmathprintnumber[print sign]{\buildb}$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of manually building and typechecking a certificate to prove a conjunction of $n$ \mintinline{coq}{True}s using \mintinline{coq}{let}-binders using \LtacTwo \todo{find pdf of manual for styling}} \label{fig:timing-conj-True-let-ltac2}
\end{figure*}

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/conj-True-app-ltac2.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=blue] table[x=param-n,y=typecheck-user]{performance-experiments/conj-True-app-ltac2.txt};
        \addlegendentry{typecheck}
        \addplotquadraticregression[no markers, blue][x=param-n,y=typecheck-user]{performance-experiments/conj-True-app-ltac2.txt};
        \edef\tca{\pgfplotstableregressiona}
        \edef\tcb{\pgfplotstableregressionb}
        \edef\tcc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\tca}n^2
            \pgfmathprintnumber[print sign]{\tcb}n
            \pgfmathprintnumber[print sign]{\tcc}$
        }
        \addplot[only marks,mark=o,color=black] table[x=param-n,y=build-user]{performance-experiments/conj-True-app-ltac2.txt};
        \addlegendentry{build}
        \addplot [thick,black] table[x=param-n,
        y={create col/linear regression={y=build-user}}
        ] % compute a linear regression from the input table
        {performance-experiments/conj-True-app-ltac2.txt};
        \edef\builda{\pgfplotstableregressiona}
        \edef\buildb{\pgfplotstableregressionb}
        \addlegendentry{
            $\pgfmathprintnumber{\builda}n
            \pgfmathprintnumber[print sign]{\buildb}$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of manually building and typechecking a certificate to prove a conjunction of $n$ \mintinline{coq}{True}s using abstraction and application using \LtacTwo} \label{fig:timing-conj-True-app-ltac2}
\end{figure*}

Recall that the typing rules for $\lambda$ and \texttt{let} are as follows:\todo{cite appendix with typing rules of Coq?}
\todo{maybe look in https://github.com/achlipala/frap and/or TAPL by Benjamin C. Pierce for how to render typing rules}
\todoask{What's the suggested way of pretty-printing typing rules?}
\todoask{Which way do substitution brackets go?}
\todoask{What convention should we use for typing rules with regard to things being types?  Maybe just copy the HoTT book?}
\begin{verbatim}
Γ ⊢ A type      Γ, x:A ⊢ B type
Γ, x:A ⊢ f:B
-------------------------------
Γ ⊢ (λ (x:A), f) : ∀ x:A, B


Γ ⊢ A type      Γ, x:A ⊢ B type
Γ ⊢ f : ∀ x:A, B
Γ ⊢ y : A
-------------------------------
Γ ⊢ f y : B[x/y]
\end{verbatim}

\begin{verbatim}
Γ ⊢ A type
Γ ⊢ y : A
Γ, x:A:=y ⊢ B type
Γ, x:A:=y ⊢ f : B
-------------------------------
Γ ⊢ (let x : A := y in f) : B[x/y]
\end{verbatim}

Let us consider the inferred types for the intermediate terms when typechecking the \mintinline{coq}{let} expression:
\begin{itemize}
  \item
  We infer the type \mintinline{coq}{and True T2} for the expression
\begin{minted}{coq}
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v2} into that type to type the expression
\begin{minted}{coq}
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We substitute \mintinline{coq}{T2 := and True T1} into this type to get the type \mintinline{coq}{and True (and True T1)} for the expression
\begin{minted}{coq}
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v1} into this type to get the type for the expression
\begin{minted}{coq}
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We substitute \mintinline{coq}{T1 := and True T0} into this type to get the type \mintinline{coq}{and True (and True (and True T0))} for the expression
\begin{minted}{coq}
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v0} into this type to get the type for the expression
\begin{minted}{coq}
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  Finally, we substitute \mintinline{coq}{T0 := True} into this type to get the type \mintinline{coq}{and True (and True (and True True))} for the expression
\begin{minted}{coq}
let T0 : Prop := True in
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
\end{itemize}
Note that we have performed linearly many substitutions into linearly-sized types, so unless substitution is constant time in size of the term being substituted, we incur quadratic overhead here.
The story for function abstraction is similar.
\todo{cite https://github.com/coq/coq/issues/8232 maybe?}

\todo{Should we run though typechecking in more detail here?}

We again have two choices to fix this:
either we can change the typechecking rules (which work just fine for small-to-medium-sized terms), or we can adjust typechecking to deal with some sort of pending substitution data, so that we only do substitution once.

\todo{maybe cite https://github.com/coq/coq/issues/11838?}

\todo{reference quadratic cbv here, which had a similar issue?}

\todo{some sort of section division marker here?}

The proof assistant can also try to heuristically share subterms for us.
Many proof assistants do some version of this, called \emph{hash consing}.
\todo{explain and cite hash consing?}

However, hash consing looses a lot of its benefit if terms are not maximally shared (and they almost never are), and can lead to very unpredictable performance when transformations unexpectedly cause a loss of sharing.
\todo{cite hash consing needing to be full to get perf benefit}
Furthermore, it's an open problem how to efficiently persist full hash consing to disk in a way that allows for diamond dependencies.
\todo{explain this more, find citation for hash consing being hard with disk}
\todo{flesh out hash consing section more}
\todo{maybe cite https://github.com/coq/coq/issues/9028\#issuecomment-600013284 about hash consing being slow}

\subsection{The Size of the Term}

Recall that Coq (and dependently typed proof assistants in general) have \emph{terms} which serve as both programs and proofs.
The essential function of a proof checker is to verify that a given term has a given type.
We obviously cannot type-check a term in better than linear time in the size of the representation of the term.

Recall that we cannot place any hard bounds on complexity of typechecking a term, as terms as simple as \mintinline{coq}{@eq_refl bool true} proving that the boolean \mintinline{coq}{true} is equal to itself can also be typechecked as proofs of arbitrarily complex decision procedures returning success.

We might reasonably hope that typechecking problems which require no interesting computation can be completed in time linear in the size of the term and its type.

However, some seemingly reasonable decisions can result in typechecking taking quadratic time in the size of the term, as we saw in \autoref{sec:sharing}.

\todo{maybe move some text form the sharing section to here?}

Even worse, typechecking can easily be unboundedly large in the size of the term when the typechecker chooses the wrong constants to unfold, even when very little work ought to be done.

\todo{discussion of conversion checking, and conversion modulo delta-beta}

Consider the problem of typechecking \mintinline{coq}{@eq_refl nat (fact 100) : @id nat (fact 100) = fact 100}, where \mintinline{coq}{fact} is the factorial function on natural numbers and \mintinline{coq}{id} is the polymorphic identity function.
\todo{should we define polymorphic identity function somewhere?}
If the typechecker either decides to unfold \mintinline{coq}{id} before unfolding \mintinline{coq}{fact}, or if it performs a breath-first search, then we get speedy performance.
However, if the typechecker instead unfolds \mintinline{coq}{id} \emph{last}, then we end up computing the normal form of $100!$, which takes a long time and a lot of memory.
See \autoref{fig:timing-eq-refl-nat-factorial}.

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/eq-refl-nat-factorial.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[x=param-n,y=constr-eq-refl-user]{performance-experiments/eq-refl-nat-factorial.txt};
        \addlegendentry{typechecking}
        \addplotfactorialregression[no markers, black][x=param-n,y=constr-eq-refl-user]{performance-experiments/eq-refl-nat-factorial.txt};
        \addlegendentry{$\pgfmathprintnumber{\pgfplotstableregressiona}n!$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of typechecking \mintinline{coq}{@eq_refl nat (fact n) : @id nat (fact n) = fact n}} \label{fig:timing-eq-refl-nat-factorial}
\end{figure*}

Note that it is by no means obvious that the typechecker can meaningfully do anything about this.
Breath-first search is significantly more complicated than depth-first, is harder to write good heuristics for, can incur enormous space overheads, and can be massively slower in cases where there are many options and the standard heuristics for depth-first unfolding in conversion-checking are sufficient.
Furthermore, the more heuristics there are to tune conversion-checking, the more ``magic'' the algorithm seems, and the harder it is to debug when the performance is inadequate.

As described in \todo{reference above example about fiat-crypto}, in fiat-crypto, we got exponential slowdown due to this issue, with an estimated overhead of over four thousand millennia of extra typechecking time in the worst examples we were trying to handle.

\todo{maybe forward reference to the number of abstraction barriers}
\todo{maybe include more about real-world fiat-crypto example here?}

\subsection{The Number of Binders} \label{sec:perf:binder-count}

This is a particular subcase of the above sections that we call out explicitly.
Often there will be some operation (for example, substitution, lifting, context-creation) that needs to happen every time there is a binder, and which, when done naively, is linear in the size of the term or the size of the context.
As a result, naïve implementations will often incur quadratic---or worse---overhead in the number of binders.

\todo{make sure we've explained proof engine and Ltac by here}

Similarly, if there is any operation that is even linear rather than constant in the number of binders in the context, then and user operation in proof mode which must be done, say, for each hypothesis, will incur an overall quadratic-or-worse performance penalty.

The claim of this subsection is not that any particular application is inherently constrained by a performance bottleneck in the number of binders, but instead that it's very, very easy to end up with quadratic-or-worse performance in the number of binders, and hence that this forms a meaningful cluster for performance bottlenecks in practice.

I will attempt to demonstrate this point with a palette of actual historical performance issues in Coq---some of which persist to this day---where the relevant axis was ``number of binders.''
None of these performance issues are insurmountable, but all of them are either a result of seemingly reasonable decisions, have subtle interplay with seemingly disparate parts of the system, or else are to this day still mysterious despite the work of developers to investigate them.

\subsubsection{Name Resolution} \label{sec:name-resolution}
One key component of interactive proof assistants is figuring out which constant is referred to by a given name.
It may be tempting to keep the context in an array or linked list.
However, if looking up which constant or variable is referred to by a name is $\mathcal O(n)$, then internalizing a term with $n$ typed binders is going to be $\mathcal O(n^2)$, because we need to do name lookups for each binder.
See \coqbug{9582} and \coqpr{9586}.

See \autoref{fig:timing-name-resolution} for the timing of name resolution in Coq.
See \autoref{fig:timing-nested-lambda-different-name} for the effect on internalizing a lambda with $n$ arguments.

\todo{mention Coq version automatically, mention why we're using a different Coq version}
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments-8-9/name-resolution.txt}
    }
    \begin{axis}[xlabel=\# binders,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        ylabel style={
            yshift = {width("$8\cdot 1$")}
        }]
        \addplot[only marks,color=black] table[x=param-n,y=uconstr-I-1000-user]{performance-experiments-8-9/name-resolution.txt};
        \addlegendentry{1000 name resolutions}
        \addplot [thick,black] table[x=param-n,
        y={create col/linear regression={y=uconstr-I-1000-user}}
        ]{performance-experiments-8-9/name-resolution.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of internalizing a name 1000 times under $n$ binders} \label{fig:timing-name-resolution}
\end{figure*}
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments-8-9/nested-lambda-different-name.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        ylabel style={
            yshift = {width("$8\cdot{}$")}
        }]
        \addplot[only marks,color=black] table[x=param-n,y=uconstr-lambda-user]{performance-experiments-8-9/nested-lambda-different-name.txt};
        \addlegendentry{parsing and internalization}
        \addplotquadraticregression[no markers, black][x=param-n,y=uconstr-lambda-user]{performance-experiments-8-9/nested-lambda-different-name.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}n^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}n
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of internalizing a function with $n$ differently-named arguments of type \mintinline{coq}{True}} \label{fig:timing-nested-lambda-different-name}
\end{figure*}

\todo{Is this called internalization or is it called elaboration}

\subsubsection{Capture-Avoiding Substitution} \label{sec:perf:capture-avoiding-subst}
If the user is presented with a proof engine interface where all context variables are named, then in general the proof engine must implement capture-avoiding substitution.
For example, if the user wants to operate inside the hole in \mintinline{coq}{(λ x, let y := x in λ x, _)}, then the user needs to be able to talk about the body of \mintinline{coq}{y}, which is not the same as the innermost \mintinline{coq}{x}.
However, if the $\alpha$-renaming is even just linear in the existing context, then creating a new hole under $n$ binders will take $\mathcal{O}(n^2)$ time in the worst case, as we may have to do $n$ renamings, each of which take time $\mathcal O(n)$.
See \coqbug{9582}, perhaps also \coqbug{8245} and \coqbug{8237} and \coqbug{8231}.

This might be the cause of the difference in \autoref{fig:timing-open-constr-n-lambda-no-types} between having different names (which do not need to be renamed) and having either no name (requiring name generation) or having all binders with the same name (requiring renaming in evar substitutions).

\todo{ADD PLOT: try to come up with a graph for renaming stuff}
\todo{ADD PLOT: check if confounders come up}

\subsubsection{Quadratic Creation of Substitutions for Existential Variables} \label{sec:perf:quadratic-evar-subst}
Recall \todo{make sure that this is mentioned previously, and that we're not rehashing things too much} that when we separate the trusted kernel from the untrusted proof engine, we want to be able to represent not-yet-finished terms in the proof engine.
The standard way to do this is to enrich the type of terms with an ``existential variable'' node, which stands for a term which will be filled later.
\todo{cite original idea for evars? (what is it?)}
Such existential variables, or evars, typically exist in a particular context.
That is, you have access to some hypotheses but not others when filling an evar.

Sometimes, reduction results in changing the context in which an evar exists.
For example, if we want to $\beta$-reduce \mintinline{coq}{(λ x, ?e₁) (S y)}, then the result is the evar \mintinline{coq}{?e₁} with \mintinline{coq}{S y} substituted for \mintinline{coq}{x}.

There are a number of ways to represent substitution, and the choices are entangled with the choices of term representation.

Note that most substitutions are either identity or lifting substitutions.
\todo{define identity and lifting substitutions}

One popular representation is the locally nameless representation~\cite{Locally2012Chargueraud,locally2007Leroy}, which we discuss more in \autoref{sec:binders:locally-nameless}.
\todo{Justify it?  Discuss other representations? Say why and how it's convenient?}
However, if we use a locally nameless term representation, then finding a compact representation for identity and lifting substitutions is quite tricky.
If the substitution representation takes $\mathcal O(n)$ time to create in a context of size $n$, then having a $\lambda$ with $n$ arguments whose types are not known takes $\mathcal O(n^2)$ time, because we end up creating identity substitutions for $n$ holes, with linear-sized contexts.

Note that fully nameless, i.e., de Bruijn term representations, do not suffer from this issue.

See \coqbug{8237} and \coqpr{11896} for a mitigation of some (but not all) issues.

See also \autoref{fig:timing-do-n-open-constr-True} and \autoref{fig:timing-open-constr-n-lambda-no-types}.

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/do-n-open-constr-True.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[x=param-n,y=open-constr-True-1000-user]{performance-experiments/do-n-open-constr-True.txt};
        \addlegendentry{construct an evar 1000 times}
        \addplot [thick,black] table[x=param-n,
         y={create col/linear regression={y=open-constr-True-1000-user}}
         ] % compute a linear regression from the input table
         {performance-experiments/do-n-open-constr-True.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}n
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of generating 1000 evars in a context of size $n$} \label{fig:timing-do-n-open-constr-True}
\end{figure*}

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/open-constr-n-lambda-no-types-same-names.txt}
        \einput{performance-experiments/open-constr-n-lambda-no-types-no-names.txt}
        \einput{performance-experiments/open-constr-n-lambda-no-types-different-names.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=*,color=black] table[x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-same-names.txt};
        \addlegendentry{same names}
        \addplotquadraticregression[no markers, black][x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-same-names.txt};
        \edef\ocsamea{\pgfplotstableregressiona}
        \edef\ocsameb{\pgfplotstableregressionb}
        \edef\ocsamec{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\ocsamea}n^2
            \pgfmathprintnumber[print sign]{\ocsameb}n
            \pgfmathprintnumber[print sign]{\ocsamec}$
        }

        \addplot[only marks,mark=o,color=blue] table[x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-no-names.txt};
        \addlegendentry{no names}
        \addplotquadraticregression[no markers, blue][x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-no-names.txt};
        \edef\ocnoa{\pgfplotstableregressiona}
        \edef\ocnob{\pgfplotstableregressionb}
        \edef\ocnoc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\ocnoa}n^2
            \pgfmathprintnumber[print sign]{\ocnob}n
            \pgfmathprintnumber[print sign]{\ocnoc}$
        }

        \addplot[only marks,mark=+,color=red] table[x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-different-names.txt};
        \addlegendentry{different names}
        \addplotquadraticregression[no markers, red][x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-different-names.txt};
        \edef\ocdifferenta{\pgfplotstableregressiona}
        \edef\ocdifferentb{\pgfplotstableregressionb}
        \edef\ocdifferentc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\ocdifferenta}n^2
            \pgfmathprintnumber[print sign]{\ocdifferentb}n
            \pgfmathprintnumber[print sign]{\ocdifferentc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of generating a $\lambda$ with $n$ binders of unknown/evar type, all of which have either no name, the same name, or different names} \label{fig:timing-open-constr-n-lambda-no-types}
\end{figure*}

\subsubsection{Quadratic Substitution in Function Application} \label{sec:perf:quadratic-application}
Consider the case of typechecking a non-dependent function applied to $n$ arguments.
If substitution is performed eagerly, following directly the rules of the type theory, \todo{cite the rules / reference an appendix}, then typechecking is quadratic.
This is because the type of the function is $\mathcal{O}(n)$, and doing substitution $n$ times on a term of size $\mathcal{O}(n)$ is quadratic.

If the term representation contains $n$-ary application nodes, it's possible to resolve this performance bottleneck by delaying the substitutions.
If only unary application nodes exist, it's much harder to solve.

Note that this is important, for example, if you want to avoid the problem of quadratically-sized certificates by making a $n$-ary conjunction-constructor which is parameterized on a list of the conjuncts.
Such a function could then be applied to the $n$ proofs of the conjuncts.

See \coqbug{8232} and \coqbug{12118} and \coqpr{8255}.

See \autoref{fig:timing-app-n}.

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/app-n-ltac2.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[x=param-n,y=typecheck-user]{performance-experiments/app-n-ltac2.txt};
        \addlegendentry{typecheck}
        \addplotquadraticregression[no markers, black][x=param-n,y=typecheck-user]{performance-experiments/app-n-ltac2.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}n^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}n
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
            }
    \end{axis}
\end{tikzpicture}
\caption{Timing of typechecking a function applied to $n$ arguments} \label{fig:timing-app-n}
\end{figure*}

\subsubsection{Quadratic Normalization by Evaluation} \label{sec:perf:quadratic-NbE} \label{sec:perf:quadratic-cbv}
Normalization by evaluation (NbE) is a nifty way to implement reduction where function abstraction in the object language is represented by function abstraction in the metalanguage.
\todo{explain or forward-reference NbE}
\todo{Read stuff in \url{https://github.com/HoTT/book/issues/995\#issuecomment-418825844} for NbE stuff}
\todo{Say more about it than that it's ``nifty''}
Coq uses NbE to implement two of its reduction machines (\mintinline{coq}{lazy} and \mintinline{coq}{cbv}).

The details of implementing NbE depend on the term representation used.
If a fancy term encoding like PHOAS \todo{reference explanation of PHOAS} is used, then it's not hard to implement a good NbE algorithm.
However, such fancy term representations incur unpredictable and hard-to-deal-with performance costs.
Most languages do not do any reduction on thunks until they are called with arguments, which means that forcing early reduction of a PHOAS-like term representation requires round-tripping though another term representation, which can be costly on large terms if there is not much to reduce.
On the other hand, other term representations need to implement either capture-avoiding substitution (for named representations) or index lifting (for de Bruijn and locally nameless representations).

The sort-of obvious way to implement this transformation is to write a function that takes a term and a binder, and either renames the binder for capture-avoiding substitution or else lifts the indices of the term.
The problem with this implementation is that if you call it every time you move a term under a binder, then moving a term under $n$ binders traverses the term $n$ times.
If the term size is also proportional to $n$, then the result is quadratic blowup in the number of binders.

See \coqbug{11151} for an occurrence of this performance issue in the wild in Coq.
See also \autoref{fig:timing-quadratic-cbv-lazy-PHOAS}.


\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/quadratic-cbv-lazy-PHOAS.txt}
    }
    \begin{axis}[xlabel=\# binders,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=o,color=blue] table[x=param-num-binders,y=cbv-user]{performance-experiments/quadratic-cbv-lazy-PHOAS.txt};
        \addlegendentry{\mintinline{coq}{cbv}}
        \addplotquadraticregression[no markers, blue][x=param-num-binders,y=cbv-user]{performance-experiments/quadratic-cbv-lazy-PHOAS.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
        \addplot[only marks,mark=*,color=black] table[x=param-num-binders,y=lazy-user]{performance-experiments/quadratic-cbv-lazy-PHOAS.txt};
        \addlegendentry{\mintinline{coq}{lazy}}
        \addplotquadraticregression[no markers, black][x=param-num-binders,y=lazy-user]{performance-experiments/quadratic-cbv-lazy-PHOAS.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of running \mintinline{coq}{cbv} and \mintinline{coq}{lazy} reduction on interpreting a PHOAS expression as a function of the number of binders} \label{fig:timing-quadratic-cbv-lazy-PHOAS}
\end{figure*}

\subsubsection{Quadratic Closure Compilation} \label{sec:perf:closure-compilation} \label{sec:perf:quadratic-vm-native}
It's important to be able to perform reduction of terms in an optimized way.
\todo{maybe give a better introductory justification to the vm and native compiler than ``it's important''?}
When doing optimized reduction in an imperative language, we need to represent closures---abstraction nodes---in some way.
Often this involves associating to each closure both some information about or code implementing the body of the function, as well as the values of all of the free variables of that closure.
\todo{cite \url{https://flint.cs.yale.edu/shao/papers/escc.html} or its references for closure compilation}
In order to have efficient lookup, we need to know the memory location storing the value of any given variable statically at closure-compilation time.
The standard way of doing this \todo{cite something for flat closure compilation} is to allocate an array of values for each closure.
If variables are represented with de Bruijn indices, for example, it's then a very easy array lookup to get the value of any variable.
Note that this allocation is linear in the number of free variables of a term.
If we have many nested binders and use all of them underneath all the binders, then every abstraction node has as many free variables as there are total binders, and hence we get quadratic overhead.

See \coqbug{11151} and \coqbug{11964} and \ocamlbug{7826} for an occurrence of this issue in the wild.
Note that this issue rarely shows up in hand-written code, only in generated code, so developers of compilers such as \texttt{ocamlc} and \texttt{gcc} might be uninterested in optimizing this case.
However, it's quite essential when doing meta-programming involving large generated terms.
It's especially essential if we want to chain together reflective automation passes that operate on different input languages and therefore require denotation and reification between the passes.
In such cases, unless our encoding language uses named or de Bruijn variable encoding, there's no way to avoid large numbers of nested binders at compilation time while preserving code sharing.
Hence if we're trying to reuse the work of existing compilers to bootstrap good performance of reduction (as is the case for the native compiler in Coq), we have trouble with cases such as this one.
\todo{reorganize this paragraph, improve it}

See also \autoref{fig:timing-quadratic-vm-PHOAS} and \autoref{fig:timing-quadratic-native-PHOAS}.

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/quadratic-vm-PHOAS.txt}
    }
    \begin{axis}[xlabel=\# binders,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[x=param-num-binders,y=vm-compute-user]{performance-experiments/quadratic-vm-PHOAS.txt};
        \addlegendentry{\mintinline{coq}{vm_compute}}
        \addplotquadraticregression[no markers, black][x=param-num-binders,y=vm-compute-user]{performance-experiments/quadratic-vm-PHOAS.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of running \mintinline{coq}{vm_compute} reduction on interpreting a PHOAS expression as a function of the number of binders} \label{fig:timing-quadratic-vm-PHOAS}
\end{figure*}

\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/quadratic-native-PHOAS.txt}
    }
    \begin{axis}[xlabel=\# binders,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[x=param-num-binders,y=native-compute-real]{performance-experiments/quadratic-native-PHOAS.txt};
        \addlegendentry{\mintinline{coq}{native_compute}}
        \addplotquadraticregression[no markers, black][x=param-num-binders,y=native-compute-real]{performance-experiments/quadratic-native-PHOAS.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of running \mintinline{coq}{native_compute} reduction on interpreting a PHOAS expression as a function of the number of binders} \label{fig:timing-quadratic-native-PHOAS}
\end{figure*}

\todo{maybe more examples here?}

\subsection{The Number of Nested Abstraction Barriers} \label{sec:axis-nested-abstraction-barriers}

\todo{ADD PLOT: This section requires digging into the historical performance issues around this to find convincing stand-alone examples so we can have graphs here.}

This axis is the most theoretical of the axes.
An abstraction barrier is an interface for making use of code, definitions, and theorems.
For example, you might define non-negative integers using a binary representation, and present the interface of zero, successor, and the standard induction principle, along with an equational theory for how induction behaves on zero and successor.
\todo{should I spell this example out more?}
You might use lists and non-negative integers to implement a hash-set datatype for storing sets of hashable values, and present the hash-set with methods for empty, add, remove, membership-testing, and some sort of fold.
Each of these is an abstraction barrier.

There are three primary ways that nested abstraction barriers can lead to performance bottlenecks: one involving conversion missteps and two involving exponential blow-up in the size of types.

\subsubsection{Conversion Troubles} \label{sec:abstraction-barriers:conversion-troubles}
If abstraction barriers are not perfectly opaque---that is, if the typechecker ever has to unfold the definitions making up the API in order to typecheck a term---then every additional abstraction barrier provides another opportunity for the typechecker to pick the wrong constant to unfold first.
\todo{add example?}
In some typecheckers, such as Coq, it's possible to provide hints to the typechecker to inform it which constants to unfold when.
In such a system, it's possible to carefully craft conversion hints so that abstraction barriers are always unfolded in the right order.
Alternatively, it might be possible to carefully craft a system which picks the right order of unfolding by using a dependency analysis.

However, most users don't bother to set up hints like this, and dependency analysis isn't sufficient to determine which abstraction barrier is ``higher up'' when there are many parts of it, only some of which are mentioned in any given part of the next abstraction barrier.
The reason users don't set up hints like this is that usually it's not necessary.
There's often minimal overhead, and things just work, even when the wrong path is picked---until the number of abstraction barriers or the size of the underlying term gets large enough.
Then we get noticeable exponential blowup, and everything is sad.
Furthermore, it's hard to know which part of conversion is incurring exponential blowup, and thus one has to basically get all of the conversion hints right, simultaneously, without any feedback, to see any performance improvement.



\subsubsection{Type Size Blowup: Abstraction Barrier Mismatch} \label{sec:abstraction-barriers:mismatch}
When abstraction barriers are leaky or misaligned, there's a cost that accumulates in the size of the types of theorems.
Consider, for example, the two different ways of using tuples:
(1) we can use the projections \mintinline{coq}{fst} and \mintinline{coq}{snd}; or
(2) we can use the eliminator \mintinline{coq}{pair_rect : ∀ A B (P : A × B → Type), (∀ a b, P (a, b)) → ∀ x, P x}.
The first gets us access to one element of the tuple at a time, while the second has us using all elements of the tuple simultaneously.

Suppose now there is one API defined in terms of \mintinline{coq}{fst} and \mintinline{coq}{snd}, and another API defined in terms of \mintinline{coq}{pair_rect}.
To make these APIs interoperate, we need to explicitly convert from one representation to another.
Furthermore, every theorem about the composition of these APIs needs to include the interoperation in talking about how they relate.

If such API mismatches are nested, or if this code size blowup interacts with conversion missteps, then the performance issues compound.

\todo{maybe be more concrete here?}

\subsubsection{Type Size Blowup: Packed vs.~Unpacked Records} \label{sec:abstraction-barriers:packed-records}
When designing APIs, especially of mathematical objects, one of the biggest choices is whether to pack the records, or whether to pass arguments in as fields.
That is, when defining a monoid, for example, there are five ways to go about specifying it:
\begin{enumerate}
    \item
    (packed)
    A \emph{monoid} consists of a type $A$, a binary operation $\cdot : A \to A \to A$, an identity element $e$, a proof that $e$ is a left- and right-identity $e \cdot a = a \cdot e = a$ for all $a$, and a proof of associativity that $(a \cdot b) \cdot c = a \cdot (b \cdot c)$.
    \item
    A \emph{monoid on a carrier type $A$} consists of a binary operation $\cdot : A \to A \to A$, an identity element $e$, a proof that $e$ is a left- and right-identity, and a proof of associativity.
    \item
    A \emph{monoid on a carrier type $A$ under the binary operation $\cdot : A \to A \to A$} consists of an identity element $e$, a proof that $e$ is a left- and right-identity, and a proof of associativity.
    \item
    (mostly unpacked)
    A \emph{monoid on a carrier type $A$ under the binary operation $\cdot : A \to A \to A$ with identity element $e$} consists of a proof that $e$ is a left- and right-identity and a proof of associativity.
    \item
    (fully unpacked) A monoid on a carrier type $A$ under the binary operation $\cdot : A \to A \to A$ with identity element $e$ using a proof $p$ that $e$ is a left- and right-identity and a proof of $q$ of associativity consists of an element of the one-element unit type.
\end{enumerate}
\todo{cite math-classes and HoTT for past design work here}

If we go with anything but the fully packed design, then we incur exponential overhead as we go up abstraction layers, as follows.
A \emph{monoid homomorphism} from a monoid $A$ to a monoid $B$ consists of a function between the carrier types, and proofs that this function respects composition and identity.
If we use an unpacked definition of monoid with $n$ type parameters, then a similar definition of a monoid homomorphism involves at least $2n+2$ type parameters.
In higher category theory, it's common to talk about morphisms between morphisms, and every additional layer here doubles the number of type arguments, and this can quickly lead to very large terms, resulting is major performance bottlenecks.
Note that number of type parameters determines the constant factor out front of the exponential growth in the number of layers of mathematical constructions.

How much is this overhead concretely?
When developing a category theory library~\cite{category-coq-experience}, we sped up overall compilation time by approximately a factor of two, from around 16 minutes to around 8 minutes, by changing one of the two parameters to a field in the definition of a category.%
\footnote{%
  See \githubhref[commit ][ of JasonGross/catdb on GitHub]{JasonGross/catdb}{209231ae3e94d5dbbc678c94930dcf585d53d555} for details.%
}

\todo{maybe figure out some examples and include perf data?}
\todo{does this need more exposition?}

\section{Conclusion of this Chapter}
\todo{How should this chapter be concluded?}
\todo{Maybe another look-forward at what comes next?}

\begin{subappendices}
\begin{comment}
\section{Comments from Andres}
in chapter 2 you start by comparing coq by non-dependently-typed languages. perhaps the more appropriate comparison would be between coq and *compilers* of other languages? Ah, you do this later.
"integers or colors or names" -- some hardcore optimization nerds may object. did you know that for all of these it is possible to sort in less than nlogn time using specialized techniques? :P (not a serious objection)
"3. Being able to run the proof assistant in your head" -- obligatory claim about this finding bugs that the developers didn't see? Also, please emphasize that this is beyond the *spec* of conversion, but about the actual *algorithm*. the GCC example kinda does that, map it back explicitly.
typechecking of applications definitely was quadratic, but may no longer be
"maximally" sharing terms is almost always heuristic, and I would call it heuristic sharing to highlight that (or something other that does not claim maximality)
hash consing itself is also slow
I would like to be explicitly credited for my contributions to this list. Somewhere in the thesis, sometime, at least when you get to claiming how many of these are you are taking credit for.
"There are three primary ways that nested abstraction barriers can lead to performance bottlenecks: one involving conversion missteps and the other involving exponential blow-up in the size of types."
"The reason users don’t set up hints like this is that usually it’s not necessary." -- and that is hard to see where the time is spent when it is necessary
Thu 7:22pm
and the non-locality of design choices is even across libraries and proof assistant implementations
Thu 7:22pm
see your abstraction layer counting stuff
Thu 7:22pm
more generally
Thu 7:38pm
if in 20 years I hear that your thesis has become popular and important, my guess would be that it was "the first paper in a field", out of "you either want to write the first paper in the field or the last paper in the field"
Thu 7:39pm
in particular, the biggest impact I could see from this is making people consider proof performance engineering a worthwhile direction of study
Thu 7:39pm
you have overwhelming evidence that proof projects need the results of this
Thu 7:40pm
probably it would help to describe how it is intellectually interesting, related to profound questions, and just cool to hack on
Thu 7:40pm
the more you can hit those points, the more likely I think people will care
Thu 7:41pm
(was my feedback useful? would you like to discuss anything about it?)

\end{comment}
\end{subappendices}
