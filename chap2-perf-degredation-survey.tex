\chapter{The Performance Landscape in Type-Theoretic Proof Assistants} \label{ch:perf-failures}

\section{Introduction}\label{sec:perf-failures:story}

As alluded to in \autoref{sec:manual-proof-long}, when writing non-automated proofs to verifying code in a proof assistant, the number of lines of proof we need to write scales with the number of lines of code being verified, typically resulting to a $10\times$ to $100\times$ overhead.
In this strategy, proof-generation and proof-checking times are reasonable, often scaling linearly with the number of lines of proof.
Automating the generation of proofs solves the overhead of proof-writing time, however, it introduces massive \emph{non-linear} overhead in the time it takes for the computer to generate and check the proof.

The main contribution of this thesis, presented in \autoref{ch:rewriting}, is a tool that solves this problem of unacceptable overhead in proof-generation and proof-checking for the Fiat Cryptography project~\cite{FiatCryptoSP19},  which we believe is broadly applicable to other domains.

In building this work, the author developed a deep understanding of the performance bottlenecks faced and addressed.
This chapter lays out the groundwork for understanding these performance bottlenecks, where they come from, and how our solution addressed the relevant performance issues.
We describe what we have seen of the landscape of performance issues in proof assistants, and provide a map for navigation. 
Our hope is that readers will be able to apply our map to performance bottlenecks they encounter in dependently-typed tactic-driven proof assistants like Coq.

\section{Exponential Domain}\label{sec:perf-failures:exponential-domain}

We sketch out the main differences between performance issues we've encountered in dependently-typed proof assistants and performance issues in other languages.
Some of these differences are showcased through a palette of real performance issues that have arisen in Coq.

The widespread commonsense in performance engineering %~\cite{commonsense-perf-engineering-order-of-operations}
is that good performance optimization happens in a particular order:
there is no use micro-optimizing code when implementing an algorithm with unacceptable performance characteristics; imagine trying to optimize the pseudorandom number generator used in bogosort~\cite{Sorting2007Gruber}, for example.\footnote{Bogosort, whose name is a portmanteau of the words bogus and sort~\cite{bogosort-name}, sorts a list by randomly permuting the list over and over until it is sorted.}
Similarly, there is no use trying to find or create a better algorithm if the problem being solved is more complicated than it needs to be; consider, for example, the difference between ray tracers and physics simulators.
Ray tracers determine what objects can be seen from a given point by drawing lines from the viewpoint to the object and seeing if it passes through any other object ``in front of'' it.
Alternatively, it is possible to provide a source of light waves and simulate the physical interaction of light with the various objects, to determine what images remain when the light arrives at a particular point.
There's no use trying to find an efficient algorithm for simulating quantum electrodynamics, though, if all that is needed is to answer ``which parts of which objects need to be drawn on the screen?''

One essential ingredient for this division of concerns---between specifying the problem, picking an efficient algorithm, and optimizing the implementation of the algorithm---is knowledge of what a typical set of inputs looks like, and what the scope looks like.
When sorting a list, we know that the length of the list and the initial ordering matter; for sorting algorithms that work for sorting lists with any type of elements, it generally doesn't matter, though, whether we're sorting a list of integers or colors or names.
Furthermore, randomized datasets tend to be reasonably representative for list ordering, though we may also care about some special cases, such as already-sorted lists, nearly sorted lists, and lists in reverse-sorted order.
We can say that sorting is always possible in $\mathcal O(n\log n)$ time, and that's a pretty good starting point.

In Coq, and other dependently-typed proof assistants, this ingredient is missing.
The domain is much larger: in theory, we want to be able to check any proof anyone might write.
Furthermore, in dependently typed proof assistants, the worst-case behavior is effectively unbounded, because any provably terminating computation can be run at typechecking time.
\minortodo{cite https://github.com/coq/coq/issues/12200}

In fact, this issue already arises for compilers of mainstream programming languages.
\minortodo{literature search on perf of compile time in mainstream compilers?}
The C++ language, for example, has \texttt{constexpr} constructions that allow running arbitrary computation at compile-time, and it's well-known that C++ templates can incur a large compile-time performance overhead.\minortodo{cite?}
However, we claim that, in most languages, even as programs scale, these performance issues are the exception rather than the rule.
Most code written in C or C++ does not hit unbounded compile-time performance bottlenecks.
Generally, for code that compiles in a reasonable amount of time, as the codebase size is scaled up, compile time will creep up linearly.

\begin{wrapfigure}{r}{.5\textwidth}
  \centering
    \beginTikzpictureStamped{
        \einput{fiat-crypto-perf-logs/montgomery-fesub-before.txt}
    }
    \begin{axis}[title={synthsis time (s) vs.\ \# limbs},
        legend pos=north west,
        width=0.4\textwidth,
        axis lines=left,
        xmin=0,
        xmax=9.5,
        scaled x ticks=false,
        scaled y ticks=false,
        xtick distance=1,
        ylabel style={
            yshift = {width("10")}
        }]
        \addplot[only marks,color=black] table[x=nlimbs,y=user]{fiat-crypto-perf-logs/montgomery-fesub-before.txt};
        \addplotexponentialregression[no markers, black][x=nlimbs,y=user,a=0.0001,b=2.5]{fiat-crypto-perf-logs/montgomery-fesub-before.txt};
        \addlegendentry{synthesis}
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona} e^{\pgfmathprintnumber{\pgfplotstableregressionb} n} $
        }
    \end{axis}
\end{tikzpicture}
    \\
    \beginTikzpictureStamped{
        \einput{fiat-crypto-perf-logs/montgomery-fesub-before.txt}
    }
    \begin{axis}[title={synthsis time (s) vs.\ \# limbs ($\log$-scale)},
%        legend pos=north west,
        width=0.4\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        ymode=log,
        xtick distance=1]
        \addplot[only marks,color=black] table[x=nlimbs,y=user]{fiat-crypto-perf-logs/montgomery-fesub-before.txt};
        \addplotexponentialregression[no markers, black][x=nlimbs,y=user,a=0.0001,b=2.5]{fiat-crypto-perf-logs/montgomery-fesub-before.txt};
%        \addlegendentry{synthesis}
%        \addlegendentry{
%            $\pgfmathprintnumber{\pgfplotstableregressiona} e^{\pgfmathprintnumber{\pgfplotstableregressionb} n} $
%        }
    \end{axis}
\end{tikzpicture}
    \caption{Timing of synthesizing subtraction\todonz{somewhat hard to make sense of these graphs without more explanation...}} \label{fig:timing-montgomery-fesub-before-user}
\end{wrapfigure}

In Coq, however, the scaling story is very different.
Compile time scales super-linearly with example size.
Frequently, users will cobble together code that works to prove a toy version of some theorem, or to verify a toy version of some program.
By virtue of the fact that humans are impatient, the code will execute in reasonable time, perhaps a couple seconds, on the toy version.
The user will then apply the same proof technique on a slightly larger example, and the proof-checking time will often be pretty similar.
After scaling the example a bit more, the proof-checking time will be noticeably slow---maybe it now takes a couple of minutes.
Suddenly, though, scaling the example just a tiny bit more will result in the compiler not finishing even if we let it run for a day or more.
This is what working in an exponential performance domain is like.

\label{sec:fiat-crypto-codegen-numbers}%
To put numbers on this, let us consider an example from Fiat Cryptography which involved generating C code to do arithmetic on very large numbers.
The code generation was parameterized on the number of machine words needed to represent a single big integer.
Our smallest toy example used two machine words; our largest example used 17.
The smallest toy example took about 14 seconds.
Based on the the compile-time performance of about a hundred examples, we expect the largest example would have taken over four thousand \emph{millennia}!
See \autoref{fig:timing-montgomery-fesub-before-user}.
(Our primary non-toy test example used four machine words and took just under a minute; the biggest realistic example we were targeting was twice that size, at eight machine words, and took about 20 hours.)

\section{Motivating the Performance Map}%\label{sec:perf-failures:story}

In most performance domains, solutions to performance bottlenecks are generally either hyper-specialized to the code being optimized or the domain of the algorithm, or else are so general as to be applicable to all performance engineering.
For example, solving a performance issue might involving caching the result of a particular computation; caching is a very general solution, while the particular computation being run is hyper-specialized.
Once factored like this, there is generally no remaining insight to be had about the particular performance bottleneck encountered.
In our experience with proof assistants, most performance bottlenecks are far from the code being written and the domain being investigated, and are yet also far from general performance engineering.
%There is structure specific to dependently typed proof assistants that shows up in performance bottlenecks.

The example above is an instance of a performance bottleneck which is neither specific the domain (in our case, cryptographic code generation) nor general enough to apply to performance engineering outside of proof assistants.

Where \emph{is} the bottleneck?
Maybe, one might ask, were we generating unreasonable amounts of code?
Each example using $n$ machine words generated $3n$ lines of code.
How can exponential performance result from linear code?

Our method involved two steps: first generate the code, then check that the generated code matches with what comes out of the verified code generator.
This may seem a bit silly, but this is actually somewhat common; in a theorem that says ``any code that comes out of this code generator satisfies this property'', we need a proof that the code we feed into the theorem actually came out of the specified code generator, and the easiest way to prove this is, roughly, to tell the proof assistant to just check that fact for you.
(It's possible to be more careful and not do the work twice, but this often makes the code a bit harder to read and understand, and is oftentimes pointless; premature optimization is the root of all evil, as they say.)
Furthermore, because we often don't want to fully compute results when checking that two things are equal---just imagine having to compute the factorial of $1000$ just to check that $1000!$ is equal to itself---the default method for checking that the code came out of the code generator is different from the method we used to compute the code in the first place.

It turns out that the actual code generation took less than 0.002\% of the total time on the largest examples we tested (just 14 seconds out of about 211 hours).
The rest of the time was spent checking that the generated code in fact matched what comes out of the verified code generator.

\section{Performance Engineering in Proof Assistants is Hard}

The fix to the example is itself quite simple, being only 21 characters long.\footnote{\texttt{Strategy 1 [Let\_In].} for those who are curious.}
However, tracking down this solution was quite involved, requiring the following pieces:
\begin{enumerate}
  \item
    A good profiling tool for proof scripts (see \autoref{sec:ltac-prof}).
    This is a standard component of a performance engineer's toolkit, but when I started my PhD, there was no adequate profiling infrastructure for Coq.
    While such a tool is essential for performance engineering in all domains, what's unusual about dependently-typed proof assistants, I claim, is that essentially \emph{every} codebase that needs to scale runs into performance issues, and furthermore these issues are frequently total blockers for development because so many of them are exponential in nature.
  \item
    Understanding the details of how Coq works under-the-hood.
    Conversion, the ability to check if two types or terms are the same, is one of the core components of any dependently-typed proof assistant.
    Understanding the details of how conversion works is generally not something users of a proof assistant want to worry about; it's like asking C programmers to keep in mind the size of \texttt{gcc}'s maximum nesting level for \texttt{\#include}'d files\footnote{It's 200, for those who are curious~\cite{C2017FSF}.} when writing basic programs.
    It's certainly something that advanced users need to be aware of, but it's not something that comes up frequently.
  \item
    Being able to run the proof assistant in your head.
    When I looked at the conversion problem, I knew immediately what the most likely cause of the performance issue was.
    But this is because I've managed to internalize most of how Coq runs in my head.

    This might seem reasonable at a glance; one expects to have to understand the system being optimized in order to optimize it.
    However, the knowledge required here is hard-won and not easily accessible.
    While I've managed to learn the details of what Coq is doing---including performance characteristics---basically without having to read the source code at all, the relevant performance characteristics are not documented anywhere, and are not even easily interpretable from the source code of Coq.
%    \todonz{[re ``without having to read the source code at all!''] is this good or bad? why didn't you read Coq src?}
    This is akin to, say, being able to learn how \texttt{gcc} represents various bits of C code, what transformations it does in what order, and what performance characteristics these transformations have, just from using \texttt{gcc} to compile C code and reading the error messages it gives you.
    These are details that should not need to be exposed to the user, but because dependent type theory is so complicated---complicated enough that it's generally assumed that users will get \emph{line-by-line interactive feedback from the compiler} while developing, the numerous design decisions and seemingly reasonable defaults and heuristics lead to subtle performance issues.
    Note, furthermore, that this performance issue is essentially about the algorithm used to implement conversion, and is not even sensible when only talking about only the spec of what it means for two terms to be convertible.

    Furthermore, note that the requirement of being able to run the typechecker in one's head is essentially the statement that the entire implementation is part of the specification.\footnote{%
      Thanks to Andres Erbsen for pointing this out to me.%
    }

    \minortodo{incorporate Andres' suggestions:
      you running the typechecker in your head is essentially the statement that if the entire implementation is part of the spec, it is possible to engineer better, and something close to this has been necessary in practice.
      the research direction you are advocating is finding a simpler performance-aware spec (perhaps by moving around interfaces or etc; my thought is that maybe we just want to get rid of the kernel and trust the proof engine).}
  \item
    Knowing how to tweak the built-in defaults for parts of the system which most users expect to be able to treat as black-boxes.
\end{enumerate}

\todo{adjust spacing of figure}
Note that even after this fix, the performance is \emph{still} exponential!
However, the performance is good enough that we deemed it not currently worth digging into the profile to understand the remaining bottlenecks.
See \autoref{fig:timing-montgomery-fesub-after-user}.

\begin{figure}[h]
    \beginTikzpictureStamped{
        \einput{fiat-crypto-perf-logs/montgomery-fesub-after.txt}
    }
    \begin{axis}[title={synthsis time (s) vs.\ \# limbs},
        legend pos=north west,
        width=0.4\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        xtick distance=2,
        ylabel style={
            yshift = {width("0")}
        }]
        \addplot[only marks,color=black] table[x=nlimbs,y=user]{fiat-crypto-perf-logs/montgomery-fesub-after.txt};
        \addplotexponentialregression[no markers, black][x=nlimbs,y=user,a=10,b=0.2]{fiat-crypto-perf-logs/montgomery-fesub-after.txt};
        \addlegendentry{synthesis}
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona} e^{\pgfmathprintnumber{\pgfplotstableregressionb} n} $
        }
    \end{axis}
\end{tikzpicture}
\quad
    \beginTikzpictureStamped{
        \einput{fiat-crypto-perf-logs/montgomery-fesub-after.txt}
    }
    \begin{axis}[title={synthsis time (s) vs.\ \# limbs ($\log$-scale)},
%        legend pos=north west,
        width=0.4\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        ymode=log,
        xtick distance=2,
        ylabel style={
            yshift = {width("0")}
        }]
        \addplot[only marks,color=black] table[x=nlimbs,y=user]{fiat-crypto-perf-logs/montgomery-fesub-after.txt};
        \addplotexponentialregression[no markers, black][x=nlimbs,y=user,a=10,b=0.2]{fiat-crypto-perf-logs/montgomery-fesub-after.txt};
%        \addlegendentry{synthesis}
%        \addlegendentry{
%            $\pgfmathprintnumber{\pgfplotstableregressiona} e^{\pgfmathprintnumber{\pgfplotstableregressionb} n} $
%        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of synthesizing subtraction after fixing the bottleneck}\label{fig:timing-montgomery-fesub-after-user}
\end{figure}


\section{Fixing Performance Bottlenecks in the Proof Assistant Itself is Also Hard}

In many domains, the performance challenges have been studied and understood, resulting in useful decompositions of the problem into subtasks that can be optimized independently.
It's rarely the case that disparate parts of a codebase must be simultaneously optimized to see any performance improvement at all.

We have not found any such study of performance challenges in proof assistants.
It seems to us that there are many disparate parts of any proof assistant satisfying the de Bruijn criterion which are deeply coupled and which cannot be performance-optimized independently.
There are many seemingly reasonable implementation choices that can be made for the kernel---the trusted proof checker---which make performance-optimizing the proof engine, which generates the proof, next to impossible.
Worse, if performance optimization is done incrementally, to avoid needless premature optimization, then it can be the case that performance-optimizing the kernel has effectively no visible impact; the most efficient proof engine design for the slower kernel might be inefficient in ways that prevent optimizations in the kernel from showing up in actual use cases, because simple proof engine implementations tend to avoid the performance bottlenecks of the kernel while simultaneously shadowing them with bottlenecks with similar performance characteristics.

\begin{comment}
\todo{Find a place for this (h/t conversation with Andres)}:  because we have a kernel and a proof engine on top of it, you need to simultaneously optimize the kernel and the proof engine to see performance improvements; if the kernel API doesn't give you good enough performance on primitives, then there's no hope to optimizing the proof engine, but at the same time if the proof engine is not optimized right, improvements in the performance of the kernel API don't have noticeable impact.

\todo{incorporate Andres' suggestions}
I like the last two sentences.
I would instead lead with something along the lines of ``in many domains, the performance challenges have been studied and understood, resulting in useful decompositions of the problem into subtasks that can be optimized independently.''
``in proof assistants, it doesn't look like anyone has even tried'' :P.
but e g signal processing was a huge mess too before the fast Fourier transform.
coq abstractions are mostly accidents of history.
no other system has a clear performance-conscious story for how these interfaces should be designed either.
\end{comment}




\section{The Four Axes of the Landscape}\label{sec:perf-axes}

We've now seen what super-linear scaling in dependently typed proof assistants looks like.
We've covered general arguments for why proof assistants might have such scaling, and what we believe broadly underpins the challenges of performance engineering in and on proof assistants.

The rest of this chapter is devoted to mapping out the landscape of performance bottlenecks we've encountered in a way that we hope will illuminate structure in the performance bottlenecks which are neither specific to the domain of the proof being checked nor general to all performance engineering.
We present a map of performance bottlenecks comprising four axes. 
These axes are by no means exhaustive, but, in our experience, most interesting performance bottlenecks scale as a super-linear factor of one or more of these axes.

\subsection{The Size of the Type} \label{sec:perf-axis:size-of-type}  \label{sec:quadratic-conj-certificate}

We start with one of the simplest axes.

Suppose we want to prove a conjunction of $n$ things, say, $\texttt{True} \wedge \texttt{True} \wedge \cdots \wedge \texttt{True}$.
For such a simple theorem, we want the size of the proof, and the time- and memory- complexity of checking it, to be linear in $n$.

Recall from \autoref{sec:debruijn-criterion} that we want a separation between the small trusted part of the proof assistant and the larger untrusted part.
The untrusted part generates certificates, which in dependently typed proof assistants are called terms, which the trusted part, the kernel, checks.

\minortodo{Mention possibility of not building proof terms at all somewhere}
\begin{minorcomment}
Andrew Appel said via private correspondence on May 7, 2020, 7:39 PM:
\begin{quotation}
There's some work on typechecking LF, in the Twelf system, where there can be performance bottlenecks if you're not careful.

The most glaringly obvious performance bottleneck in Coq is that it builds proof terms, when one should really use the futuristic technique of using data abstraction, in the type system, to distinguish ``proposition'' from ``theorem''; as done in that state-of-the-art system, Edinburgh LCF.  And presumably HOL, HOL light, Isabelle/HOL, etc.
\end{quotation}
\end{minorcomment}
\minortodo{mention the issue that LCF approach does not fully satisfy de Bruijn criterion, does not protect against plugins and Obj.magic}
\minortodo{share reasoning of ppedrot from \textcite{NewCoqTactics2016Pedrot}}

The obvious certificate to prove a conjunction $A \wedge B$ is to hold a certificate $a$ proving $A$ and a certificate $b$ proving $B$.
In Coq, this certificate is called \texttt{conj} and it takes four parameters: $A$, $B$, $a : A$, and $b : B$.
Perhaps you can already spot the problem.

To prove a conjunction of $n$ things, we end up repeating the type $n$ times in the certificate, resulting in a term that is quadratic in the size of the type.
We see in \autoref{fig:timing-conj-True-repeat-constructor} the time it takes to do this in Coq's tactic mode via \mintinline{coq}{repeat constructor}.
If we are careful to construct the certificate manually without duplicating work, we see that it takes linear time for Coq to build the certificate and quadratic time for Coq to check the certificate; see \autoref{fig:timing-conj-True-ltac2}.
\minortodo{improve data collection on perf test}
\minortodo{make a note about more complicated types causing scaling factors to be worse, and not just impacting the leaves}

\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/conj-True-repeat-constructor.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[col sep=comma,x=param-n,y=repeat-constructor-user]{performance-experiments/conj-True-repeat-constructor.txt};
        \addlegendentry{\mintinline{coq}{repeat constructor}}
        \addplotquadraticregression[no markers, black][x=param-n,y=repeat-constructor-user][col sep=comma]{performance-experiments/conj-True-repeat-constructor.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}n^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}n
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of \mintinline{coq}{repeat constructor} to prove a conjunction of $n$ \mintinline{coq}{True}s} \label{fig:timing-conj-True-repeat-constructor}
\end{figure}

\begin{figure}
\beginTikzpictureStamped{
    \einput{performance-experiments/conj-True-ltac2.txt}
}
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=blue] table[col sep=comma,x=param-n,y=typecheck-user]{performance-experiments/conj-True-ltac2.txt};
        \addlegendentry{typecheck}
        \addplotquadraticregression[no markers, blue][x=param-n,y=typecheck-user][col sep=comma]{performance-experiments/conj-True-ltac2.txt};
        \edef\tca{\pgfplotstableregressiona}
        \edef\tcb{\pgfplotstableregressionb}
        \edef\tcc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\tca}n^2
            \pgfmathprintnumber[print sign]{\tcb}n
            \pgfmathprintnumber[print sign]{\tcc}$
        }
        \addplot[only marks,mark=o,color=black] table[col sep=comma,x=param-n,y=build-user]{performance-experiments/conj-True-ltac2.txt};
        \addlegendentry{build}
        \addplot [thick,black] table[col sep=comma,x=param-n,
            y={create col/linear regression={y=build-user}}
            ] % compute a linear regression from the input table
            {performance-experiments/conj-True-ltac2.txt};
        \edef\builda{\pgfplotstableregressiona}
        \edef\buildb{\pgfplotstableregressionb}
        \addlegendentry{
            $\pgfmathprintnumber{\builda}n
            \pgfmathprintnumber[print sign]{\buildb}$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of manually building and typechecking a certificate to prove a conjunction of $n$ \mintinline{coq}{True}s using \LtacTwo} \label{fig:timing-conj-True-ltac2}
\end{figure}

Note that for small, and even medium-sized examples, it's pretty reasonable to do duplicative work.
It's only when we reach very large examples that we start hitting non-linear behavior.

There are two obvious solutions for this problem:
\begin{enumerate}
    \item
    We can drop the type parameters from the \texttt{conj} certificates.
    \item
    We can implement some sort of sharing, where common subterms of the type only exist once in the representation.
\end{enumerate}

\subsubsection{Dropping Type Parameters: Nominal vs.\ Structural Typing} \label{sec:nominal-vs-structural} \label{sec:dropping-constructor-parameters}
The first option requires that the proof assistant implement structural typing rather than nominal typing~\cite[19.3 Nominal and Structural Type Systems]{tapl}.
\minortodo{Find a place for this note: }Note that it doesn't actually require structural; we can do it with nominal typing if we enforce everywhere that we can only compare terms who are known to be the same type, because not having structural typing results in having a single kernel term with multiple non-unifiable types.
\minortodo{Look into \textcite{Efficient1998Necula} and \url{http://adam.chlipala.net/papers/StrictTLDI05/} for past work on eliding some parameters}
\minortodo{maybe look into TAPL}
\minortodo{explain structural and nominal typing more}
Morally, the reason for this is that if we have an inductive record type\minortodo{have we explained inductive types yet? }\minortodo{have we explained records yet?} whose fields do not constrain the parameters of the inductive type family\minortodo{have we explained parameters vs indices and inductive type families yet?}, then we need to consider different instantiations of the same inductive type family to be convertible.
That is, if we have a phantom record such as\minortodo{mention where the name ``phantom'' comes from?}
\begin{minted}{coq}
Record Phantom (A : Type) := phantom {}.
\end{minted}
and our implementation does not include \texttt{A} as an argument to \texttt{phantom}, then we must consider \texttt{phantom} to be both of type \texttt{Phantom nat} and \texttt{Phantom bool}, even though \texttt{nat} and \texttt{bool} are not the same.
I have requested this feature in \coqissue{5293}.
Note, however, that sometimes it is important for such phantom types to be considered distinct when doing type-level programming.
\minortodo{Come up with better justification for having nominal typing available?}

\subsubsection{Sharing} \label{sec:sharing}
The alternative to eliminating the duplicative arguments is to ensure that the duplication is at-most constant sized.
There are two ways to do this: either the user can explicitly share subterms so that the size of the term is in fact linear in the size of the goal, or the proof assistant can ensure maximal sharing of subterms.
\minortodo{explain this better}

There are two ways for the user to share subterms: using let-binders, and using function abstraction.
For example, rather than writing
\begin{minted}{coq}
@conj True (and True (and True True)) I (@conj True (and True True) I (@conj True True I I))
\end{minted}
and having roughly $n^2$ occurrences\footnote{The exact count is $n(n+1)/2 - 1$.} of \mintinline{coq}{True} when we are trying to prove a conjunction of $n$ \mintinline{coq}{True}s, the user can instead write
\begin{minted}{coq}
let T0 : Prop := True in
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
which has only $n$ occurrences of \mintinline{coq}{True}.
Alternatively, the user can write
\begin{minted}{coq}
(λ (T0 : Prop) (v0 : T0),
  (λ (T1 : Prop) (v1 : T1),
    (λ (T2 : Prop) (v2 : T2), @conj True T2 I v2)
      (and True T1) (@conj True T1 I v1))
    (and True T0) (@conj True T0 I v0))
  True I
\end{minted}

Unfortunately, both of these incur quadratic typechecking cost, even though the size of the term is linear.
See \autoref{fig:timing-conj-True-let-ltac2} and \autoref{fig:timing-conj-True-app-ltac2}.
\minortodo{improve data collection on perf test}

\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/conj-True-let-ltac2.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=blue] table[col sep=comma,x=param-n,y=typecheck-user]{performance-experiments/conj-True-let-ltac2.txt};
        \addlegendentry{typecheck}
        \addplotquadraticregression[no markers, blue][x=param-n,y=typecheck-user][col sep=comma]{performance-experiments/conj-True-let-ltac2.txt};
        \edef\tca{\pgfplotstableregressiona}
        \edef\tcb{\pgfplotstableregressionb}
        \edef\tcc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\tca}n^2
            \pgfmathprintnumber[print sign]{\tcb}n
            \pgfmathprintnumber[print sign]{\tcc}$
        }
        \addplot[only marks,mark=o,color=black] table[col sep=comma,x=param-n,y=build-user]{performance-experiments/conj-True-let-ltac2.txt};
        \addlegendentry{build}
        \addplot [thick,black] table[col sep=comma,x=param-n,
        y={create col/linear regression={y=build-user}}
        ] % compute a linear regression from the input table
        {performance-experiments/conj-True-let-ltac2.txt};
        \edef\builda{\pgfplotstableregressiona}
        \edef\buildb{\pgfplotstableregressionb}
        \addlegendentry{
            $\pgfmathprintnumber{\builda}n
            \pgfmathprintnumber[print sign]{\buildb}$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of manually building and typechecking a certificate to prove a conjunction of $n$ \mintinline{coq}{True}s using \mintinline{coq}{let}-binders using \LtacTwo} \label{fig:timing-conj-True-let-ltac2}
\end{figure}

\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/conj-True-app-ltac2.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=blue] table[col sep=comma,x=param-n,y=typecheck-user]{performance-experiments/conj-True-app-ltac2.txt};
        \addlegendentry{typecheck}
        \addplotquadraticregression[no markers, blue][x=param-n,y=typecheck-user][col sep=comma]{performance-experiments/conj-True-app-ltac2.txt};
        \edef\tca{\pgfplotstableregressiona}
        \edef\tcb{\pgfplotstableregressionb}
        \edef\tcc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\tca}n^2
            \pgfmathprintnumber[print sign]{\tcb}n
            \pgfmathprintnumber[print sign]{\tcc}$
        }
        \addplot[only marks,mark=o,color=black] table[col sep=comma,x=param-n,y=build-user]{performance-experiments/conj-True-app-ltac2.txt};
        \addlegendentry{build}
        \addplot [thick,black] table[col sep=comma,x=param-n,
        y={create col/linear regression={y=build-user}}
        ] % compute a linear regression from the input table
        {performance-experiments/conj-True-app-ltac2.txt};
        \edef\builda{\pgfplotstableregressiona}
        \edef\buildb{\pgfplotstableregressionb}
        \addlegendentry{
            $\pgfmathprintnumber{\builda}n
            \pgfmathprintnumber[print sign]{\buildb}$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of manually building and typechecking a certificate to prove a conjunction of $n$ \mintinline{coq}{True}s using abstraction and application using \LtacTwo} \label{fig:timing-conj-True-app-ltac2}
\end{figure}

Recall that the typing rules for $\lambda$ and \texttt{let} are as follows:\minortodo{cite appendix with typing rules of Coq?}
%\todo{maybe look in https://github.com/achlipala/frap and/or TAPL by Benjamin C. Pierce for how to render typing rules}
%\todoask{What's the suggested way of pretty-printing typing rules?}
%\todoask{Which way do substitution brackets go?}
%\todoask{What convention should we use for typing rules with regard to things being types?  Maybe just copy the HoTT book?}
\begin{prooftree}
  \AxiomC{$Γ, x:A ⊢ f : B$}
  \UnaryInfC{$Γ ⊢ (λ (x:A), f) : ∀ x:A, B$}
\end{prooftree}
\begin{prooftree}
  \AxiomC{$Γ ⊢ f : ∀ x:A, B$}
  \AxiomC{$Γ ⊢ a:A$}
  \BinaryInfC{$Γ ⊢ f(a) : B[a/x]$}
\end{prooftree}
\begin{comment}
\begin{verbatim}
Γ ⊢ A type      Γ, x:A ⊢ B type
Γ, x:A ⊢ f:B
-------------------------------
Γ ⊢ (λ (x:A), f) : ∀ x:A, B


Γ ⊢ A type      Γ, x:A ⊢ B type
Γ ⊢ f : ∀ x:A, B
Γ ⊢ y : A
-------------------------------
Γ ⊢ f y : B[x/y]
\end{verbatim}
\end{comment}

\begin{prooftree}
  \AxiomC{$Γ ⊢ a:A$}
  \AxiomC{$Γ, x : A ≔ a ⊢ f : B$}
  \BinaryInfC{$Γ ⊢ (\letin[{x : A ≔ a}{f}]) : B[a/x]$}
\end{prooftree}

\begin{comment}
\begin{verbatim}
Γ ⊢ A type
Γ ⊢ y : A
Γ, x:A:=y ⊢ B type
Γ, x:A:=y ⊢ f : B
-------------------------------
Γ ⊢ (let x : A := y in f) : B[x/y]
\end{verbatim}
\end{comment}

Let us consider the inferred types for the intermediate terms when typechecking the \mintinline{coq}{let} expression:
\begin{itemize}
  \item
  We infer the type \mintinline{coq}{and True T2} for the expression
\begin{minted}{coq}
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v2} into that type to type the expression
\begin{minted}{coq}
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We substitute \mintinline{coq}{T2 := and True T1} into this type to get the type \mintinline{coq}{and True (and True T1)} for the expression
\begin{minted}{coq}
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v1} into this type to get the type for the expression
\begin{minted}{coq}
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We substitute \mintinline{coq}{T1 := and True T0} into this type to get the type \mintinline{coq}{and True (and True (and True T0))} for the expression
\begin{minted}{coq}
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  We perform the no-op substitution of \mintinline{coq}{v0} into this type to get the type for the expression
\begin{minted}{coq}
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
  \item
  Finally, we substitute \mintinline{coq}{T0 := True} into this type to get the type \mintinline{coq}{and True (and True (and True True))} for the expression
\begin{minted}{coq}
let T0 : Prop := True in
let v0 : T0   := I in
let T1 : Prop := and True T0 in
let v1 : T1   := @conj True T0 I v0 in
let T2 : Prop := and True T1 in
let v2 : T2   := @conj True T1 I v0 in
@conj True T2 I v2
\end{minted}
\end{itemize}
Note that we have performed linearly many substitutions into linearly-sized types, so unless substitution is constant time in size of the term being substituted, we incur quadratic overhead here.
The story for function abstraction is similar.
\minortodo{cite https://github.com/coq/coq/issues/8232 maybe?}

\minortodo{Should we run though typechecking in more detail here?}

We again have two choices to fix this:
either we can change the typechecking rules (which work just fine for small-to-medium-sized terms), or we can adjust typechecking to deal with some sort of pending substitution data, so that we only do substitution once.

\minortodo{maybe cite https://github.com/coq/coq/issues/11838?}

\minortodo{reference quadratic cbv here, which had a similar issue?}

\minortodo{some sort of section division marker here?}

The proof assistant can also try to heuristically share subterms for us.
Many proof assistants do some version of this, called \emph{hash consing}.
\minortodo{explain and cite hash consing?}

However, hash consing looses a lot of its benefit if terms are not maximally shared (and they almost never are), and can lead to very unpredictable performance when transformations unexpectedly cause a loss of sharing.
\minortodo{cite hash consing needing to be full to get perf benefit}
Furthermore, it's an open problem how to efficiently persist full hash consing to disk in a way that allows for diamond dependencies.
\minortodo{explain this more, find citation for hash consing being hard with disk}
\minortodo{flesh out hash consing section more}
\minortodo{maybe cite https://github.com/coq/coq/issues/9028\#issuecomment-600013284 about hash consing being slow}

\subsection{The Size of the Term}

Recall that Coq (and dependently typed proof assistants in general) have \emph{terms} which serve as both programs and proofs.
The essential function of a proof checker is to verify that a given term has a given type.
We obviously cannot type-check a term in better than linear time in the size of the representation of the term.

Recall that we cannot place any hard bounds on complexity of typechecking a term, as terms as simple as \mintinline{coq}{@eq_refl bool true} proving that the boolean \mintinline{coq}{true} is equal to itself can also be typechecked as proofs of arbitrarily complex decision procedures returning success.

We might reasonably hope that typechecking problems which require no interesting computation can be completed in time linear in the size of the term and its type.

However, some seemingly reasonable decisions can result in typechecking taking quadratic time in the size of the term, as we saw in \autoref{sec:sharing}.

\minortodo{maybe move some text form the sharing section to here?}

Even worse, typechecking can easily be unboundedly large in the size of the term when the typechecker chooses the wrong constants to unfold, even when very little work ought to be done.

\minortodo{discussion of conversion checking, and conversion modulo delta-beta}

Consider the problem of typechecking \mintinline{coq}{@eq_refl nat (fact 100) : @id nat (fact 100) = fact 100}, where \mintinline{coq}{fact} is the factorial function on natural numbers and \mintinline{coq}{id} is the polymorphic identity function.
\minortodo{should we define polymorphic identity function somewhere?}
If the typechecker either decides to unfold \mintinline{coq}{id} before unfolding \mintinline{coq}{fact}, or if it performs a breath-first search, then we get speedy performance.
However, if the typechecker instead unfolds \mintinline{coq}{id} \emph{last}, then we end up computing the normal form of $100!$, which takes a long time and a lot of memory.
See \autoref{fig:timing-eq-refl-nat-factorial}.

\begin{figure}
  \centering
    \beginTikzpictureStamped{
        \einput{performance-experiments/eq-refl-nat-factorial.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.4\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[col sep=comma,x=param-n,y=constr-eq-refl-user]{performance-experiments/eq-refl-nat-factorial.txt};
        \addlegendentry{typechecking}
        \addplotfactorialregression[no markers, black][x=param-n,y=constr-eq-refl-user][col sep=comma]{performance-experiments/eq-refl-nat-factorial.txt};
        \addlegendentry{$\pgfmathprintnumber{\pgfplotstableregressiona}n!$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of typechecking \mintinline{coq}{@eq_refl nat (fact n) : @id nat (fact n) = fact n}} \label{fig:timing-eq-refl-nat-factorial}
\end{figure}

Note that it is by no means obvious that the typechecker can meaningfully do anything about this.
Breath-first search is significantly more complicated than depth-first, is harder to write good heuristics for, can incur enormous space overheads, and can be massively slower in cases where there are many options and the standard heuristics for depth-first unfolding in conversion-checking are sufficient.
Furthermore, the more heuristics there are to tune conversion-checking, the more ``magic'' the algorithm seems, and the harder it is to debug when the performance is inadequate.

As described in \autoref{sec:fiat-crypto-codegen-numbers}, in fiat-crypto\todonz{first ref to name}, we got exponential slowdown due to this issue, with an estimated overhead of over four thousand millennia of extra typechecking time in the worst examples we were trying to handle.

\minortodo{maybe forward reference to the number of abstraction barriers}
\minortodo{maybe include more about real-world fiat-crypto example here?}

\subsection{The Number of Binders} \label{sec:perf:binder-count}

This is a particular subcase of the above sections that we call out explicitly.
Often there will be some operation (for example, substitution, lifting, context-creation) that needs to happen every time there is a binder, and which, when done naïvely, is linear in the size of the term or the size of the context.
As a result, naïve implementations will often incur quadratic---or worse---overhead in the number of binders.

\minortodo{make sure we've explained proof engine and Ltac by here}

Similarly, if there is any operation that is even linear rather than constant in the number of binders in the context, then and user operation in proof mode which must be done, say, for each hypothesis, will incur an overall quadratic-or-worse performance penalty.

The claim of this subsection is not that any particular application is inherently constrained by a performance bottleneck in the number of binders, but instead that it's very, very easy to end up with quadratic-or-worse performance in the number of binders, and hence that this forms a meaningful cluster for performance bottlenecks in practice.

I will attempt to demonstrate this point with a palette of actual historical performance issues in Coq---some of which persist to this day---where the relevant axis was ``number of binders.''
None of these performance issues are insurmountable, but all of them are either a result of seemingly reasonable decisions, have subtle interplay with seemingly disparate parts of the system, or else are to this day still mysterious despite the work of developers to investigate them.

\subsubsection{Name Resolution} \label{sec:name-resolution}
One key component of interactive proof assistants is figuring out which constant is referred to by a given name.
It may be tempting to keep the context in an array or linked list.
However, if looking up which constant or variable is referred to by a name is $\mathcal O(n)$, then internalizing a term with $n$ typed binders is going to be $\mathcal O(n^2)$, because we need to do name lookups for each binder.
See \coqbug{9582} and \coqpr{9586}.

See \autoref{fig:timing-name-resolution} for the timing of name resolution in Coq.
\todonz{More details about what the graph shows}
See \autoref{fig:timing-nested-lambda-different-name} for the effect on internalizing a lambda with $n$ arguments.

\minortodo{mention Coq version automatically, mention why we're using a different Coq version}
\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments-8-9/name-resolution.txt}
    }
    \begin{axis}[xlabel=\# binders,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        ylabel style={
            yshift = {width("$8\cdot 1$")}
        }]
        \addplot[only marks,color=black] table[col sep=comma,x=param-n,y=uconstr-I-1000-user]{performance-experiments-8-9/name-resolution.txt};
        \addlegendentry{1000 name resolutions}
        \addplot [thick,black] table[col sep=comma,x=param-n,
        y={create col/linear regression={y=uconstr-I-1000-user}}
        ]{performance-experiments-8-9/name-resolution.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}$}
    \end{axis}
\end{tikzpicture}
\caption{Timing of internalizing a name 1000 times under $n$ binders} \label{fig:timing-name-resolution}
\end{figure}
\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments-8-9/nested-lambda-different-name.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false,
        ylabel style={
            yshift = {width("$8\cdot{}$")}
        }]
        \addplot[only marks,color=black] table[col sep=comma,x=param-n,y=uconstr-lambda-user]{performance-experiments-8-9/nested-lambda-different-name.txt};
        \addlegendentry{parsing and internalization}
        \addplotquadraticregression[no markers, black][x=param-n,y=uconstr-lambda-user][col sep=comma]{performance-experiments-8-9/nested-lambda-different-name.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}n^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}n
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of internalizing a function with $n$ differently-named arguments of type \mintinline{coq}{True}} \label{fig:timing-nested-lambda-different-name}
\end{figure}

\minortodo{Is this called internalization or is it called elaboration}

\subsubsection{Capture-Avoiding Substitution} \label{sec:perf:capture-avoiding-subst}
If the user is presented with a proof engine interface where all context variables are named, then in general the proof engine must implement capture-avoiding substitution.
For example, if the user wants to operate inside the hole in \mintinline{coq}{(λ x, let y := x in λ x, _)}, then the user needs to be able to talk about the body of \mintinline{coq}{y}, which is not the same as the innermost \mintinline{coq}{x}.
However, if the $\alpha$-renaming is even just linear in the existing context, then creating a new hole under $n$ binders will take $\mathcal{O}(n^2)$ time in the worst case, as we may have to do $n$ renamings, each of which take time $\mathcal O(n)$.
See \coqbug{9582}, perhaps also \coqbug{8245} and \coqbug{8237} and \coqbug{8231}.

This might be the cause of the difference in \autoref{fig:timing-open-constr-n-lambda-no-types} between having different names (which do not need to be renamed) and having either no name (requiring name generation) or having all binders with the same name (requiring renaming in evar substitutions).

\minortodo{ADD PLOT: try to come up with a graph for renaming stuff}
\minortodo{ADD PLOT: check if confounders come up}

\subsubsection{Quadratic Creation of Substitutions for Existential Variables} \label{sec:perf:quadratic-evar-subst}
Recall \minortodo{make sure that this is mentioned previously, and that we're not rehashing things too much} that when we separate the trusted kernel from the untrusted proof engine, we want to be able to represent not-yet-finished terms in the proof engine.
The standard way to do this is to enrich the type of terms with an ``existential variable'' node, which stands for a term which will be filled later.
\minortodo{cite original idea for evars? (what is it?)}
Such existential variables, or evars, typically exist in a particular context.
That is, you have access to some hypotheses but not others when filling an evar.

Sometimes, reduction results in changing the context in which an evar exists.
For example, if we want to $\beta$-reduce \mintinline{coq}{(λ x, ?e₁) (S y)}, then the result is the evar \mintinline{coq}{?e₁} with \mintinline{coq}{S y} substituted for \mintinline{coq}{x}.

There are a number of ways to represent substitution, and the choices are entangled with the choices of term representation.

Note that most substitutions are either identity or lifting substitutions.
\minortodo{define identity and lifting substitutions}

One popular representation is the locally nameless representation~\cite{Locally2012Chargueraud,locally2007Leroy}, which we discuss more in \autoref{sec:binders:locally-nameless}.
\minortodo{Justify it?  Discuss other representations? Say why and how it's convenient?}
However, if we use a locally nameless term representation, then finding a compact representation for identity and lifting substitutions is quite tricky.
If the substitution representation takes $\mathcal O(n)$ time to create in a context of size $n$, then having a $\lambda$ with $n$ arguments whose types are not known takes $\mathcal O(n^2)$ time, because we end up creating identity substitutions for $n$ holes, with linear-sized contexts.

Note that fully nameless, i.e., de Bruijn term representations, do not suffer from this issue.

See \coqbug{8237} and \coqpr{11896} for a mitigation of some (but not all) issues.

See also \autoref{fig:timing-do-n-open-constr-True} and \autoref{fig:timing-open-constr-n-lambda-no-types}.

\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/do-n-open-constr-True.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[col sep=comma,x=param-n,y=open-constr-True-1000-user]{performance-experiments/do-n-open-constr-True.txt};
        \addlegendentry{construct an evar 1000 times}
        \addplot [thick,black] table[col sep=comma,x=param-n,
         y={create col/linear regression={y=open-constr-True-1000-user}}
         ] % compute a linear regression from the input table
         {performance-experiments/do-n-open-constr-True.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}n
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of generating 1000 evars in a context of size $n$} \label{fig:timing-do-n-open-constr-True}
\end{figure}

\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/open-constr-n-lambda-no-types-same-names.txt}
        \einput{performance-experiments/open-constr-n-lambda-no-types-no-names.txt}
        \einput{performance-experiments/open-constr-n-lambda-no-types-different-names.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=*,color=black] table[col sep=comma,x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-same-names.txt};
        \addlegendentry{same names}
        \addplotquadraticregression[no markers, black][x=param-n,y=open-constr-user][col sep=comma]{performance-experiments/open-constr-n-lambda-no-types-same-names.txt};
        \edef\ocsamea{\pgfplotstableregressiona}
        \edef\ocsameb{\pgfplotstableregressionb}
        \edef\ocsamec{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\ocsamea}n^2
            \pgfmathprintnumber[print sign]{\ocsameb}n
            \pgfmathprintnumber[print sign]{\ocsamec}$
        }

        \addplot[only marks,mark=o,color=blue] table[col sep=comma,x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-no-names.txt};
        \addlegendentry{no names}
        \addplotquadraticregression[no markers, blue][x=param-n,y=open-constr-user][col sep=comma]{performance-experiments/open-constr-n-lambda-no-types-no-names.txt};
        \edef\ocnoa{\pgfplotstableregressiona}
        \edef\ocnob{\pgfplotstableregressionb}
        \edef\ocnoc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\ocnoa}n^2
            \pgfmathprintnumber[print sign]{\ocnob}n
            \pgfmathprintnumber[print sign]{\ocnoc}$
        }

        \addplot[only marks,mark=+,color=red] table[col sep=comma,x=param-n,y=open-constr-user]{performance-experiments/open-constr-n-lambda-no-types-different-names.txt};
        \addlegendentry{different names}
        \addplotquadraticregression[no markers, red][x=param-n,y=open-constr-user][col sep=comma]{performance-experiments/open-constr-n-lambda-no-types-different-names.txt};
        \edef\ocdifferenta{\pgfplotstableregressiona}
        \edef\ocdifferentb{\pgfplotstableregressionb}
        \edef\ocdifferentc{\pgfplotstableregressionc}
        \addlegendentry{
            $\pgfmathprintnumber{\ocdifferenta}n^2
            \pgfmathprintnumber[print sign]{\ocdifferentb}n
            \pgfmathprintnumber[print sign]{\ocdifferentc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of generating a $\lambda$ with $n$ binders of unknown/evar type, all of which have either no name, the same name, or different names} \label{fig:timing-open-constr-n-lambda-no-types}
\end{figure}

\subsubsection{Quadratic Substitution in Function Application} \label{sec:perf:quadratic-application}
Consider the case of typechecking a non-dependent function applied to $n$ arguments.
If substitution is performed eagerly, following directly the rules of the type theory, \minortodo{cite the rules / reference an appendix, }then typechecking is quadratic.
This is because the type of the function is $\mathcal{O}(n)$, and doing substitution $n$ times on a term of size $\mathcal{O}(n)$ is quadratic.

If the term representation contains $n$-ary application nodes, it's possible to resolve this performance bottleneck by delaying the substitutions.
If only unary application nodes exist, it's much harder to solve.

Note that this is important, for example, if you want to avoid the problem of quadratically-sized certificates by making a $n$-ary conjunction-constructor which is parameterized on a list of the conjuncts.
Such a function could then be applied to the $n$ proofs of the conjuncts.

See \coqbug{8232} and \coqbug{12118} and \coqpr{8255}.

See \autoref{fig:timing-app-n}.

\todonz{[previous two paragraphs] say more}

\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/app-n-ltac2.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[col sep=comma,x=param-n,y=typecheck-user]{performance-experiments/app-n-ltac2.txt};
        \addlegendentry{typecheck}
        \addplotquadraticregression[no markers, black][x=param-n,y=typecheck-user][col sep=comma]{performance-experiments/app-n-ltac2.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}n^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}n
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
            }
    \end{axis}
\end{tikzpicture}
\caption{Timing of typechecking a function applied to $n$ arguments} \label{fig:timing-app-n}
\end{figure}

\subsubsection{Quadratic Normalization by Evaluation} \label{sec:perf:quadratic-NbE} \label{sec:perf:quadratic-cbv}
Normalization by evaluation (NbE) is a nifty way to implement reduction where function abstraction in the object language is represented by function abstraction in the metalanguage.
We discuss the details of how to implement NbE in \autoref{sec:thunk-eval-subst-term}.
\minortodo{explain NbE?}
\minortodo{Read stuff in \url{https://github.com/HoTT/book/issues/995\#issuecomment-418825844} for NbE stuff}
\minortodo{Say more about it than that it's ``nifty''}
Coq uses NbE to implement two of its reduction machines (\mintinline{coq}{lazy} and \mintinline{coq}{cbv}).

The details of implementing NbE depend on the term representation used.
If a fancy term encoding like PHOAS, which we explain in \autoref{sec:binders:PHOAS}, is used\minortodo{don't split the ``PHOAS ... is used'' with text}, then it's not hard to implement a good NbE algorithm.
However, such fancy term representations incur unpredictable and hard-to-deal-with performance costs.
Most languages do not do any reduction on thunks until they are called with arguments, which means that forcing early reduction of a PHOAS-like term representation requires round-tripping though another term representation, which can be costly on large terms if there is not much to reduce.
On the other hand, other term representations need to implement either capture-avoiding substitution (for named representations) or index lifting (for de Bruijn and locally nameless representations).

The sort-of obvious way to implement this transformation is to write a function that takes a term and a binder, and either renames the binder for capture-avoiding substitution or else lifts the indices of the term.
The problem with this implementation is that if you call it every time you move a term under a binder, then moving a term under $n$ binders traverses the term $n$ times.
If the term size is also proportional to $n$, then the result is quadratic blowup in the number of binders.

See \coqbug{11151} for an occurrence of this performance issue in the wild in Coq.
See also \autoref{fig:timing-quadratic-cbv-lazy-PHOAS}.


\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/quadratic-cbv-lazy-PHOAS.txt}
    }
    \begin{axis}[xlabel=\# binders,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=o,color=blue] table[col sep=comma,x=param-num-binders,y=cbv-user]{performance-experiments/quadratic-cbv-lazy-PHOAS.txt};
        \addlegendentry{\mintinline{coq}{cbv}}
        \addplotquadraticregression[no markers, blue][x=param-num-binders,y=cbv-user][col sep=comma]{performance-experiments/quadratic-cbv-lazy-PHOAS.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
        \addplot[only marks,mark=*,color=black] table[col sep=comma,x=param-num-binders,y=lazy-user]{performance-experiments/quadratic-cbv-lazy-PHOAS.txt};
        \addlegendentry{\mintinline{coq}{lazy}}
        \addplotquadraticregression[no markers, black][x=param-num-binders,y=lazy-user][col sep=comma]{performance-experiments/quadratic-cbv-lazy-PHOAS.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of running \mintinline{coq}{cbv} and \mintinline{coq}{lazy} reduction on interpreting a PHOAS expression as a function of the number of binders} \label{fig:timing-quadratic-cbv-lazy-PHOAS}
\end{figure}

\subsubsection{Quadratic Closure Compilation} \label{sec:perf:closure-compilation} \label{sec:perf:quadratic-vm-native}
It's important to be able to perform reduction of terms in an optimized way.
\minortodo{maybe give a better introductory justification to the vm and native compiler than ``it's important''?}
When doing optimized reduction in an imperative language, we need to represent closures---abstraction nodes---in some way.
Often this involves associating to each closure both some information about or code implementing the body of the function, as well as the values of all of the free variables of that closure~\cite{Efficient2000Shao}.
%\minortodo{cite \url{https://flint.cs.yale.edu/shao/papers/escc.html} or its references for closure compilation}
In order to have efficient lookup, we need to know the memory location storing the value of any given variable statically at closure-compilation time.
The standard way of doing this \minortodo{cite something for flat closure compilation} is to allocate an array of values for each closure.
If variables are represented with de Bruijn indices, for example, it's then a very easy array lookup to get the value of any variable.
Note that this allocation is linear in the number of free variables of a term.
If we have many nested binders and use all of them underneath all the binders, then every abstraction node has as many free variables as there are total binders, and hence we get quadratic overhead.

See \coqbug{11151} and \coqbug{11964} and \ocamlbug{7826} for an occurrence of this issue in the wild.
Note that this issue rarely shows up in hand-written code, only in generated code, so developers of compilers such as \texttt{ocamlc} and \texttt{gcc} might be uninterested in optimizing this case.
However, it's quite essential when doing meta-programming involving large generated terms.
It's especially essential if we want to chain together reflective automation passes that operate on different input languages and therefore require denotation and reification between the passes.
In such cases, unless our encoding language uses named or de Bruijn variable encoding, there's no way to avoid large numbers of nested binders at compilation time while preserving code sharing.
Hence if we're trying to reuse the work of existing compilers to bootstrap good performance of reduction (as is the case for the native compiler in Coq), we have trouble with cases such as this one.
\minortodo{reorganize this paragraph, improve it}

See also \autoref{fig:timing-quadratic-vm-PHOAS} and \autoref{fig:timing-quadratic-native-PHOAS}.

\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/quadratic-vm-PHOAS.txt}
    }
    \begin{axis}[xlabel=\# binders,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[col sep=comma,x=param-num-binders,y=vm-compute-user]{performance-experiments/quadratic-vm-PHOAS.txt};
        \addlegendentry{\mintinline{coq}{vm_compute}}
        \addplotquadraticregression[no markers, black][x=param-num-binders,y=vm-compute-user][col sep=comma]{performance-experiments/quadratic-vm-PHOAS.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of running \mintinline{coq}{vm_compute} reduction on interpreting a PHOAS expression as a function of the number of binders} \label{fig:timing-quadratic-vm-PHOAS}
\end{figure}

\begin{figure}
    \beginTikzpictureStamped{
        \einput{performance-experiments/quadratic-native-PHOAS.txt}
    }
    \begin{axis}[xlabel=\# binders,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,color=black] table[col sep=comma,x=param-num-binders,y=native-compute-real]{performance-experiments/quadratic-native-PHOAS.txt};
        \addlegendentry{\mintinline{coq}{native_compute}}
        \addplotquadraticregression[no markers, black][x=param-num-binders,y=native-compute-real][col sep=comma]{performance-experiments/quadratic-native-PHOAS.txt};
        \addlegendentry{
            $\pgfmathprintnumber{\pgfplotstableregressiona}(\text{\# binders})^2
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionb}(\text{\# binders})
            \pgfmathprintnumber[print sign]{\pgfplotstableregressionc}$
        }
    \end{axis}
\end{tikzpicture}
\caption{Timing of running \mintinline{coq}{native_compute} reduction on interpreting a PHOAS expression as a function of the number of binders} \label{fig:timing-quadratic-native-PHOAS}
\end{figure}

\minortodo{maybe more examples here?}

\subsection{The Number of Nested Abstraction Barriers} \label{sec:axis-nested-abstraction-barriers}

\minortodo{ADD PLOT: This section requires digging into the historical performance issues around this to find convincing stand-alone examples so we can have graphs here.}

This axis is the most theoretical of the axes.
An abstraction barrier is an interface for making use of code, definitions, and theorems.
For example, you might define non-negative integers using a binary representation, and present the interface of zero, successor, and the standard induction principle, along with an equational theory for how induction behaves on zero and successor.
\minortodo{should I spell this example out more?}
You might use lists and non-negative integers to implement a hash-set datatype for storing sets of hashable values, and present the hash-set with methods for empty, add, remove, membership-testing, and some sort of fold.
Each of these is an abstraction barrier.

There are three primary ways that nested abstraction barriers can lead to performance bottlenecks: one involving conversion missteps and two involving exponential blow-up in the size of types.

\subsubsection{Conversion Troubles} \label{sec:abstraction-barriers:conversion-troubles}
If abstraction barriers are not perfectly opaque---that is, if the typechecker ever has to unfold the definitions making up the API in order to typecheck a term---then every additional abstraction barrier provides another opportunity for the typechecker to pick the wrong constant to unfold first.
\minortodo{add example?}
In some typecheckers, such as Coq, it's possible to provide hints to the typechecker to inform it which constants to unfold when.
In such a system, it's possible to carefully craft conversion hints so that abstraction barriers are always unfolded in the right order.
Alternatively, it might be possible to carefully craft a system which picks the right order of unfolding by using a dependency analysis.

However, most users don't bother to set up hints like this, and dependency analysis isn't sufficient to determine which abstraction barrier is ``higher up'' when there are many parts of it, only some of which are mentioned in any given part of the next abstraction barrier.
The reason users don't set up hints like this is that usually it's not necessary.
There's often minimal overhead, and things just work, even when the wrong path is picked---until the number of abstraction barriers or the size of the underlying term gets large enough.
Then we get noticeable exponential blowup, and everything is sad.
\todonz{[re ``is sad''] more precise?}
Furthermore, it's hard to know which part of conversion is incurring exponential blowup, and thus one has to basically get all of the conversion hints right, simultaneously, without any feedback, to see any performance improvement.



\subsubsection{Type Size Blowup: Abstraction Barrier Mismatch} \label{sec:abstraction-barriers:mismatch}
When abstraction barriers are leaky or misaligned, there's a cost that accumulates in the size of the types of theorems.
Consider, for example, the two different ways of using tuples:
(1) we can use the projections \mintinline{coq}{fst} and \mintinline{coq}{snd}; or
(2) we can use the eliminator \mintinline{coq}{pair_rect : ∀ A B (P : A × B → Type), (∀ a b, P (a, b)) → ∀ x, P x}.
The first gets us access to one element of the tuple at a time, while the second has us using all elements of the tuple simultaneously.

Suppose now there is one API defined in terms of \mintinline{coq}{fst} and \mintinline{coq}{snd}, and another API defined in terms of \mintinline{coq}{pair_rect}.
To make these APIs interoperate, we need to explicitly convert from one representation to another.
Furthermore, every theorem about the composition of these APIs needs to include the interoperation in talking about how they relate.

If such API mismatches are nested, or if this code size blowup interacts with conversion missteps, then the performance issues compound.

\minortodo{maybe be more concrete here?}

\minortodo{transition?}

\label{sec:abstraction-barriers:mismatch:example:exponential-laws:main-text}
Let us consider things a bit more generally.

\citetitle{Structure1996Sussman} defines abstraction as naming and manipulating compound elements as units~\cite[p.~6]{Structure1996Sussman}.
An \emph{abstraction barrier} is a collection of definitions and theorems about those definitions that together provide an interface such a compound element.
For example, we might define an interface for sorting a list, together with a proof that sorting any list results in a sorted list.
Or we might define an interface for key-value maps (perhaps implemented as association lists, or hash-maps, or binary search trees, or in some other way).

\emph{Piercing} an abstraction barrier is the act of manipulating the compound element by its components, rather than through the interface.
For example, suppose we have implemented key-value maps as association lists, representing the map as a list of key-value pairs, and provided some interface.
Any function which, for example, asks for the first element of the association list has pierced the abstraction barrier of our interface.

We might say that an abstraction barrier is \emph{leaky} if we ever need to pierce it, or perhaps if our program does in fact pierce the abstraction barrier, even if the piercing is needless.
(Which definition we choose is not of great significance for this thesis.)

In proof assistants like Coq, using \mintinline{coq}{unfold}, \mintinline{coq}{simpl}, or \mintinline{coq}{cbn} can often indicate a leaky abstraction barrier, where in order to prove a property we unfold the interface we are given to see how it is implemented.
This is all well and good when we are in the process of defining the abstraction barrier---unfolding the definition of sorting a list, for example, to prove that sorting the list gives back a list with all the same elements---but can be problematic when used more pervasively.

Let us look at an example from a category theory library we implemented in Coq~\cite{category-coq-experience}, which we introduce in \autoref{sec:category-theory-library}.
Category theory generalizes functions and product types, and the example we present here is a category-theoretic version of the isomorphism between functions of type $C_1 \times C_2 \to D$ which take a pair of elements $c_1 \in C_1$ and $c_2 \in C_2$ and return an element of $D$, and functions of type $C_1 \to (C_2 \to D)$ which take a single argument $c_1 \in C_1$ and return a function from $C_2$ to $D$.
We write this isomorphism as
\[
(C₁ × C₂ → D) ≅ (C₁ → (C₂ → D))
\]
In computer science, this is known as (un)currying.
The abstractions used in formalizing this example are as follows
\begin{itemize}
\item
  A \emph{category} $\mathcal C$ is a collection of objects and composable arrows (called \emph{morphisms}) between those objects, subject to some algebraic laws.
  The class of objects is generally denoted $\Ob[\mathcal C]$ and the class of morphisms between $x, y\in \Ob[\mathcal C]$ is generally denoted $\Hom[\mathcal C](x, y)$.
  Categories are a sort of generalization of sets or types.
\item
  The \emph{product category} $\mathcal C \times \mathcal D$ generalizes the Cartesian product of sets.
\item
  An \emph{isomorphism} between objects $x$ and $y$ in a category $\mathcal C$, written $x \cong y$, is a pair of morphisms from $x$ to $y$ and from $y$ to $x$ such that the composition in either direction is the identity morphism.
\item
  A \emph{functor} is an arrow between categories, mapping objects to objects and morphisms to morphisms, subject to some algebraic laws.
  The action of a functor $F$ on an object $x$ is often denoted $F(x)$.
  As the action of $F$ on a morphism $m$ is often also denoted $F(m)$, we will use $F_0$ to denote the action on objects and $F_1$ to denote the action on morphisms when it might otherwise be unclear.
\item
  A \emph{natural transformation} is an arrow between functors $F$ and $G$ consisting of a way of mapping from the on-object-action of $F$ to the on-object-action of $G$, satisfying some algebraic laws.
\item
  A category of functors $\mathcal C \to \mathcal D$ is the category whose objects are functors from $\mathcal C$ to $\mathcal D$ and whose morphisms are natural transformations.
  This category generalizes the notion of function types or of sets of functions.
\item
  The category of categories, generally denoted \Cat, is a category whose objects are themselves categories and whose morphisms are functors.
  Much like the set of all sets or the type of all types, the categories in \Cat\space are subject to size restrictions discussed further in \autoref{sec:category-of-categories}.
\end{itemize}
Although we eventually go into a bit more of the detail of these definitions throughout \autoref{sec:category-def} and again in \autoref{sec:category-def}, we advise the interested reader to consult the rich existing literature on category theory, including for example \textcite{awodey2010category} and \textcite{mac1998categories}.

There are only seven components of the isomorphism $(\mathcal C₁ × \mathcal C₂ → \mathcal D) ≅ (\mathcal C₁ → (\mathcal C₂ → \mathcal D))$ which are not proofs of algebraic laws.
Their definition, spelled out in \autoref{fig:exponential-laws:def:math} and given in Gallina Coq code (with suitable notations) in \autoref{fig:exponential-laws:def:coq}, is relatively trivial.

Typechecking the code that defines these components, however, takes nearly two seconds!
This is more than 200$\times$ slower than defining this data in the particular case of the category of sets.%
\footnote{%
  See \autoref{sec:abstraction-barriers:mismatch:example:exponential-laws:full-code:sets} for the code used to make this timing measurement.%
}
We attribute this to the large types generated and the non-trivial conversion problems which require unfolding various definitions, i.e., piercing various abstraction barriers.

\begin{figure}
  To define currying, going from $(\mathcal C₁ × \mathcal C₂ → \mathcal D)$ to $(\mathcal C₁ → (\mathcal C₂ → \mathcal D))$:
  \begin{enumerate}
  \item
    Each functor $F : \mathcal C₁ × \mathcal C₂ → \mathcal D$ gets mapped to a functor which takes in an object $c_1 \in \Ob[\mathcal C₁]$ and returns a functor which takes in an object $c_2 \in \Ob[\mathcal C₂]$ and returns the object $F((c_1, c_2)) \in \Ob[\mathcal D]$.
  \item
    The action of the returned functor on morphisms in $\mathcal C₂$ is to first lift this morphism from $\mathcal C₂$ to $\mathcal C₁ × \mathcal C₂$ by pairing with the identity morphism on $c_1$, and then to return the image of this morphism under $F$.
  \item
    The action of the outer functor on morphisms $m_1 \in \Hom[\mathcal C₁]$ is to return the natural transformation which, for each object $c_2\in \Ob[\mathcal C₂]$ first pairs the morphism $m_1$ with the identity on $c_2$ and then returns the image of this morphism in $\mathcal C₁ × \mathcal C₂$ under $F$.
  \item
    Each natural transformation $T \in \Hom[\mathcal C₁ × \mathcal C₂ → \mathcal D]$ gets mapped to the natural transformation in $\mathcal C₁ → (\mathcal C₂ → \mathcal D)$ which, after binding $c_1$ and $c_2$ returns the morphism in $\mathcal D$ given by the action of $T$ on $(c_1, c_2)$.
  \end{enumerate}
  To define uncurrying, going from $(\mathcal C₁ → (\mathcal C₂ → \mathcal D))$ to $(\mathcal C₁ × \mathcal C₂ → \mathcal D)$:
  \begin{enumerate}[resume]
  \item
    Each functor $F : \mathcal C₁ → (\mathcal C₂ → \mathcal D)$ gets mapped to the functor which takes in an object $(c_1, c_2)\in \Ob[\mathcal C₁ × \mathcal C₂]$ and returns $(F(c_1))(c_2)$.
  \item
    The action of this functor on morphisms $(m_1, m_2) \in \Hom[\mathcal C₁ × \mathcal C₂]$ is to compose $F(m_1)$ applied to a suitable object of $\mathcal C_2$ with $F$ applied to a suitable object of $c_1$ and then applied to $m_2$.
  \item
    Each natural transformation $T \in \Hom[\mathcal C₁ → (\mathcal C₂ → \mathcal D)]$ gets mapped to the natural transformation which maps each object $(c_1, c_2)\in \Ob[\mathcal C₁ × \mathcal C₂]$ to the morphism $(T(c_1))(c_2)$ in $\Hom[\mathcal D]$.
  \end{enumerate}
  While this is a mouthful, there is no insight in any of these definitions; for each component, there is exactly one choice that can be made which has the correct type.
  \caption{%
    \label{fig:exponential-laws:def:math}%
    The interesting components of $(\mathcal C₁ × \mathcal C₂ → \mathcal D) ≅ (\mathcal C₁ → (\mathcal C₂ → \mathcal D))$.%
  }
\end{figure}

\begin{figure}
\begin{minted}{coq}
(** [(C₁ × C₂ → D) ≅ (C₁ → (C₂ → D))] *)
(** We denote functors by pairs of maps on objects ([λₒ]) and
    morphisms ([λₘ]), and natural transformations as a single map
    ([λₜ]) *)
Time Program Definition curry_iso (C₁ C₂ D : Category)
  : (C₁ * C₂ -> D) ≅ (C₁ -> (C₂ -> D)) :>>> Cat
  := {| fwd
        := λₒ F, λₒ c₁, λₒ c₂, F ₀  (c₁, c₂)
                      ; λₘ m , F ₁  (identity c₁, m)
               ; λₘ m₁, λₜ c₂, F ₁  (m₁, identity c₂)
         ; λₘ T, λₜ c₁, λₜ c₂, T (c₁, c₂);
        bwd
        := λₒ F, λₒ '(c₁, c₂), (F ₀  c₁)₀ c₂
               ; λₘ '(m₁, m₂), (F ₁  m₁) _ ∘ (F ₀  _)₁ m₂
         ; λₘ T, λₜ '(c₁, c₂), (T c₁) c₂ |}.
(* Finished transaction in 1.958 secs (1.958u,0.s) (successful) *)
\end{minted}
  \caption{%
    \label{fig:exponential-laws:def:coq}%
    The interesting components of $(\mathcal C₁ × \mathcal C₂ → \mathcal D) ≅ (\mathcal C₁ → (\mathcal C₂ → \mathcal D))$, in Coq.
    The surrounding definitions and notations required for this example to typecheck are given in \autoref{sec:abstraction-barriers:mismatch:example:exponential-laws:full-code}.%
    \todo{Appendix .1 is a bad ref, how to format?}
  }
\end{figure}

While two seconds is long, there is an even more serious issue that arises when attempting to prove the algebraic laws.
The types here are already a bit long:
The goal that going from $(C₁ × C₂ → D)$ to $(C₁ → (C₂ → D))$ and back again is the identity is only about 24 lines after $\beta$ reduction (when \mintinline{coq}{Set Printing All} is on, there are about 3\,300 words).

However, if we pierce the abstraction barrier of functor composition, the goal blows up to about 254 lines (about 18\,000 words with \mintinline{coq}{Set Printing All})!
This blow-up is due to the fact that the opaque proofs that functor composition is functorial take the entirety of the functors being composed as arguments.
Hence unfolding the composition of two functors duplicates those functors many times over.
If we must compose more than two functors, we get even more blow-up.

Piercing this barrier also shows up in proof-checking time.
If we first decompose the goal into the separate equalities we wish to prove and only then unfold the abstraction barrier (thereby side-stepping the issue of passing large arguments to opaque proofs), it takes less than a tenth of a second to prove each of the two algebraic laws of the isomorphism.
However, if we instead unfold the definitions first and then decompose the goal into separate goals, it takes about 5$\times$ longer to check the proof.

Readers interested in the full compiling code for this example can refer to \autoref{sec:abstraction-barriers:mismatch:example:exponential-laws:full-code}.


\minortodo{should I write text and code with vector example?  we'd probably need to index lists over vectors of types to get really hairy things...}

\begin{comment}
\begin{minted}{coq}
Require Import Coq.Arith.Arith.
Require Import Coq.Program.Basics.
Require Import Coq.micromega.Lia.
Require Import Coq.Lists.List.
Require Import Coq.Logic.Eqdep_dec.
Import ListNotations.
Local Open Scope list_scope.
Declare Scope vector_scope.
Delimit Scope vector_scope with vector.
Local Open Scope vector_scope.
Local Set Primitive Projections.
Set Implicit Arguments.
Import EqNotations.

Module Vector.
  Record t A n := of_list { to_list : list A ; length_to_list : List.length to_list = n }.
  Bind Scope vector_scope with t.
  Definition nil {A} : t A 0 := of_list nil eq_refl.
  Definition cons {A n} (x : A) (xs : t A n) : t A (S n)
    := of_list (cons x (to_list xs)) (f_equal S (length_to_list xs)).
  Infix "::" := cons : vector_scope.
  Notation "[ ]" := nil : vector_scope.
  Notation "[ x ]" := (cons x nil) : vector_scope.
  Notation "[ x ; y ; .. ; z ]" :=  (cons x (cons y .. (cons z nil) ..)) : vector_scope.
  Lemma eq_iff {A n} {x y : t A n}
    : x = y <-> to_list x = to_list y.
  Proof.
    split; intro H; [ subst; reflexivity | destruct x, y; cbn in H; subst ].
    apply f_equal, UIP_dec; decide equality.
  Qed.
  Hint Rewrite length_to_list app_length (@eq_iff) : rewrite_db.
  Hint Unfold to_list : unfold_db.
  Ltac inner_tac := intros; repeat (autounfold with unfold_db; autorewrite with rewrite_db); try lia.
  Local Obligation Tactic := first [ abstract now inner_tac | inner_tac ].
  Program Definition app {A n m} (xs : t A n) (ys : t A m) : t A (n + m)
    := of_list (to_list xs ++ to_list ys) _.
  Infix "++" := app : vector_scope.
  Hint Unfold app : unfold_db.
  Lemma to_list_rew {A n m} {pf : n = m} {xs}
    : to_list (rew [t A] pf in xs) = to_list xs.
  Proof. subst; reflexivity. Qed.
  Lemma to_list_rew_rev {A n m} {pf : n = m} {xs}
    : to_list (rew <- [t A] pf in xs) = to_list xs.
  Proof. subst; reflexivity. Qed.
  Hint Rewrite (@to_list_rew) (@to_list_rew_rev) app_assoc : rewrite_db.
  Program Definition app_assoc {A x y z} (xs : t A x) (ys : t A y) (zs : t A z)
    : (rew [t A] Nat.add_assoc _ _ _ in (xs ++ (ys ++ zs)))
      = (xs ++ ys) ++ zs := _.
  Program Definition app_assoc' {A x y z} (xs : t A x) (ys : t A y) (zs : t A z)
    : xs ++ (ys ++ zs) = rew <- [t A] Nat.add_assoc _ _ _ in ((xs ++ ys) ++ zs)
    := _.
  Hint Rewrite rev_length : rewrite_db.
  Program Definition rev {A n} (xs : t A n) : t A n
    := of_list (rev (to_list xs)) _.
  Hint Unfold rev : unfold_db.
  Hint Rewrite rev_involutive : rewrite_db.
  Program Definition rev_involutive {A n} (xs : t A n)
    : rev (rev xs) = xs := _.
  Hint Rewrite rev_app_distr : rewrite_db.
  Program Definition rev_app_distr {A n m} (xs : t A n) (ys : t A m)
    : (rew [t A] Nat.add_comm _ _ in (rev (xs ++ ys))) = rev ys ++ rev xs
    := _.
  Program Definition rev_app_distr' {A n m} (xs : t A n) (ys : t A m)
    : rev (xs ++ ys) = rew <- [t A] Nat.add_comm _ _ in (rev ys ++ rev xs)
    := _.
End Vector.

Module HList.




Definition vector (n : nat) := {
\end{minted}
\end{comment}

\subsubsection{Type Size Blowup: Packed vs.\ Unpacked Records} \label{sec:abstraction-barriers:packed-records}
When designing APIs, especially of mathematical objects, one of the biggest choices is whether to pack the records, or whether to pass arguments in as fields.
That is, when defining a monoid, for example, there are five ways to go about specifying it:
\begin{enumerate}
    \item
    (packed)
    A \emph{monoid} consists of a type $A$, a binary operation $\cdot : A \to A \to A$, an identity element $e$, a proof that $e$ is a left- and right-identity $e \cdot a = a \cdot e = a$ for all $a$, and a proof of associativity that $(a \cdot b) \cdot c = a \cdot (b \cdot c)$.
    \item
    A \emph{monoid on a carrier type $A$} consists of a binary operation $\cdot : A \to A \to A$, an identity element $e$, a proof that $e$ is a left- and right-identity, and a proof of associativity.
    \item
    A \emph{monoid on a carrier type $A$ under the binary operation $\cdot : A \to A \to A$} consists of an identity element $e$, a proof that $e$ is a left- and right-identity, and a proof of associativity.
    \item
    (mostly unpacked)
    A \emph{monoid on a carrier type $A$ under the binary operation $\cdot : A \to A \to A$ with identity element $e$} consists of a proof that $e$ is a left- and right-identity and a proof of associativity.
    Note that MathClasses~\cite{MathClasses,Type2011Spitters} uses this strategy, as discussed in \textcite{Packaging2009Garillot}.
    \item
    (fully unpacked) A monoid on a carrier type $A$ under the binary operation $\cdot : A \to A \to A$ with identity element $e$ using a proof $p$ that $e$ is a left- and right-identity and a proof of $q$ of associativity consists of an element of the one-element unit type.
\end{enumerate}
\minortodo{maybe cite HoTT for past design work here, and find a good citation for packing vs unpacking}

If we go with anything but the fully packed design, then we incur exponential overhead as we go up abstraction layers, as follows.
A \emph{monoid homomorphism} from a monoid $A$ to a monoid $B$ consists of a function between the carrier types, and proofs that this function respects composition and identity.
If we use an unpacked definition of monoid with $n$ type parameters, then a similar definition of a monoid homomorphism involves at least $2n+2$ type parameters.
In higher category theory, it's common to talk about morphisms between morphisms, and every additional layer here doubles the number of type arguments, and this can quickly lead to very large terms, resulting is major performance bottlenecks.
Note that number of type parameters determines the constant factor out front of the exponential growth in the number of layers of mathematical constructions.

How much is this overhead concretely?
When developing a category theory library~\cite{category-coq-experience}, we sped up overall compilation time by approximately a factor of two, from around 16 minutes to around 8 minutes, by changing one of the two parameters to a field in the definition of a category.%
\footnote{%
  See \githubref[commit ][ of JasonGross/catdb on GitHub]{JasonGross/catdb}{209231ae3e94d5dbbc678c94930dcf585d53d555} for details.%
}

\minortodo{maybe figure out some examples and include perf data?}
\minortodo{does this need more exposition?}

\section{Conclusion of this Chapter}
\todo{How should this chapter be concluded?}
\todo{Maybe another look-forward at what comes next?}

\begin{subappendices}
\begin{comment}
\section{Comments from Andres}
in chapter 2 you start by comparing coq by non-dependently-typed languages. perhaps the more appropriate comparison would be between coq and *compilers* of other languages? Ah, you do this later.
"integers or colors or names" -- some hardcore optimization nerds may object. did you know that for all of these it is possible to sort in less than nlogn time using specialized techniques? :P (not a serious objection)
"3. Being able to run the proof assistant in your head" -- obligatory claim about this finding bugs that the developers didn't see? Also, please emphasize that this is beyond the *spec* of conversion, but about the actual *algorithm*. the GCC example kinda does that, map it back explicitly.
typechecking of applications definitely was quadratic, but may no longer be
"maximally" sharing terms is almost always heuristic, and I would call it heuristic sharing to highlight that (or something other that does not claim maximality)
hash consing itself is also slow
I would like to be explicitly credited for my contributions to this list. Somewhere in the thesis, sometime, at least when you get to claiming how many of these are you are taking credit for.
"There are three primary ways that nested abstraction barriers can lead to performance bottlenecks: one involving conversion missteps and the other involving exponential blow-up in the size of types."
"The reason users don’t set up hints like this is that usually it’s not necessary." -- and that is hard to see where the time is spent when it is necessary
Thu 7:22pm
and the non-locality of design choices is even across libraries and proof assistant implementations
Thu 7:22pm
see your abstraction layer counting stuff
Thu 7:22pm
more generally
Thu 7:38pm
if in 20 years I hear that your thesis has become popular and important, my guess would be that it was "the first paper in a field", out of "you either want to write the first paper in the field or the last paper in the field"
Thu 7:39pm
in particular, the biggest impact I could see from this is making people consider proof performance engineering a worthwhile direction of study
Thu 7:39pm
you have overwhelming evidence that proof projects need the results of this
Thu 7:40pm
probably it would help to describe how it is intellectually interesting, related to profound questions, and just cool to hack on
Thu 7:40pm
the more you can hit those points, the more likely I think people will care
Thu 7:41pm
(was my feedback useful? would you like to discuss anything about it?)

\end{comment}
\end{subappendices}

\todofrom{Beta Ziliani}{Hi Jason, let me give you some preliminary feedback on Chapter 2 (the only I got to read so far, sorry!). These are presentation details, with the only big problem being that I felt at times like I was being thrown stuff to the face without much heads-up. But probably it doesn't help that I'm reading it in bits, so don't mind much at this comment.
%
  About the content, I like it. I'm aware of most of these issues, but having them presented like this gives a nice perspective of the problem. One comment about packed/unpacked records: MathComp decided to pack the records precisely for this reason. I remember in a presentation of MathClasses (I can try to find where it was if you want), Georges bashed the student of Bas Spitters precisely for this reason, claiming MathClasses won't scale to real-world applications. I don't know what the status of MathClasses is now, and if Georges' prediction stands, but I thought you'll find this interesting/amusing.
%
  Details:
  In 2.1 I was missing some heads-up of the steps of the program you're measuring, in order to understand where the bottleneck was coming from. In particular this paragraph is confusing, as I thought at this time we were talking JUST about code generation:
%
  ``Maybe, you might ask, were we generating unreasonable amounts of code? Each
  example using n machine words generated 3n lines of code. Furthermore, the actual
  code generation took less than 0.002\% of the total time on the largest examples we
  tested (just 14 seconds out of about 211 hours). How can this be?''
%
  In general, I think that the graphics might be scaled significantly, they took an entire page now and it sounds a bit too much (detail).
%
  There are a few typos, I can give it to you but probably you'll spot them on a next read.}

  %If the reader walks away from this thesis with this level of understanding---that there was a super-linear performance bottleneck in the automation of generating verified low-level cryptographic code, and the project presented in \autoref{ch:rewriting} fixed this problem---that is accurate.

%We want readers to be able to understand where the performance issue came from, how it fits into a broader structure of performance issues in proof assistants, and how the solution fixes the performance problem.


%In this chapter, after arguing that such structure exists and is interesting, we will illuminate the structure.

%Up to this point, I expect I'll have told the reader why proof assistants are important, what they are, and about the main project I worked on of synthesizing crypto code.
%I'll have told them about how manual proof scales with poor constant factors, and automation is the solution.
%I'll have told them that the problem with automation, in my experience, is that it scales super-linearly.

%This chapter aims first to convince the reader that solving these super-linear scaling factors involves more than just ``find the slowness and do the obvious thing to fix it'', and second to demystify the additional structure that's present.

%The reason for this argument is that I want the reader to understand more deeply the solution and tool I'm presenting in \autoref{ch:rewriting}, as well as the other contributions of this thesis.
%Not just ``there was a performance issue, Jason fixed it, here's the solution.''
