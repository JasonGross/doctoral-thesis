\chapter{Design-based fixes} \label{ch:design} \label{ch:api-design}

\todo{cite \textcite{Deep2015Gu} in this chapter, very relevant}

\section{Introduction}
In \Autoref{ch:intro,ch:perf-failures}, we talked about two different fundamental sources of performance bottlenecks in proof assistants:
the power that comes from having dependent types, in \autoref{sec:why-how-dependent-types};
and the de Bruijn criterion of having a small trusted kernel, in \autoref{sec:debruijn-criterion}.
In this chapter, we will dive further into the performance issues arising from the first of these design decisions, expanding on \fullref{sec:axis-nested-abstraction-barriers}, and proposing some general guidelines for handling these performance bottlenecks.

This chapter is primarily geared at the users of proof assistants, and especially at proof-assistant library developers.

We saw in \nameref{sec:axis-nested-abstraction-barriers} three different ways that design choices for abstraction barriers can impact performance:
We saw in \nameref{sec:abstraction-barriers:mismatch} that API mismatch results in type-size blowup.
We saw in \nameref{sec:abstraction-barriers:conversion-troubles} that imperfectly opaque abstraction barriers result in slowdown due to needless calls to the conversion checker.
We saw in \nameref{sec:abstraction-barriers:packed-records} how the choice of whether to use packed or unpacked records impacts performance.

In this chapter, we will focus primarily on the first of these ways; while it might seem like a simple question of good design, it turns out that good API design in dependently-typed programming languages is significantly harder than in simply-typed programming languages.
Mitigating the second source of performance bottlenecks, imperfectly opaque abstraction barriers, on the other hand, is actually just a question of meticulous tracking of how abstraction barriers are defined and used, and designing them so that they all unfolding is explicit.
However, we will present an exception to the rule of opaque abstraction barriers in \autoref{sec:duality-conversion} in which deliberate breaking of all abstraction barriers in a careful way can result in performance gains of up to a factor of two:
\autoref{sec:duality-conversion} presents one of our favorite design patterns for categorical constructions: a way of coaxing Coq's definitional equality into implementing \emph{proof by duality}, one of the most widely known ideas in category theory.
\minortodo{adjust the tone of what comes after the colon in the previous sentence to fit with this paper}
Finally, the question of whether to use packed or unpacked records is actually a genuine trade-off in both design-space and performance, as far as I can tell;
the non-performance design considerations have been discussed before in \textcite{Packaging2009Garillot}, while the performance implications are relatively straightforward.
As far as I'm aware, there's not really a good way to get the best of all worlds.

Much of this chapter will draw on examples and experience from a category theory library we implemented in Coq~\cite{category-coq-experience}, which we introduce in \autoref{sec:category-theory-library}.

\section{When And How To Use Dependent Types Painlessly}\label{sec:when-how-dependent-types}
The extremes are relatively easy:
\begin{itemize}
\item
  Total separation between proofs and programs, so that programs are simply typed, works relatively well
\item
  Pre-existing mathematics, where objects are fully bundled with proofs and never need to be separated from them, also works relatively well
\item
  The rule of thumb in the middle:
  it is painful to recombine proofs and programs after you separate them;
  if you are doing it to define an opaque transformation that acts on proof-carrying code, that is okay, but if you cannot make that abstraction barrier, enormous pain results.
\item
  For example, if you have length-indexed lists and want to index into them with elements of a finite type, things are fine until you need to divorce the index from its proof of finiteness.
  If you, for example, want to index into, say, the concatenation of two lists, with an index into the first of the lists, then you will likely run into trouble, because you are trying to consider the index separately from its proof of finitude, but you have to recombine them to do the indexing.
\end{itemize}
\minortodo{flesh out this section}
\minortodo{More examples from the rewriter?}
\minortodo{forward-cite the rewriter?}

\section{A Brief Introduction To Our Category Theory Library}\label{sec:category-theory-library}
\subsection{Introduction}
Category theory~\cite{mac1998categories} is a popular all-encompassing mathematical formalism that casts familiar mathematical ideas from many domains in terms of a few unifying concepts.
A \emph{category} can be described as a directed graph plus algebraic laws stating equivalences between paths through the graph.
Because of this spartan philosophical grounding, category theory is sometimes referred to in good humor as ``formal abstract nonsense.''
Certainly the popular perception of category theory is quite far from pragmatic issues of implementation.
Our implementation of category theory has run squarely into issues of design and efficient implementation of type theories, proof assistants, and developments within them.

One might presume that it is a routine exercise to transliterate categorical concepts from the whiteboard to Coq.
Most category theorists would probably be surprised to learn that standard constructions ``run too slowly,'' but in our experience that is exactly the result of experimenting with na√Øve first Coq implementations of categorical constructs.
It is important to tune the library design to minimize the cost of manipulating terms and proving interesting theorems.

Category theory, said to be ``notoriously hard to formalize''~\cite{harrison1996formalized}, provides a good stress test of any proof assistant, highlighting problems in usability and efficiency.

Formalizing the connection between universal morphisms and adjunctions provides a typical example of our experience with performance.
A \emph{universal morphism} is a construct in category theory generalizing extrema from calculus.
An \emph{adjunction} is a weakened notion of equivalence.
In the process of rewriting our library to be compatible with homotopy type theory, we discovered that cleaning up this construction conceptually resulted in a significant slow-down, because our first attempted rewrite resulted in a leaky abstraction barrier and, most importantly, large goals (\autoref{sec:term-size}).
Plugging the holes there reduced goal sizes by two orders of magnitude\footnote{The word count of the larger of the two relevant goals went from 7,312 to 191.}, which led to a factor of ten speedup in that file (from 39s to 3s), but incurred a factor of three slow-down in the file where we defined the abstraction barriers (from 7s to 21s).\footnote{%
  See \githubref[commit ][ in HoTT/HoTT on GitHub]{HoTT/HoTT}{eb0099005171e642d467047933660980ddc66280} for more details.%
}
Working around slow projections of $\Sigma$ types (\autoref{sec:nested-sigma-types}) and being more careful about code reuse each gave us back half of that lost time.\footnote{%
  See commits \githubref[][]{HoTT/HoTT}{c1e7ae32e86d51563da7a73b726c5cdd3246794a}, \githubref[][]{HoTT/HoTT}{93a1258becb49a76315663fa7626058a4e1a1959}, \githubref[][]{HoTT/HoTT}{bab2b34719108dbc5da66ac40c6fe01e64f4071b}, and \githubref[][]{HoTT/HoTT}{3b0932f21edc050b17f5e7843357dbcfc57533ce} in HoTT/HoTT on GitHub for more details.%
}

Although pre-existing formalizations of category theory in proof assistants abound~\cite{Ahrens2013,megacz-coq-categories,o2004towards,copumpkin/categories,ConCaT%
%fullbib:,CatsInZFC%
,spitters2010developing%
%fullbib:,kozen2006automating%
,math-overflow-formalizations%
%fullbib:,benediktahrens/coinductives,benediktahrens-Foundations-typesystems,Carvalho1998,jmchapman/restriction-categories,konn/category-agda,crypto-agda/crypto-agda,mattam82-cat,Coalgebras,Algebra,ahrens2010categorical,weber02program,pcapriotti/agda-categories,huet2000constructive,altucher1990mechanically,Category2-AFP,MathClasses,benediktahrens/rezk-completion,mohri1995formalization,spiwackverified%
%,rs-/triangles%
%fullbib:,caccamo2001higher,aczel1993galois,wilander2005bicategory,logical2001implicit,dyckhoff1985category,wilander2012constructing,harrison1996formalized,agerholm1995experiments,nuo2013second%
}, %\footnote{This list very incomplete; see also \url{http://mathoverflow.net/questions/152497}% is more comprehensive, but we decided to cap our bibliography at one page, given the page limit.
%}
we chose to implement our library~\cite{HoTT/HoTT-categories} from scratch.
Beginning from scratch allowed me to familiarize myself with both category theory and Coq, without simultaneously having to familiarize myself with a large pre-existing code base.
%Additionally, starting from scratch forced us to confront all of the decisions involved in designing such a library, and gave us the confidence to change the definitions of basic concepts multiple times to try out various designs, including fully rewriting the library at least three times.
%Although this paper is much more about the design of category theory libraries in general than our library in particular, we include a comparison of our library~\cite{HoTT/HoTT-categories} with selected extant category theory libraries in \autoref{sec:compare-libraries}.
%At present, our library subsumes many of the constructions in most other such Coq libraries, and is not lacking any constructions in other libraries that are of a complexity requiring significant type checking time, other than monoidal categories.


\medskip

We begin our discussion in \autoref{sec:categories} considering a mundane aspect of type definitions that has large consequences for usability and performance.
With the expressive power of Coq's logic Gallina, we often face a choice of making \emph{parameters} of a type family explicit arguments to it, which looks like universal quantification; or of including them within values of the type, which looks like existential quantification.
As a general principle, we found that the universal or \emph{outside} style improves the user experience modulo performance, while the existential or \emph{inside} style speeds up type checking.
The rule that we settled on was: \emph{inside} definitions for pieces that are usually treated as black boxes by further constructions, and \emph{outside} definitions for pieces whose internal structure is more important later on.

\autoref{sec:duality-conversion} presents one of our favorite design patterns for categorical constructions: a way of coaxing Coq's definitional equality into implementing \emph{proof by duality}, one of the most widely known ideas in category theory.
In \autoref{sec:other}, we describe a few other design choices that had large impacts on usability and performance, often of a few orders of magnitude.
\minortodo{this is duplicative with text above}


\section{Internalizing Duality Arguments in Type Theory} \label{sec:duality-conversion}

  In general, we tried to design our library so that trivial proofs on paper remain trivial when formalized.
  One of Coq's main tools to make proofs trivial is the definitional equality, where some facts follow by computational reduction of terms.
  We came up with some small tweaks to core definitions that allow a common family of proofs by \emph{duality} to follow by computation.

  Proof by duality is a common idea in higher mathematics: sometimes, it is productive to flip the directions of all the arrows.
  For example, if some fact about least upper bounds is provable, chances are that the same kind of fact about greatest lower bounds will also be provable in roughly the same way, by replacing ``greater than''s with ``less than''s and vice versa.

  Concretely, there is a dualizing operation on categories that inverts the directions of the morphisms:
\begin{minted}{coq}
Notation "C ·µí·µñ" := ({| Ob := Ob C; Hom x y := Hom C y x; ... |}).
\end{minted}

  Dualization can be used, roughly, for example, to turn a proof that Cartesian product is an associative operation into a proof that disjoint union is an associative operation; products are dual to disjoint unions.

  One of the simplest examples of duality in category theory is initial and terminal objects.
  In a category \cat C, an initial object $0$ is one that has a unique morphism $0 \to x$ to every object $x$ in \cat C; a terminal object $1$ is one that has a unique morphism $x \to 1$ from every object $x$ in \cat C.
  Initial objects in \cat C are terminal objects in $\cat{C}^\text{op}$.
  The initial object of any category is unique up to isomorphism; for any two initial objects $0$ and $0'$, there is an isomorphism $0 \cong 0'$.
  By flipping all of the arrows around, we can prove, by duality, that the terminal object is unique up to isomorphism.  More precisely, from a proof that an initial object of $\cat{C}^{\text{op}}$ is unique up to isomorphism, we get that any two terminal objects $1'$ and $1$ in $\cat{C}$, which are initial in $\cat{C}^{\text{op}}$, are isomorphic in $\cat{C}^{\text{op}}$.  Since an isomorphism $x \cong y$ in $\cat{C}^\text{op}$ is an isomorphism $y \cong x$ in \cat C, we get that $1$ and $1'$ are isomorphic in $\cat C$.

  It is generally straightforward to see that there is an isomorphism between a theorem and its dual, and the technique of dualization is well-known to category theorists, among others.  We discovered that, by being careful about how we defined things, we could make theorems be judgmentally equal to their duals!  That is, when we prove a theorem
  \begin{align*}
  \mintinline{coq}{initial_ob_unique : ‚àÄ C  } & \mintinline{coq}{(x y : Ob C),} \\
  & \mintinline{coq}{is_initial_ob x ‚Üí is_initial_ob y ‚Üí x ‚âÖ y},
  \end{align*}
  we can define another theorem
  \begin{align*}
  \mintinline{coq}{terminal_ob_unique : ‚àÄ C  }& \mintinline{coq}{(x y : Ob C),} \\
  & \mintinline{coq}{is_terminal_ob x ‚Üí is_terminal_ob y ‚Üí x ‚âÖ y}
  \end{align*}
  as
  \begin{center}
  \mintinline{coq}{terminal_ob_unique C x y H H' := initial_ob_unique C·µí·µñ y x H' H}.
  \end{center}
  % In homotopy type theory, we can convey our discovery by saying that not only are theorems homotopic to their duals, but they can be made to be judgmentally equal to their duals.
  Interestingly, we found that in proofs with sufficiently complicated types, it can take a few seconds or more for Coq to accept such a definition; we are not sure whether this is due to peculiarities of the reduction strategy of our version of Coq, or speed dependency on the size of the normal form of the type (rather than on the size of the unnormalized type), or something else entirely.

  In contrast to the simplicity of witnessing the isomorphism, it takes a significant amount of care in defining concepts, often to get around deficiencies of Coq, to achieve \emph{judgmental} duality.
  Even now, we were unable to achieve this ideal for some theorems.
  For example, category theorists typically identify the functor category $\cat{C}^\text{op} \to \cat{D}^\text{op}$ (whose objects are functors $\cat{C}^\text{op} \to \cat{D}^\text{op}$ and whose morphisms are natural transformations) with $(\cat{C} \to \cat{D})^\text{op}$ (whose objects are functors $\cat{C} \to \cat{D}$ and whose morphisms are flipped natural transformations).
  These categories are canonically isomorphic (by the dualizing natural transformations), and, with the univalence axiom~\cite{HoTTBook}, they are equal as categories!
  However, to make these categories definitionally equal, we need to define functors as a structural record type (see \autoref{sec:nominal-vs-structural}) rather than a nominal one.

  \subsection{Duality Design Patterns}
    One of the simplest theorems about duality is that it is involutive; we have that $(\cat{C}^{\text{op}})^{\text{op}} = \cat{C}$.
    In order to internalize proof by duality via judgmental equality, we sometimes need this equality to be judgmental.
    Although it is impossible in general in Coq 8.4 (see \hyperref[sec:no-judgmental-eta]{dodging judgmental \texorpdfstring{$\eta$}{Œ∑} on records} below), the latest version of Coq available when we were creating this library, we want at least to have it be true for any explicit category (that is, any category specified by giving its objects, morphisms, etc., rather than referred to via a local variable).

    \subsubsection{Removing Symmetry} \label{sec:remove-symmetry}
      Taking the dual of a category, one constructs a proof that $f \circ (g \circ h) = (f \circ g) \circ h$ from a proof that $(f \circ g) \circ h = f \circ (g \circ h)$.
      The standard approach is to apply symmetry.
      However, because applying symmetry twice results in a judgmentally different proof, we decided instead to extend the definition of \texttt{Category} to require both a proof of $f \circ (g \circ h) = (f \circ g) \circ h$ and a proof of $(f \circ g) \circ h = f \circ (g \circ h)$.
      Then our dualizing operation simply swaps the proofs.
      We added a convenience constructor for categories that asks only for one of the proofs, and applies symmetry to get the other one.
      Because we formalized 0-truncated category theory, where the type of morphisms is required to have unique identity proofs, asking for this other proof does not result in any coherence issues.

    \subsubsection{Dualizing the Terminal Category}
      To make everything work out nicely, we needed the terminal category, which is the category with one object and only the identity morphism, to be the dual of itself.
      We originally had the terminal category as a special case of the discrete category on $n$ objects.
      Given a type $T$ with uniqueness of identity proofs, the discrete category on $T$ has as objects inhabitants of $T$, and has as morphisms from $x$ to $y$ proofs that $x = y$.
      These categories are not judgmentally equal to their duals, because the type $x = y$ is not judgmentally the same as the type $y = x$.
      As a result, we instead used the indiscrete category, which has \texttt{unit} as its type of morphisms.

    \subsubsection{Which Side Does the Identity Go On?}
      The last tricky obstacle we encountered was that when defining a functor out of the terminal category, it is necessary to pick whether to use the right identity law or the left identity law to prove that the functor preserves composition; both will prove that the identity composed with itself is the identity.
      The problem is that dualizing the functor leads to a road block where either concrete choice turns out to be ``wrong,'' because the dual of the functor out of the terminal category will not be judgmentally equal to another instance of itself.
      To fix this problem, we further extended the definition of category to require a proof that the identity composed with itself is the identity.

    \subsubsection{Dodging Judgmental \texorpdfstring{$\eta$}{Œ∑} on Records}  \label{sec:no-judgmental-eta}
      The last problem we ran into was the fact that sometimes, we really, really wanted judgmental $\eta$ on records.
      The $\eta$ rule for records says any application of the record constructor to all the projections of an object yields exactly that object; e.g. for pairs, $x \equiv (x_1, x_2)$ (where $x_1$ and $x_2$ are the first and second projections, respectively).
      For categories, the $\eta$ rule says that given a category \cat C, for a ``new'' category defined by saying that its objects are the objects of \cat C, its morphisms are the morphisms of \cat C, \ldots, the ``new'' category is judgmentally equal to \cat C.

      In particular, we wanted to show that any functor out of the terminal category is the opposite of some other functor; namely, any $F : 1 \to \cat C$ should be equal to $(F^{\text{op}})^{\text{op}} : 1 \to (\cat C^{\text{op}})^{\text{op}}$.
      However, without the judgmental $\eta$ rule for records, a local variable $\cat C$ cannot be judgmentally equal to $(\cat C^{\text{op}})^{\text{op}}$, which reduces to an application of the constructor for a category, unless the $\eta$ rule is built into the proof assistant.
      To get around the problem, we made two variants of dual functors: given $F : \cat C \to \cat D$, we have $F^{\text{op}} : \cat C^{\text{op}} \to \cat D^{\text{op}}$, and given $F : C^{\text{op}} \to \cat D^{\text{op}}$, we have $F^{\text{op}'} : \cat C \to \cat D$.
      There are two other flavors of dual functors, corresponding to the other two pairings of ${}^{\text{op}}$ with domain and codomain, but we have been glad to avoid defining them so far.  As it was, we ended up having four variants of dual natural transformation, and are very glad that we did not need sixteen.
      When Coq 8.5 was released, we no longer needed to pull this trick, as we could simply enable the $\eta$ rule for records judgmentally.


  \subsection{Moving Forward: Computation Rules for Pattern Matching} \label{sec:compute-match}
    While we were able to work around most of the issues that we had in internalizing proof by duality, things would have been far nicer if we had more $\eta$ rules.
    The $\eta$ rule for records is explained above.
    The $\eta$ rule for equality says that the identity function is judgmentally equal to the function $f : \forall x\, y, x = y \to x = y$ defined by pattern matching on the first proof of equality; this rule is necessary to have any hope that applying symmetry twice is judgmentally the identity transformation.
%    Matthieu Sozeau is currently working on giving Coq judgmental $\eta$ for records with one or more fields, though not for equality%fullbib:~\cite{mattam82/coq-polyproj}%
%    .

    \autoref{sec:associators} will give more examples of the pain of manipulating pattern matching on equality.
    Homotopy type theory provides a framework that systematizes reasoning about proofs of equality, turning a seemingly impossible task into a manageable one.
    However, there is still a significant burden associated with reasoning about equalities, because so few of the rules are judgmental.

    We are currently attempting to divine the appropriate computation rules for pattern matching constructs, in the hopes of making reasoning with proofs of equality more pleasant.\footnote{See \coqbug[Coq Issue ]{3179} and \coqbug[Coq Issue ]{3119}.}

\section{A Sampling of Abstraction Barriers}
\minortodo{maybe this section should come first?}

We acknowledge that the concept of performance issues arising from choices of abstraction barriers may seem a bit counter-intuitive.
After all, abstraction barriers generally live in the mind of the developer, in some sense, and it seems a bit insane to say that performance of the code depends on the mental state of the programmer.

Therefore, we will describe a sampling of abstraction barriers and the design choices that went into them, drawn from real examples\minortodoask{how much should I talk about the category theory library itself in my thesis?}, as well as the performance issues that arose from these choices.

A few other pervasive strategies made non-trivial differences for proof performance or simplicity.

  \subsection{Identities vs.~Equalities; Associators} \label{sec:associators}
    There are a number of constructions that are provably equal, but which we found more convenient to construct transformations between instead, despite the increased verbosity of such definitions.
    This is especially true of constructions that strayed towards higher category theory.
    For example, when constructing the Grothendieck construction of a functor to the category of categories, we found it easier to first generalize the construction from functors to pseudofunctors.
    The definition of a pseudofunctor results from replacing various equalities in the definition of a functor with isomorphisms (analogous to bijections between sets or types), together with proofs that the isomorphisms obey various coherence properties.
    This replacement helped because there are fewer operations on isomorphisms (namely, just composition and inverting), and more operations on proofs of equality (pattern matching, or anything definable via induction); when we were forced to perform all of the operations in the same way, syntactically, it was easier to pick out the operations and reason about them.

    Another example was defining the (co)unit of adjunction composition, where instead of a proof that $F \circ (G \circ H) = (F \circ G) \circ H$, we used a natural transformation, a coherent mapping between the actions of functors.
    Where equality-based constructions led to computational reduction getting stuck at casts, the constructions with natural transformations reduce in all of the expected contexts.

  \subsection{Opacity; Linear Dependence of Speed on Term Size}\label{sec:equality-reflection}\label{sec:term-size}

    Coq is slow at dealing with large terms.
    For goals around 175,000 words long\footnote{When we had objects as arguments rather than fields (see \autoref{sec:arguments-vs-fields}), we encountered goals of about 219,633 words when constructing pointwise Kan extensions.}, we have found that simple tactics like \texttt{apply f\_equal} take around 1 second to execute, which makes interactive theorem proving very frustrating.\footnote{See also \url{https://coq.inria.fr/bugs/show\_bug.cgi?id=3280}.}
    Even more frustrating is the fact that the largest contribution to this size is often arguments to irrelevant functions, i.e., functions that are provably equal to all other functions of the same type.
    (These are proofs related to algebraic laws like associativity, carried inside many constructions.)

    Opacification helps by preventing the type checker from unfolding some definitions, but it is not enough: the type checker still has to deal with all of the large arguments to the opaque function.
    Hash-consing might fix the problem completely.

    Alternatively, it would be nice if, given a proof that all of the inhabitants of a type were equal, we could forget about terms of that type, so that their sizes would not impose any penalties on term manipulation.  %  One could imagine a version of Coq's logic that knows to treat all proofs of an equality as equivalent to each other.  Alternatively, there might be some way to ignore these terms when doing most computation, without changing the underlying theory.
    One solution might be irrelevant fields, like those of Agda, or implemented via the Implicit CiC~\cite{barras2008implicit,logical2001implicit}.

  \subsection{Abstraction Barriers} \label{sec:abstraction-barriers}

    In many projects, choosing the right abstraction barriers is essential to reducing mistakes, improving maintainability and readability of code, and cutting down on time wasted by programmers trying to hold too many things in their heads at once.
    This project was no exception; we developed an allergic reaction to constructions with more than four or so arguments, after making one too many mistakes in defining limits and colimits.
    Limits are a generalization, to arbitrary categories, of subsets of Cartesian products.
    Colimits are a generalization, to arbitrary categories, of disjoint unions modulo equivalence relations.

    Our original flattened definition of limits involved a single definition with 14 nested binders for types and algebraic properties.
    After a particularly frustrating experience hunting down a mistake in one of these components, we decided to factor the definition into a larger number of simpler definitions, including familiar categorical constructs like terminal objects and comma categories.
    This refactoring paid off even further when some months later we discovered the universal morphism definition of adjoint functors%fullbib:~\cite{wiki:adjoint-functors:universal-morphisms%
    %fullbib:,ncatlab:adjoint+functor:UniversalArrows%
    %fullbib:}%
    .
    With a little more abstraction, we were able to reuse the same decomposition to prove the equivalence between universal morphisms and adjoint functors, with minimal effort.

    Perhaps less typical of programming experience, we found that picking the right abstraction barriers could drastically reduce compile time by keeping details out of sight in large goal formulas.
    In the instance discussed in the introduction, we got a factor of ten speed-up by plugging holes in a leaky abstraction barrier!\footnote{See \githubref[commit ][ in HoTT/HoTT on GitHub]{HoTT/HoTT}{eb0099005171e642d467047933660980ddc66280} for the exact change.}

\subsection{Nested Œ£ Types} \label{sec:nested-sigma-types}
  In Coq, there are two ways to represent a data structure with one constructor and many fields:
  as a single inductive type with one constructor (records), or as a nested $\Sigma$ type.
  For instance, consider a record type with two type fields $A$ and $B$ and a function $f$ from $A$ to $B$.
  A logically equivalent encoding would be $\Sigma A. \; \Sigma B. \; A \to B$.
  There are two important differences between these encodings in Coq.

  \label{sec:prim-record-proj}
  The first is that while a theorem statement may abstract over all possible $\Sigma$ types, it may not abstract over all record types, which somehow have a less first-class status.
  Such a limitation is inconvenient and leads to code duplication.

  The far more pressing problem, overriding the previous point, is that nested $\Sigma$ types have horrendous performance, and are sometimes a few orders of magnitude slower.
  The culprit is projections from nested $\Sigma$ types, which, when unfolded (as they must be, to do computation), each take almost the entirety of the nested $\Sigma$ type as an argument, and so grow in size very quickly.

  Let's consider a toy example to see the asymptotic performance.
  To construct a nested $\Sigma$ type with three fields of type \mintinline{coq}{unit}, we can write the type:
  \begin{center}
      \mintinline{coq}{{ _ : unit & { _ : unit & unit }}}
  \end{center}
  If we want to project out the final field, we must write \mintinline{coq}{projT2 (projT2 x)} which, when implicit arguments are included, expands to \minortodo{figure out why spacing is strange above this code example}
  \begin{center}
      \mintinline{coq}{@projT2 unit (Œª _ : unit, unit) (@projT2 unit (Œª _ : unit, { _ : unit & unit }) x)}
  \end{center}
  This term grows quadratically in the number of projections because the type of the $n^\text{th}$ field is repeated approximately $2n$ times.
  This is even more of a problem when we need to \mintinline{coq}{destruct x} to prove something about the projections, as we need to \mintinline{coq}{destruct} it as many times as their are fields, which adds another factor of $n$ to the performance cost of building the proof from scratch; in Coq, this cost is either avoided due to sharing or else is hidden by a quadratic factor which a much larger constant factor.
  Note that this is a sort-of dual to the problem of \autoref{sec:quadratic-conj-certificate}; there, we encountered quadratic overhead in applying the constructors (which is also a problem here), whereas right now we are discussing quadratic overhead in applying the eliminators.
  See \autoref{fig:timing-make-destruct-nested-sig} for the performance details.

  We can avoid much of the cost of building the projection term by using \emph{primitive projections} (see \autoref{sec:primitive-projections} for more explanation of this feature).
  Note that this feature is a sort-of dual to the proposed feature of dropping constructor parameters described in \autoref{sec:dropping-constructor-parameters}.
  This does drastically reduce the overhead of building the projection term, but only cuts in half the constant factor in destructing the variable so as to prove something about the projection.
  See \autoref{fig:timing-make-destruct-nested-prim-sig} for performance details.

  There are two solutions to this issue:
  \begin{enumerate}
      \item use built-in \emph{record} types
      \item carefully define intermediate abstraction barriers to avoid the quadratic overhead
  \end{enumerate}

  Both of these essentially solve the issue of quadratic overhead in projecting out the fields.
  This is the benefit of good abstraction barriers.

  In Coq 8.11, \mintinline{coq}{destruct} is unfortunately still quadratic due to issues with name generation, but the constant factor is much smaller; see \autoref{fig:timing-make-destruct-record} and \coqbug{12271}.

  We now come to the question: how much do we pay for using this abstraction barrier?
  That is, how much is the one-time cost of defining the abstraction barrier.
  Obviously, we can just make definitions for each of the projections and for the eliminator, and pay the cubic (or perhaps even quartic; see the leading term in \autoref{fig:timing-make-destruct-nested-sig}) overhead once.
  There's an interesting question, though, of if we can avoid this overhead all-together.

  As seen in \autoref{fig:timing-make-destruct-record}, using records partially avoids the overhead.
  Defining the record type, though, still incurs a quadratic factor due to hash consing the projections; see \coqbug{12270}.

  \minortodo{remove use of ``you''}
  If your proof assistant does not support records out-of-the-box, or you want to avoid using them for whatever reason\footnote{Note that the UniMath library~\cite{voevodsky_2015,UniMath,introduction2018Grayson} does this. \minortodo{explain reasoning}}, you can instead define intermediate abstraction barriers by hand.
  Here is what code that almost works looks like for four fields:
\begin{minted}{coq}
Local Set Implicit Arguments.
Record sigT {A} (P : A -> Type) := existT { projT1 : A ; projT2 : P projT1 }.
Definition sigT_eta {A P} (x : @sigT A P) : x = existT P (projT1 x) (projT2 x).
Proof. destruct x; reflexivity. Defined.
Definition _T0 := unit.
Definition _T1 := @sigT unit (fun _ : unit => _T0).
Definition _T2 := @sigT unit (fun _ : unit => _T1).
Definition _T3 := @sigT unit (fun _ : unit => _T2).
Definition T := _T3.
Definition Build_T0 (x0 : unit) : _T0 := x0.
Definition Build_T1 (x0 : unit) (rest : _T0) : _T1
  := @existT unit (fun _ : unit => _T0) x0 rest.
Definition Build_T2 (x0 : unit) (rest : _T1) : _T2
  := @existT unit (fun _ : unit => _T1) x0 rest.
Definition Build_T3 (x0 : unit) (rest : _T2) : _T3
  := @existT unit (fun _ : unit => _T2) x0 rest.
Definition Build_T (x0 : unit) (x1 : unit) (x2 : unit) (x3 : unit) : T
  := Build_T3 x0 (Build_T2 x1 (Build_T1 x2 (Build_T0 x3))).

Definition _T0_proj (x : _T0) : unit := x.
Definition _T1_proj1 (x : _T1) : unit := projT1 x.
Definition _T1_proj2 (x : _T1) : _T0 := projT2 x.
Definition _T2_proj1 (x : _T2) : unit := projT1 x.
Definition _T2_proj2 (x : _T2) : _T1 := projT2 x.
Definition _T3_proj1 (x : _T3) : unit := projT1 x.
Definition _T3_proj2 (x : _T3) : _T2 := projT2 x.

Definition proj_T_1 (x : T) : unit := _T3_proj1 x.
Definition proj_T_1_rest (x : T) : _T2 := _T3_proj2 x.
Definition proj_T_2 (x : T) : unit := _T2_proj1 (proj_T_1_rest x).
Definition proj_T_2_rest (x : T) : _T1 := _T2_proj2 (proj_T_1_rest x).
Definition proj_T_3 (x : T) : unit := _T1_proj1 (proj_T_2_rest x).
Definition proj_T_3_rest (x : T) : _T0 := _T1_proj2 (proj_T_2_rest x).
Definition proj_T_4 (x : T) : unit := _T0_proj (proj_T_3_rest x).

Definition _T0_eta (x : _T0) : x = Build_T0 (_T0_proj x) := @eq_refl _T0 x.
Definition _T1_eta (x : _T1) : x = Build_T1 (_T1_proj1 x) (_T1_proj2 x)
  := @sigT_eta unit (fun _ : unit => _T0) x.
Definition _T2_eta (x : _T2) : x = Build_T2 (_T2_proj1 x) (_T2_proj2 x)
  := @sigT_eta unit (fun _ : unit => _T1) x.
Definition _T3_eta (x : _T3) : x = Build_T3 (_T3_proj1 x) (_T3_proj2 x)
  := @sigT_eta unit (fun _ : unit => _T2) x.

Definition T_eta (x : T)
 : x = Build_T (proj_T_1 x) (proj_T_2 x) (proj_T_3 x) (proj_T_4 x)
  := let lhs3 := x in
     let lhs2 := _T3_proj2 lhs3 in
     let lhs1 := _T2_proj2 lhs2 in
     let lhs0 := _T1_proj2 lhs1 in
     let final := _T0_proj lhs0 in
     let rhs0 := Build_T0 final in
     let rhs1 := Build_T1 (_T1_proj1 lhs1) rhs0 in
     let rhs2 := Build_T2 (_T2_proj1 lhs2) rhs1 in
     let rhs3 := Build_T3 (_T3_proj1 lhs3) rhs2 in
     (((@eq_trans _T3)
         lhs3 (Build_T3 (_T3_proj1 lhs3) lhs2) rhs3
         (_T3_eta lhs3)
         ((@f_equal _T2 _T3 (Build_T3 (_T3_proj1 lhs3)))
            lhs2 rhs2
            ((@eq_trans _T2)
               lhs2 (Build_T2 (_T2_proj1 lhs2) lhs1) rhs2
               (_T2_eta lhs2)
               ((@f_equal _T1 _T2 (Build_T2 (_T2_proj1 lhs2)))
                  lhs1 rhs1
                  ((@eq_trans _T1)
                     lhs1 (Build_T1 (_T1_proj1 lhs1) lhs0) rhs1
                     (_T1_eta lhs1)
                     ((@f_equal _T0 _T1 (Build_T1 (_T1_proj1 lhs1)))
                        lhs0 rhs0
                        (_T0_eta lhs0)))))))
      : x = Build_T (proj_T_1 x) (proj_T_2 x) (proj_T_3 x) (proj_T_4 x)).

Import EqNotations.
Definition T_rect (P : T -> Type)
           (f : forall (x0 : unit) (x1 : unit) (x2 : unit) (x3 : unit),
               P (Build_T x0 x1 x2 x3))
           (x : T)
  : P x
  := rew <- [P] T_eta x in
     f (proj_T_1 x) (proj_T_2 x) (proj_T_3 x) (proj_T_4 x).
\end{minted}
  It only almost works because, although the overall size of the terms, even accounting for implicits, is linear in the number of fields, we still incur a quadratic number of unfoldings in the final cast node in the proof of \mintinline{coq}{T_eta}.
  Note that this cast node is only present to make explicit the conversion problem that must happen; removing it does not break anything, but then the quadratic cost is hidden in non-trivial substitutions of the \mintinline{coq}{let}-binders into the types.
  It might be possible to avoid this quadratic factor by being even more careful, but I was unable to find a way to do it.%
  \footnote{%
      Note that even reflective automation (see \autoref{ch:reflection}) is not sufficient to solve this issue.
      Essentially, the bottleneck is that at the bottom of the chain of \mintinline{coq}{let}-binders in the $\eta$ proof, we have two different types for the $\eta$-principle.
      One of them uses the globally-defined projections out of \mintinline{coq}{T}, while the other uses the projections of \mintinline{coq}{x} defined in the local context.
      We need to convert between these two types in linear time.
      Converting between two differently defined projections takes time linear in the number of under-the-hood projections, i.e., linear in the number of fields.
      Doing this once for each projection thus takes quadratic time.
      Using a reflective representation of nested $\Sigma$ types, and thus being able to prove the $\eta$ principle once and for all in constant time, would not help here, because it takes quadratic time to convert between the type of the $\eta$ principle in reflective-land and the type that we want.
      One thing that might help would be to have a version of conversion checking that was both memoized and could perform in-place reduction; see \coqbug{12269}.%
  }
  Worse, though, due to the issue with nested \mintinline{coq}{let}-binders described in \autoref{sec:sharing}, we would still incur a quadratic typechecking cost.

  We can, however, avoid this cost by turning on primitive projections via \texttt{Set Primitive Projections} at the top of this block of code:
  this enables judgmental $\eta$-conversion for primitive records, whence we can prove \mintinline{coq}{T_eta} with the proof term \mintinline{coq}{@eq_refl T x}.
  See \Autoref{fig:timing-make-destruct-record-definition-abstraction-performance-experiments/make-nested-prim-prod-abstraction.txt,fig:timing-make-destruct-record-definition-abstraction-performance-experiments/make-nested-prim-sig-abstraction.txt,fig:timing-make-destruct-record-definition-abstraction-performance-experiments/make-nested-prod-abstraction.txt,fig:timing-make-destruct-record-definition-abstraction-performance-experiments/make-nested-sig-abstraction.txt} for performance details.
  \minortodo{Select one of these graphs and use it and give it the label {fig:timing-make-destruct-record-definition-abstraction}}

  \minortodo{move figures up to be closer to references}

  \begin{figure*}
      \beginTikzpictureStamped{
          \einput{performance-experiments/make-nested-sig2.txt}
          \einput{performance-experiments/destruct-nested-sig2.txt}
          \einput{performance-experiments/make-nested-sig2-ltac2.txt}
      }
      \begin{axis}[xlabel=$n$,
          ylabel=time (s),
          legend pos=north west,
          width=0.95\textwidth,
          axis lines=left,
          xmin=0,
          xmax=1000,
          ymin=0,
          ymax=10,
          scaled x ticks=false,
          scaled y ticks=false]
          \addplot[only marks,mark=square,color=black] table[col sep=comma,x=param-n,y=repeat-destruct-user]{performance-experiments/destruct-nested-sig2.txt};
          \addlegendentry{\mintinline{coq}{repeat destruct}}
          \addplotquadraticregression[no markers, black][x=param-n,y=repeat-destruct-user][col sep=comma]{performance-experiments/destruct-nested-sig2.txt};
          \edef\destructa{\pgfplotstableregressiona}\edef\destructb{\pgfplotstableregressionb}\edef\destructc{\pgfplotstableregressionc}
          \addlegendentry{$\pgfmathprintnumber{\destructa}n^2\pgfmathprintnumber[print sign]{\destructb}n\pgfmathprintnumber[print sign]{\destructc}$}

          \addplot[only marks,mark=triangle,color=black] table[col sep=comma,x=param-n,y=type-user]{performance-experiments/make-nested-sig2-ltac2.txt};
          \addlegendentry{\mintinline{coq}{Constr.type} on an \LtacTwo-built projection}
          \addplotcubicregression[no markers, black][x=param-n,y=type-user][col sep=comma]{performance-experiments/make-nested-sig2-ltac2.txt};
          \edef\typea{\pgfplotstableregressiona}\edef\typeb{\pgfplotstableregressionb}\edef\typec{\pgfplotstableregressionc}\edef\typed{\pgfplotstableregressiond}
          \addlegendentry{$\pgfmathprintnumber{\typea}n^3\pgfmathprintnumber[print sign]{\typeb}n^2\pgfmathprintnumber[print sign]{\typec}n\pgfmathprintnumber[print sign]{\typed}$}

          \addplot[only marks,mark=*,color=black] table[col sep=comma,x=param-n,y=cbv-user]{performance-experiments/make-nested-sig2.txt};
          \addlegendentry{building the projection via \mintinline{coq}{cbv}}
          \addplotcubicregression[no markers, black][x=param-n,y=cbv-user][col sep=comma]{performance-experiments/make-nested-sig2.txt};
          \edef\cbva{\pgfplotstableregressiona}\edef\cbvb{\pgfplotstableregressionb}\edef\cbvc{\pgfplotstableregressionc}\edef\cbvd{\pgfplotstableregressiond}
          \addlegendentry{$\pgfmathprintnumber{\cbva}n^3\pgfmathprintnumber[print sign]{\cbvb}n^2\pgfmathprintnumber[print sign]{\cbvc}n\pgfmathprintnumber[print sign]{\cbvd}$}
      \end{axis}
  \end{tikzpicture}
  \caption{There are two ways we look at the performance of building a term like \mintinline{coq}{projT1 (projT2 ... (projT2 x))} with $n$ \mintinline{coq}{projT2}s: we can define a recursive function that computes this term and then use \mintinline{coq}{cbv} to reduce away the recursion, and time how long this takes; or we can build the term using \LtacTwo\space and then typecheck it.
    This plot displays both of these methods, and in addition displays the time it takes to run \mintinline{coq}{destruct} to break $x$ into its component fields, as lower bound for how long it takes to prove anything about a nested $\Sigma$ type with $n$ fields.} \label{fig:timing-make-destruct-nested-sig}
\end{figure*}

  \begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/make-nested-prim-sig2.txt}
        \einput{performance-experiments/destruct-nested-prim-sig2.txt}
        \einput{performance-experiments/make-nested-prim-sig2-ltac2.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        xmax=10000,
        ymin=0,
        ymax=10,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=square,color=black] table[col sep=comma,x=param-n,y=repeat-destruct-user]{performance-experiments/destruct-nested-prim-sig2.txt};
        \addlegendentry{\mintinline{coq}{repeat destruct}}
        \addplotquadraticregression[no markers, black][x=param-n,y=repeat-destruct-user][col sep=comma]{performance-experiments/destruct-nested-prim-sig2.txt};
        \edef\destructa{\pgfplotstableregressiona}\edef\destructb{\pgfplotstableregressionb}\edef\destructc{\pgfplotstableregressionc}
        \addlegendentry{$\pgfmathprintnumber{\destructa}n^2\pgfmathprintnumber[print sign]{\destructb}n\pgfmathprintnumber[print sign]{\destructc}$}

        \addplot[only marks,mark=triangle,color=black] table[col sep=comma,x=param-n,y=type-user]{performance-experiments/make-nested-prim-sig2-ltac2.txt};
        \addlegendentry{\mintinline{coq}{Constr.type} on an \LtacTwo-built projection}
        \addplotquadraticregression[no markers, black][x=param-n,y=type-user][col sep=comma]{performance-experiments/make-nested-prim-sig2-ltac2.txt};
        \edef\typea{\pgfplotstableregressiona}\edef\typeb{\pgfplotstableregressionb}\edef\typec{\pgfplotstableregressionc}
        \addlegendentry{$\pgfmathprintnumber{\typea}n^2\pgfmathprintnumber[print sign]{\typeb}n\pgfmathprintnumber[print sign]{\typec}$}

        \addplot[only marks,mark=*,color=black] table[col sep=comma,x=param-n,y=cbv-user]{performance-experiments/make-nested-prim-sig2.txt};
        \addlegendentry{building the projection via \mintinline{coq}{cbv}}
        \addplotquadraticregression[no markers, black][x=param-n,y=cbv-user][col sep=comma]{performance-experiments/make-nested-prim-sig2.txt};
        \edef\cbva{\pgfplotstableregressiona}\edef\cbvb{\pgfplotstableregressionb}\edef\cbvc{\pgfplotstableregressionc}
        \addlegendentry{$\pgfmathprintnumber{\cbva}n^2\pgfmathprintnumber[print sign]{\cbvb}n\pgfmathprintnumber[print sign]{\cbvc}$}
    \end{axis}
\end{tikzpicture}
\caption{The same graph as \autoref{fig:timing-make-destruct-nested-sig}, but with primitive projections turned on.
Note that the $x$-axis is $10\times$ larger on this plot.} \label{fig:timing-make-destruct-nested-prim-sig}
\end{figure*}

  \begin{figure*}
      \beginTikzpictureStamped{
          \einput{performance-experiments/make-destruct-nested-record.txt}
          \einput{performance-experiments/make-destruct-nested-prim-record.txt}
        }
        \begin{axis}[xlabel=$n$,
            ylabel=time (s),
            legend pos=north west,
            width=0.95\textwidth,
            axis lines=left,
            xmin=0,
            scaled x ticks=false,
            scaled y ticks=false]
            \addplot[only marks,mark=triangle,color=black] table[col sep=comma,x=param-n,y=record-user]{performance-experiments/make-destruct-nested-record.txt};
            \addlegendentry{\mintinline{coq}{Record} command (without primitive projections)}
            \addplotquadraticregression[no markers, black][x=param-n,y=record-user][col sep=comma]{performance-experiments/make-destruct-nested-record.txt};
            \edef\recorda{\pgfplotstableregressiona}\edef\recordb{\pgfplotstableregressionb}\edef\recordc{\pgfplotstableregressionc}
            \addlegendentry{$\pgfmathprintnumber{\recorda}n^2\pgfmathprintnumber[print sign]{\recordb}n\pgfmathprintnumber[print sign]{\recordc}$}

            \addplot[only marks,mark=square*,color=black] table[col sep=comma,x=param-n,y=destruct-user]{performance-experiments/make-destruct-nested-prim-record.txt};
            \addlegendentry{\mintinline{coq}{destruct} (with primitive projections)}
            \addplotquadraticregression[no markers, black][x=param-n,y=destruct-user][col sep=comma]{performance-experiments/make-destruct-nested-prim-record.txt};
            \edef\primdestructa{\pgfplotstableregressiona}\edef\primdestructb{\pgfplotstableregressionb}\edef\primdestructc{\pgfplotstableregressionc}
            \addlegendentry{$\pgfmathprintnumber{\primdestructa}n^2\pgfmathprintnumber[print sign]{\primdestructb}n\pgfmathprintnumber[print sign]{\primdestructc}$}

            \addplot[only marks,mark=square,color=black] table[col sep=comma,x=param-n,y=destruct-user]{performance-experiments/make-destruct-nested-record.txt};
            \addlegendentry{\mintinline{coq}{destruct} (without primitive projections)}
            \addplotquadraticregression[no markers, black][x=param-n,y=destruct-user][col sep=comma]{performance-experiments/make-destruct-nested-record.txt};
            \edef\destructa{\pgfplotstableregressiona}\edef\destructb{\pgfplotstableregressionb}\edef\destructc{\pgfplotstableregressionc}
            \addlegendentry{$\pgfmathprintnumber{\destructa}n^2\pgfmathprintnumber[print sign]{\destructb}n\pgfmathprintnumber[print sign]{\destructc}$}

            \addplot[only marks,mark=triangle*,color=black] table[col sep=comma,x=param-n,y=record-user]{performance-experiments/make-destruct-nested-prim-record.txt};
            \addlegendentry{\mintinline{coq}{Record} command (with primitive projections)}
            \addplotquadraticregression[no markers, black][x=param-n,y=record-user][col sep=comma]{performance-experiments/make-destruct-nested-prim-record.txt};
            \edef\primrecorda{\pgfplotstableregressiona}\edef\primrecordb{\pgfplotstableregressionb}\edef\primrecordc{\pgfplotstableregressionc}
            \addlegendentry{$\pgfmathprintnumber{\primrecorda}n^2\pgfmathprintnumber[print sign]{\primrecordb}n\pgfmathprintnumber[print sign]{\primrecordc}$}
        \end{axis}
    \end{tikzpicture}
    \caption{Timing of running a \mintinline{coq}{Record} command to define a record with $n$ fields, and the time to \mintinline{coq}{destruct} such a record.
    Note that building the goal involving projecting out the last field takes less than 0.001s for all numbers of fields that we tested.
(Presumably for large enough numbers of fields, we'd start getting a logarithmic overhead from parsing the name of the final field, which, when represented as $x$ followed by the field number in base 10, does grow in size as $\log_{10}n$.)
Note that the non-monotonic timing is \emph{reproducible}, and we have asked the Coq developers about it at \coqbug{12270}.}
\label{fig:timing-make-destruct-nested-record}
\end{figure*}

%#!/usr/bin/env bash
%
%set -e
%
%cat "$0" | sed s'/^/%/g'
%
%COLORS=(red green blue cyan magenta yellow black gray brown lime olive orange pink purple teal violet darkgray lightgray)
%MARKS=(o asterisk star oplus otimes square square* triangle triangle* diamond diamond* pentagon pentagon* - '|' oplus* otimes*)
%
%for kind in performance-experiments/make-nested-*-abstraction.txt; do
%    cat <<'EOF'
%\begin{figure*}
%    \beginTikzpictureStamped{
%EOF
%    echo '        \einput{'"$kind"'}'
%    cat <<'EOF'
%    }
%    \begin{axis}[xlabel=$n$,
%        ylabel=time (s),
%        legend pos=north west,
%        width=0.95\textwidth,
%        axis lines=left,
%        xmin=0,
%        ymin=0,
%        scaled x ticks=false,
%        scaled y ticks=false]
%EOF
%    i=0
%    for y in $(grep -o '[^ ]*-user' $kind); do
%        #,mark=square,color=black
%        echo '        \addplot[only marks,'"mark=${MARKS[$i]},color=${COLORS[i]}"'] table[col sep=comma,x=param-n,y='"$y"']{'"$kind"'};'
%        echo '        \addlegendentry{'"$y"'}'
%        i=$(($i+1))
%    done
%    cat <<'EOF'
%    \end{axis}
%\end{tikzpicture}
%EOF
%    echo '\caption{timing-'"$kind"'} \label{fig:timing-make-destruct-record-definition-abstraction-'"$kind"'}'
%    cat <<'EOF'
%\end{figure*}
%EOF
%done
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/make-nested-prim-prod-abstraction.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        ymin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=o,color=red] table[col sep=comma,x=param-n,y=constr-final-goal-make-final-goal-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{constr-final-goal-make-final-goal-user}
        \addplot[only marks,mark=asterisk,color=green] table[col sep=comma,x=param-n,y=everything-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{everything-user}
        \addplot[only marks,mark=star,color=blue] table[col sep=comma,x=param-n,y=make-Build-T-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{make-Build-T-user}
        \addplot[only marks,mark=oplus,color=cyan] table[col sep=comma,x=param-n,y=make-T-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{make-T-user}
        \addplot[only marks,mark=otimes,color=magenta] table[col sep=comma,x=param-n,y=make-T-eta-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{make-T-eta-user}
        \addplot[only marks,mark=square,color=yellow] table[col sep=comma,x=param-n,y=make-T-projs-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{make-T-projs-user}
        \addplot[only marks,mark=square*,color=black] table[col sep=comma,x=param-n,y=make-T-rect-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{make-T-rect-user}
        \addplot[only marks,mark=triangle,color=gray] table[col sep=comma,x=param-n,y=make-Ts-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{make-Ts-user}
        \addplot[only marks,mark=triangle*,color=brown] table[col sep=comma,x=param-n,y=make-f-ty-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{make-f-ty-user}
        \addplot[only marks,mark=diamond,color=lime] table[col sep=comma,x=param-n,y=making-user]{performance-experiments/make-nested-prim-prod-abstraction.txt};
        \addlegendentry{making-user}
    \end{axis}
\end{tikzpicture}
\caption{timing-performance-experiments/make-nested-prim-prod-abstraction.txt} \label{fig:timing-make-destruct-record-definition-abstraction-performance-experiments/make-nested-prim-prod-abstraction.txt}
\end{figure*}
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/make-nested-prim-sig-abstraction.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        ymin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=o,color=red] table[col sep=comma,x=param-n,y=constr-final-goal-make-final-goal-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{constr-final-goal-make-final-goal-user}
        \addplot[only marks,mark=asterisk,color=green] table[col sep=comma,x=param-n,y=everything-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{everything-user}
        \addplot[only marks,mark=star,color=blue] table[col sep=comma,x=param-n,y=make-Build-T-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{make-Build-T-user}
        \addplot[only marks,mark=oplus,color=cyan] table[col sep=comma,x=param-n,y=make-T-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{make-T-user}
        \addplot[only marks,mark=otimes,color=magenta] table[col sep=comma,x=param-n,y=make-T-eta-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{make-T-eta-user}
        \addplot[only marks,mark=square,color=yellow] table[col sep=comma,x=param-n,y=make-T-projs-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{make-T-projs-user}
        \addplot[only marks,mark=square*,color=black] table[col sep=comma,x=param-n,y=make-T-rect-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{make-T-rect-user}
        \addplot[only marks,mark=triangle,color=gray] table[col sep=comma,x=param-n,y=make-Ts-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{make-Ts-user}
        \addplot[only marks,mark=triangle*,color=brown] table[col sep=comma,x=param-n,y=make-f-ty-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{make-f-ty-user}
        \addplot[only marks,mark=diamond,color=lime] table[col sep=comma,x=param-n,y=making-user]{performance-experiments/make-nested-prim-sig-abstraction.txt};
        \addlegendentry{making-user}
    \end{axis}
\end{tikzpicture}
\caption{timing-performance-experiments/make-nested-prim-sig-abstraction.txt} \label{fig:timing-make-destruct-record-definition-abstraction-performance-experiments/make-nested-prim-sig-abstraction.txt}
\end{figure*}
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/make-nested-prod-abstraction.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        ymin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=o,color=red] table[col sep=comma,x=param-n,y=constr-final-goal-make-final-goal-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{constr-final-goal-make-final-goal-user}
        \addplot[only marks,mark=asterisk,color=green] table[col sep=comma,x=param-n,y=eta-cast-pf-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{eta-cast-pf-user}
        \addplot[only marks,mark=star,color=blue] table[col sep=comma,x=param-n,y=eta-constr-pf-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{eta-constr-pf-user}
        \addplot[only marks,mark=oplus,color=cyan] table[col sep=comma,x=param-n,y=eta-make-let-ins-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{eta-make-let-ins-user}
        \addplot[only marks,mark=otimes,color=magenta] table[col sep=comma,x=param-n,y=eta-make-pf-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{eta-make-pf-user}
        \addplot[only marks,mark=square,color=yellow] table[col sep=comma,x=param-n,y=eta-refine-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{eta-refine-user}
        \addplot[only marks,mark=square*,color=black] table[col sep=comma,x=param-n,y=eta-refine-and-close-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{eta-refine-and-close-user}
        \addplot[only marks,mark=triangle,color=gray] table[col sep=comma,x=param-n,y=everything-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{everything-user}
        \addplot[only marks,mark=triangle*,color=brown] table[col sep=comma,x=param-n,y=make-Build-T-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{make-Build-T-user}
        \addplot[only marks,mark=diamond,color=lime] table[col sep=comma,x=param-n,y=make-T-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{make-T-user}
        \addplot[only marks,mark=diamond*,color=olive] table[col sep=comma,x=param-n,y=make-T-eta-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{make-T-eta-user}
        \addplot[only marks,mark=pentagon,color=orange] table[col sep=comma,x=param-n,y=make-T-projs-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{make-T-projs-user}
        \addplot[only marks,mark=pentagon*,color=pink] table[col sep=comma,x=param-n,y=make-T-rect-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{make-T-rect-user}
        \addplot[only marks,mark=-,color=purple] table[col sep=comma,x=param-n,y=make-Ts-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{make-Ts-user}
        \addplot[only marks,mark=|,color=teal] table[col sep=comma,x=param-n,y=make-f-ty-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{make-f-ty-user}
        \addplot[only marks,mark=oplus*,color=violet] table[col sep=comma,x=param-n,y=making-user]{performance-experiments/make-nested-prod-abstraction.txt};
        \addlegendentry{making-user}
    \end{axis}
\end{tikzpicture}
\caption{timing-performance-experiments/make-nested-prod-abstraction.txt} \label{fig:timing-make-destruct-record-definition-abstraction-performance-experiments/make-nested-prod-abstraction.txt}
\end{figure*}
\begin{figure*}
    \beginTikzpictureStamped{
        \einput{performance-experiments/make-nested-sig-abstraction.txt}
    }
    \begin{axis}[xlabel=$n$,
        ylabel=time (s),
        legend pos=north west,
        width=0.95\textwidth,
        axis lines=left,
        xmin=0,
        ymin=0,
        scaled x ticks=false,
        scaled y ticks=false]
        \addplot[only marks,mark=o,color=red] table[col sep=comma,x=param-n,y=constr-final-goal-make-final-goal-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{constr-final-goal-make-final-goal-user}
        \addplot[only marks,mark=asterisk,color=green] table[col sep=comma,x=param-n,y=eta-cast-pf-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{eta-cast-pf-user}
        \addplot[only marks,mark=star,color=blue] table[col sep=comma,x=param-n,y=eta-constr-pf-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{eta-constr-pf-user}
        \addplot[only marks,mark=oplus,color=cyan] table[col sep=comma,x=param-n,y=eta-make-let-ins-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{eta-make-let-ins-user}
        \addplot[only marks,mark=otimes,color=magenta] table[col sep=comma,x=param-n,y=eta-make-pf-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{eta-make-pf-user}
        \addplot[only marks,mark=square,color=yellow] table[col sep=comma,x=param-n,y=eta-refine-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{eta-refine-user}
        \addplot[only marks,mark=square*,color=black] table[col sep=comma,x=param-n,y=eta-refine-and-close-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{eta-refine-and-close-user}
        \addplot[only marks,mark=triangle,color=gray] table[col sep=comma,x=param-n,y=everything-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{everything-user}
        \addplot[only marks,mark=triangle*,color=brown] table[col sep=comma,x=param-n,y=make-Build-T-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{make-Build-T-user}
        \addplot[only marks,mark=diamond,color=lime] table[col sep=comma,x=param-n,y=make-T-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{make-T-user}
        \addplot[only marks,mark=diamond*,color=olive] table[col sep=comma,x=param-n,y=make-T-eta-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{make-T-eta-user}
        \addplot[only marks,mark=pentagon,color=orange] table[col sep=comma,x=param-n,y=make-T-projs-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{make-T-projs-user}
        \addplot[only marks,mark=pentagon*,color=pink] table[col sep=comma,x=param-n,y=make-T-rect-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{make-T-rect-user}
        \addplot[only marks,mark=-,color=purple] table[col sep=comma,x=param-n,y=make-Ts-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{make-Ts-user}
        \addplot[only marks,mark=|,color=teal] table[col sep=comma,x=param-n,y=make-f-ty-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{make-f-ty-user}
        \addplot[only marks,mark=oplus*,color=violet] table[col sep=comma,x=param-n,y=making-user]{performance-experiments/make-nested-sig-abstraction.txt};
        \addlegendentry{making-user}
    \end{axis}
\end{tikzpicture}
\caption{timing-performance-experiments/make-nested-sig-abstraction.txt} \label{fig:timing-make-destruct-record-definition-abstraction-performance-experiments/make-nested-sig-abstraction.txt}
\end{figure*}

\minortodo{Find more examples to talk about here?}

\begin{comment}
\begin{subappendices}
 \todo{this chapter}

\section{Abstraction barriers}
e.g., f (fst x) (snd x) where fst x := let (a, b) := x in a; vs let (a, b) := x in f a b.


\section{more stuff from CT performance paper / presentation}
\todo{this section}
\end{subappendices}
\end{comment}
