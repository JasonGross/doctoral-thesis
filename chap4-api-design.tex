\chapter{Design-based fixes} \label{ch:design} \label{ch:api-design}

Okay, so that's the like introduction introduction and like. Palate of slowness. The next section is one of the two big sections on like components of the system that is important for performance and this section is geared at users of proof assistance mainly. And. This is about how to design your APIs. 

How to design the like interface of functions that you're doing so. Most a lot of languages have a type system this is things that are like if you have a function that takes a string and you pass it an integer you want to know that you made a mistake. 

And caulk cock has this on steroids because not only can you say this function takes an integer where this function takes a string you can also say things like this function takes if this. Turing machine holds an element of the one element set and if this turning machine does not halt an element of the empty cell, you can't quite say that but you can say if it holds within n steps see you can say this takes a number n and the turing machine t and if, T holds within n steps then it takes an element of the one element set and if t does not hold with an end steps that it takes an element of the empty set what is the utility is this kind of precision, oh this lets you pass around troops. 

Like just by virtue of being able to call the function you have a proof that some fact is true. Right okay, so it's like a computation like means to make things more efficient than oh it's it's a source of power. It's like. Let's you write functions about that like taken proofs that you and like let's you write functions that prove things correct it let's you do things like this is a function that takes an approved that ah, This list is not empty and it's out of proof that like if you append that list to itself the resulting list is not empty. 

This is useful why because you want to prove things. Okay, for example. Maybe you want to say. This function takes in a C program. It takes in. A function that's describes the behavior of the C program and it takes an approved that the C program matches the behavior of the function. 

And what it spits out is some assembly code and a proof that the assembly code matches the behavior of the function. I see. 

And like this is how you get verified programs, but it doesn't matter the thing where you said that well. I still understand the part of like why using it as a function is useful or something. Like making a function out of it what's the alternative if you didn't have it like as a function calling it like what are you saving when you have a function that does this oh so it's sort of a question of how you found your mathematics on or like how you how you're doing mathematics, so like one way of doing proofs is that ah programs improve through the same thing. 

So you can reuse all the machinery for writing programs to also write proofs and you can reuse all the machinery for like making sure that a program is well typed to making sure that a proof is valid. Okay, it's kind of neat a different way you could do it as you could found your math on set theory and you could be like, I'm just applying axioms. 

This is how proof assistant called Isabel. Hall works almost for higher order logic. 

What is the higher order logic h o l. 

So like there is a sort of design choice of like what? Like how you're building your prefix system tonight out have much to say about. Preface systems that are not based on type theory, okay? Oh. Other questions no okay so. Great so you can put arbitrary amounts of computation into this like thing that in other languages is used for checking that you don't try to pass an integer as a string. 

Um, and that's kind of neat it's kind of cool it also turns out to be. Useful because the part that does the checking is decently optimized in particular when you're like running fully running a program that's pretty optimized and so if you can stick all of the proof like all of the hard work into this question of how I passing an element of the empty set or of the one element set. 

Then you're like, you'll happy. Um, Also if you restrict yourself. To only asking questions like do I have a string or like asking questions like please make sure I have a string not an integer. You're also pretty happy like things are fast you need to do your proof separately but things still work nicely. 

The problem is that with all this power. That usually works it's like very tempting to use a and so you're like, okay, give me a list give me the length of the list also give me a proof that the the list has that length. And like this is fine and it works fine until you try to scale it and then it doesn't okay, ah because you're like okay, well if I like. 

Append to lists. I spit out a proof that like the so I spit out that the length should be like the sum of the two lengths I spit out of the proof that like the depending of the lists has length equal to the sum of the two lens. And then you're like okay and if I reverse a list it keeps the same length, and maybe now you want to like match like in one part of the code you were like I reversed my lists and then I appended them in reverse order. 

And another part you were like I I like first depended the lists and then I reversed the whole thing and like these are the same and I should be able to know that they're the same. 

But now here you're like length one plus length two and here you're like length two plus one. And so now your numbers are in the same and you're proofs are different. Ah, and maybe you manage to like make call accidentally try to figure out exactly what the proof is. 

Rather than trying to just like verify that your numbers line up or something. And now suddenly it's spending a lot of time trying to figure out exactly what your complicated proof is. And this is why you when you like make your example a little bit bigger and careful on this place it like takes the other path and now instead of being like, oh oops. 

I guess this path doesn't work after one second now, it's like. I'm gonna keep trying for a month hmm. And then you're real sad. 

So the the lesson today out of this is like either put all of the work into the ah into the types. We're all turnatively like either but all of the work into the computational power at the types, or make sure that you're not using the computational power of the types at all. 

What you mean not using the computational power of the types at all, you're not computing anything when you're asking like is string the same thing as string or string the same thing as integer, oh what would you do instead oh I mean there there's like nothing to compute you just compare them, okay? 

Right and then you're like, um, but like what about the identity function applied to string and you're like yeah yeah that's fine and so you can do a little bit of computation but if you're not careful then you're then you end up in this land where you're trying to compute with proofs and if you're not being careful, then you explode what's the extra stuff that you get from using the identity function versus comparing two strings, ah, Like what does it become more efficient or like what's the utility so like? 

Generally how it works is that you get to do more abstraction. Like. Like if you've ever dealt with monads in Haskell. 

There are fancy abstraction things that you can do with it, okay and like most of them don't require much computation at the type level the require a little bit of computation at the site level. Um, that's fine. And it's also fine to do all your computation at the type level but it's not fine is when you. 

When you're sort of like well do as much computation as you need. And if you're not being careful about how much computation is done at the site level, which you're usually not like it's something that's hard to track. Then and it's like the computer doesn't help you because you are going to like it it keeps working on small examples, but then you're real sad when you. 

When you end up with a little bit too much. 

Okay so that's that's this first section those the second section right oh like the first one it's the first of the like two big chunks of like things you want to do right this this thing so this part of cop that's like checking if two types do the same this is called conversion, okay, so the sections and conversions oh it's on like performance bottlenecks and conversion, okay? 


\todo{this chapter}

\section{Abstraction barriers}
e.g., f (fst x) (snd x) where fst x := let (a, b) := x in a; vs let (a, b) := x in f a b.

\section{When and how to use dependent types painlessly}
The extremes are relatively easy:
\begin{itemize}
\item Total separation between proofs and programs, so that programs are simply typed, works relatively well
\item Pre-existing mathematics, where objects are fully bundled with proofs and never need to be separated from them, also works relatively well
\item The rule of thumb in the middle: it is painful to recombine proofs and programs after you separate them; if you are doing it to define an opaque transformation that acts on proof-carrying code, that is okay, but if you cannot make that abstraction barrier, enormous pain results.
\item For example, if you have length-indexed lists and want to index into them with elements of a finite type, things are fine until you need to divorce the index from its proof of finiteness.  If you, for example, want to index into, say, the concatenation of two lists, with an index into the first of the lists, then you will likely run into trouble, because you are trying to consider the index separately from its proof of finitude, but you have to recombine them to do the indexing.
\end{itemize}

More examples from the rewriter?

\section{more stuff from CT performance paper / presentation}
\todo{this section}
