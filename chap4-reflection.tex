\chapter{Reflective Program Transformation} \label{ch:reflection}

\section{Introduction} \label{sec:reification-by-parametricity:intro}

\todo{reformat this for flow, right now it's from reification-by-parametricity}

Proof by reflection~\cite{ReflectionTACS97} is an established method for employing verified proof procedures, within larger proofs.
There are a number of benefits to using verified functional programs written in the proof assistant's logic, instead of tactic scripts.
We can often prove that procedures always terminate without attempting fallacious proof steps, and perhaps we can even prove that a procedure gives logically complete answers, for instance telling us definitively whether a proposition is true or false.
In contrast, tactic-based procedures may encounter runtime errors or loop forever.
As a consequence, if we want to keep the trusted codebase small, as discussed in \autoref{sec:debruijn-criterion}, these tactic procedures must output proof terms, justifying their decisions, and these terms can grow large, making for slower proving and requiring transmission of large proof terms to be checked slowly by others.
A verified procedure need not generate a certificate for each invocation.

The starting point for proof by reflection is \emph{reification}: translating a ``native'' term of the logic into an explicit abstract syntax tree.
We may then feed that tree to verified procedures or any other functional programs in the logic.
The benefits listed above are particularly appealing in domains where goals are very large.
For instance, consider verification of large software systems, where we might want to reify thousands of lines of source code.
Popular methods turn out to be surprisingly slow, often to the point where, counter-intuitively, the majority of proof-execution time is spent in reification -- unless the proof engineer invests in writing a plugin directly in the proof assistant's metalanguage (e.g., OCaml for Coq).

\todo{move this paragraph elsewhere}
In this paper, we show that reification can be both simpler and faster than with standard methods.
Perhaps surprisingly, we demonstrate how to reify terms almost entirely through reduction in the logic, with a small amount of tactic code for setup and no ML programming.
Though our techniques should be broadly applicable, especially in proof assistants based on type theory, our experience is with Coq, and we review the requisite background in the remainder of this introduction.
In \autoref{sec:reif-survey}, we summarize our survey into prior approaches to reification and provide high-quality implementations and documentation for them, serving a tutorial function independent of our new contributions.
Experts on the subject might want to skip directly to \autoref{sec:reification-by-parametricity}, which explains our alternative technique.
We benchmark our approach against 18 competitors in \autoref{sec:perf}.

\subsection{Proof-Script Primer}
\todo{rename this subsection}
Basic Coq proofs are often written as lists of steps such as \mintinline{coq}{induction} on some structure, \mintinline{coq}{rewrite} using a known equivalence, or \mintinline{coq}{unfold} of a definition.
Very quickly, proofs can become long and tedious, both to write and to read, and hence Coq provides \Ltac, a scripting language for proofs.
As theorems and proofs grow in complexity, users frequently run into performance and maintainability issues with \Ltac.
Consider the case where we want to prove that a large algebraic expression, involving many \letindots\space expressions, is even:
\begin{minted}{coq}
Inductive is_even : nat -> Prop :=
| even_O : is_even O
| even_SS : forall x, is_even x -> is_even (S (S x)).
Goal is_even (let x := 100 * 100 * 100 * 100 in
              let y := x * x * x * x in
              y * y * y * y).
\end{minted}
Coq stack-overflows if we try to reduce this goal.
As a workaround, we might write a lemma that talks about evenness of \letindots, plus one about evenness of multiplication, and we might then write a tactic that composes such lemmas.

\todo{factor some of this with chapter 2}
Even on smaller terms, though, proof size can quickly become an issue.
If we give a naive proof that 7000 is even, the proof term will contain all of the even numbers between 0 and 7000, giving a proof-term-size blow-up at least quadratic in size (recalling that natural numbers are represented in unary; the challenges remain for more efficient base encodings).
Clever readers will notice that Coq could share subterms in the proof tree, recovering a term that is linear in the size of the goal.
However, such sharing would have to be preserved very carefully, to prevent size blow-up from unexpected loss of sharing, and today's Coq version does not do that sharing.
Even if it did, tactics that rely on assumptions about Coq's sharing strategy become harder to debug, rather than easier.

\subsection{Reflective-Automation Primer}\label{sec:evenness}
Enter reflective automation, which simultaneously solves both the problem of performance and the problem of debuggability.
Proof terms, in a sense, are traces of a proof script.
They provide Coq's kernel with a term that it can check to verify that no illegal steps were taken.
Listing every step results in large traces.

\begin{wrapfigure}[9]{r}{6cm}
%\vspace{-33pt}
\begin{minted}{coq}
Fixpoint check_is_even (n : nat) : bool
  := match n with
     | 0 => true
     | 1 => false
     | S (S n) => check_is_even n
     end.
\end{minted}
%\vspace{-18pt}
\caption{Evenness Checking}\label{fig:check-is-even}
\end{wrapfigure}
The idea of reflective automation is that, if we can get a formal encoding of our goal, plus an algorithm to \emph{check} the property we care about, then we can do much better than storing the entire trace of the program.
We can prove that our checker is correct once and for all, removing the need to trace its steps.

A simple evenness checker can just operate on the unary encoding of natural numbers (\autoref{fig:check-is-even}).
We can use its correctness theorem to prove goals much more quickly:
\begin{minted}{coq}
Theorem soundness : forall n, check_is_even n = true -> is_even n.
Goal is_even 2000.
  Time repeat (apply even_SS || apply even_O). (* 1.8 s *)
  Undo.
  Time apply soundness; vm_compute; reflexivity. (* 0.004 s *)
\end{minted}
The tactic \mintinline{coq}{vm_compute} tells Coq to use its virtual machine for reduction, to compute the value of \mintinline{coq}{check_is_even 2000}, after which \mintinline{coq}{reflexivity} proves that \mintinline{coq}{true = true}.
Note how much faster this method is.
In fact, even the asymptotic complexity is better; this new algorithm is linear rather than quadratic in \mintinline{coq}{n}.

However, even this procedure takes a bit over three minutes to prove the goal \mintinline{coq}{is_even (10 * 10 * 10 * 10 * 10 * 10 * 10 * 10 * 10)}.
To do better, we need a formal representation of terms or expressions.

\subsection{Reflective-Syntax Primer}
\todo{should the footnote be uncommented here?}
Sometimes, to achieve faster proofs, we must be able to tell, for example, whether we got a term by multiplication or by addition, and not merely whether its normal form is 0 or a successor.%
%\footnote{%
%  Sometimes this distinction is necessary for generating a proof at all, as is the case in \mintinline{coq}{nsatz} and \mintinline{coq}{romega}; there is no way to prove that addition is commutative if you cannot identify what things you were adding in the first place.%
%}

\begin{wrapfigure}[4]{r}{6cm}
%\vspace{-45pt}
\begin{minted}{coq}
Inductive expr :=
| NatO : expr
| NatS (x : expr) : expr
| NatMul (x y : expr) : expr.
\end{minted}
%\vspace{-15pt}
\caption{Simple Expressions}\label{fig:inductive-expr-no-letin}
\end{wrapfigure}

A reflective automation procedure generally has two steps.
The first step is to \emph{reify} the goal into some abstract syntactic representation, which we call the \emph{term language} or an \emph{expression language}.
The second step is to run the algorithm on the reified syntax.

What should our expression language include?
At a bare minimum, we must have multiplication nodes, and we must have \mintinline{coq}{nat} literals.
If we encode \mintinline{coq}{S} and \mintinline{coq}{O} separately, a decision that will become important later in~\autoref{sec:reification-by-parametricity}, we get the inductive type of \autoref{fig:inductive-expr-no-letin}.

Before diving into methods of reification, let us write the evenness checker.
\begin{minted}{coq}
Fixpoint check_is_even_expr (t : expr) : bool
  := match t with
     | NatO => true
     | NatS x => negb (check_is_even_expr x)
     | NatMul x y => orb (check_is_even_expr x) (check_is_even_expr y)
     end.
\end{minted}
%We have used \mintinline{coq}{negb} and \mintinline{coq}{orb} from the standard library for boolean negation and disjunction respectively.

Before we can state the soundness theorem (whenever this checker returns \mintinline{coq}{true}, the represented number is even), we must write the function that tells us what number our expression represents, called \emph{denotation} or \emph{interpretation}:
\begin{minted}{coq}
Fixpoint denote (t : expr) : nat
  := match t with
     | NatO => O
     | NatS x => S (denote x)
     | NatMul x y => denote x * denote y
     end.

Theorem check_is_even_expr_sound (e : expr)
  : check_is_even_expr e = true -> is_even (denote e).
\end{minted}

Given a tactic \mintinline{coq}{Reify} to produce a reified term from a \mintinline{coq}{nat}, we can time \mintinline{coq}{check_is_even_expr}.
It is instant on the last example.%, \mintinline{coq}{10 * 10 * 10 * 10 * 10 * 10 * 10 * 10 * 10}.

Before we proceed to reification, we will introduce one more complexity.
If we want to support our initial example with \letindots\space efficiently, we must also have \mintinline{coq}{let}-expressions.
Our current procedure that inlines \mintinline{coq}{let}-expressions takes 19 seconds, for example, on \mintinline{coq}{let x0 := 10 * 10 in let x1 := x0 * x0 in … let x24 := x23 * x23 in x24}.
The choices of representation of binders, which are essential to encoding \mintinline{coq}{let}-expressions, include higher-order abstract syntax (HOAS)~\cite{HOAS}, parametric higher-order abstract syntax (PHOAS)~\cite{PhoasICFP08} also known as weak HOAS~\cite{weak2013Ciaffaglione}, de Bruijn indices~\cite{debruijn1972}, nominal representations~\cite{Nominal2003Pitts}, locally nameless representations~\cite{Locally2012Chargueraud,locally2007Leroy}, named representations, and nested abstract syntax~\cite{Nested2012Hirschowitz,deBruijn1999Bird}.
A survey of a number of options for binding can be found in \autocite{Engineering2008Aydemir}.
\todoask{is this a place to cite POPLMark?  What should be said about it?  Should I mention more sorts of representations?}

Although we will eventually choose the PHOAS representation for the tools presented in \autoref{ch:rewriting,ch:reification-by-parametricity}, we will also briefly survey some of the options for encoding binders, with an eye towards performance implications.

\subsubsection{PHOAS} \label{sec:binders:PHOAS}

The PHOAS representation is particularly convenient.
In PHOAS, expression binders are represented by binders in Gallina, the functional language of Coq, and the expression language is parameterized over the type of the binder.
%We make this binder type implicit so that we can often omit writing it.
Let us define a constant and notation for \mintinline{coq}{let} expressions as definitions (a common choice in real Coq developments, to block Coq's default behavior of inlining \mintinline{coq}{let} binders silently; the same choice will also turn out to be useful for reification later).
We thus have: \label{sec:phoas-expr-def}
\begin{minted}{coq}
Inductive expr {var : Type} :=
| NatO : expr
| NatS : expr -> expr
| NatMul : expr -> expr -> expr
| Var : var -> expr
| LetIn : expr -> (var -> expr) -> expr.
Definition Let_In {A B} (v : A) (f : A -> B) := let x := v in f x.
Notation "'dlet' x := v 'in' f" := (Let_In v (fun x => f)).
Notation "'elet' x := v 'in' f" := (LetIn v (fun x => f)).
Fixpoint denote (t : @expr nat) : nat
  := match t with
     | NatO => O
     | NatS x => S (denote x)
     | NatMul x y => denote x * denote y
     | Var v => v
     | LetIn v f => dlet x := denote v in denote (f x)
     end.
Fixpoint check_is_even_expr (t : @expr bool) : bool
  := match t with
     | NatO => true
     | NatS x => negb (check_is_even_expr x)
     | NatMul x y => orb (check_is_even_expr x) (check_is_even_expr y)
     | Var v_even => v_even
     | LetIn v f => let v_even := check_is_even_expr v in
                    check_is_even_expr (f v_even)
     end.
\end{minted}

Note, importantly, that \mintinline{coq}{check_is_even_expr} and \mintinline{coq}{denote} take \mintinline{coq}{expr}s with \emph{different} instantiations of the \mintinline{coq}{var} parameter.
This is necessary so that we can store the information about whether or not a particular \mintinline{coq}{let}-bound expression is even (or what its denotation is) in the variable node itself.
However, this means that we cannot reuse the same expression as arguments to both functions to formulate the soundness condition.
Instead, we must introduce a notion of \emph{relatedness} of expressions with different instantiations of the \mintinline{coq}{var} parameter.

Such a relatedness predicate has one constructor for each constructor of \mintinline{coq}{expr}, essentially encoding that the two expressions have the same structure.
For the \mintinline{coq}{Var} case, we defer to membership in a list of ``related'' variables, which we extend each time we go underneath a binder.
\begin{minted}{coq}
Inductive related {var1 var2 : Type}
  : list (var1 * var2) -> @expr var1 -> @expr var2 -> Prop :=
| RelatedNatO {Γ}
 : related Γ NatO NatO
| RelatedNatS {Γ e1 e2}
 : related Γ e1 e2 -> related Γ (NatS e1) (NatS e2)
| RelatedNatMul {Γ x1 x2 y1 y2}
 : related Γ x1 x2 -> related Γ y1 y2 -> related Γ (NatMul x1 y1) (NatMul x2 y2)
| RelatedVar {Γ v1 v2}
 : (v1, v2) ∈ Γ -> related Γ (Var v1) (Var v2)
| RelatedLetIn {Γ e1 e2 f1 f2}
 : related Γ e1 e2 -> (∀ v1 v2, related ((v1, v2) :: Γ) (f1 v1) (f2 v2))
   -> related Γ (LetIn e1 f1) (LetIn e2 f2).
\end{minted}

Conventionally, syntax trees are parametric over the value of the \mintinline{coq}{var} parameter, and we require that all instantiations give related ASTs (in the empty context), whence we call the parametric AST \emph{well-formed}:
\begin{minted}{coq}
Definition Expr := ∀ var, @expr var.
Definition Wf (e : Expr) := ∀ var1 var2, related [] (e var1) (e var2)
\end{minted}

We could then prove a modified form of our soundness theorem:
\begin{minted}{coq}
Theorem check_is_even_expr_sound (e : Expr) (H : Wf e)
: check_is_even_expr (e bool) = true -> is_even (denote (e nat)).
\end{minted}

To complete the picture, we would need a tactic \mintinline{coq}{Reify} which took in a term of type \mintinline{coq}{nat} and gave back a term of type \mintinline{coq}{forall var, @expr var}, plus a tactic \mintinline{coq}{prove_wf} which solved a goal of the form \mintinline{coq}{Wf e} by repeated application of constructors.
Given these, we could solve an evenness goal by writing%
\footnote{%
  Note that for the \mintinline{coq}{refine} to be fast, we must issue something like \mintinline{coq}{Strategy -10 [denote]} to tell Coq to unfold \mintinline{coq}{denote} before \mintinline{coq}{Let_In}.
  }
\begin{minted}{coq}
match goal with
| [ |- is_even ?v ]
  => let e := Reify v in
     refine (check_is_even_expr_sound e _ _);
     [ prove_wf | vm_compute; reflexivity ]
end.
\end{minted}
%\todo{Should we mention that for the \mintinline{coq}{refine} to be fast, we need a \mintinline{coq}{Strategy} command to tell Coq to unfold \mintinline{coq}{denote} before \mintinline{coq}{Let_In}?}

\subsubsection{Multiple Types} \label{sec:multiple-types-ASTs}
One important point, not yet mentioned, is that sometimes we want our reflective language to handle multiple types of terms.
For example, we might want to enrich our language of expressions with lists.
Since expressions like ``take the successor of this list'' don't make sense, the natural choice is to index the inductive over codes for types.

We might write:
\begin{minted}{coq}
Inductive type := Nat | List (_ : type).
Inductive expr {var : type -> Type} : type -> Type :=
| NatO : expr Nat
| NatS : expr Nat -> expr Nat
| NatMul : expr Nat -> expr Nat -> expr Nat
| Var {t} : var t -> expr t
| LetIn {t1 t2} : expr t1 -> (var t1 -> expr t2) -> expr t2
| Nil {t} : expr (List t)
| Cons {t} : expr t -> expr (List t) -> expr (List t)
| Length {t} : expr (List t) -> expr Nat.
\end{minted}
We would then have to adjust the definitions of the other functions accordingly.
The type signatures of the might functions become

\begin{minted}{coq}
Fixpoint denote_type (t : type) : Type
  := match t with
     | Nat => nat
     | List t => list (denote_type t)
     end.
Fixpoint even_data_of_type (t : type) : Type
  := match t with
     | Nat => bool (* is the nat even or not? *)
     | List t => list (even_data_of_type t)
     end.
Fixpoint denote {t} (e : @expr denote_type t) : denote_type t.
Fixpoint check_is_even_expr {t} (e : @expr even_data_of_type t)
  : even_data_of_type t.
Inductive related {var1 var2 : type -> Type}
  : list { t : type & var1 t * var2 t} -> ∀ {t}, @expr var1 t -> @expr var2 t -> Prop.
Definition Expr (t : type) := ∀ var, @expr var t.
Definition Wf {t} (e : Expr t)
  := ∀ var1 var2, related [] (e var1) (e var2).
\end{minted}

See, e.g., \citet{PhoasICFP08} for a fuller treatment.

\subsubsection{de Bruijn Indices} \label{sec:binders:de-bruijn}
The idea behind \emph{de Bruijn indices} is that variables are encoded by numbers which count up starting from the nearest enclosing binder.
We might write
\begin{minted}{coq}
Inductive expr :=
| NatO : expr
| NatS : expr -> expr
| NatMul : expr -> expr -> expr
| Var : nat -> expr
| LetIn : expr -> expr -> expr.
Fixpoint denote (default : nat) (Γ : list nat) (t : @expr nat) : nat
  := match t with
     | NatO => O
     | NatS x => S (denote default Γ x)
     | NatMul x y => denote default Γ x * denote default Γ y
     | Var idx => nth_default default Γ idx
     | LetIn v f => dlet x := denote default Γ v in
                    denote default (x :: Γ) f
     end.
\end{minted}
If we wanted a more efficient representation, we could choose better data-structures for the context $\Gamma$ and variable indices than linked lists and unary-encoded natural numbers.
One particularly convenient choice, in Coq, would be using the efficient \mintinline{coq}{PositiveMap.t} data-structure which encodes a finite map of binary-encoded \mintinline{coq}{positive}s to any type.

One unfortunate result is that the natural denotation function is no longer total.
Here we have chosen to give a denotation function which returns a default element when a variable reference is too large, but we could instead choose to return an \mintinline{coq}{option nat}.

This causes further problems when dealing with an AST type which can represent terms of multiple types.
In that case, we might annotate each variable node with a type code, mandate decidable equality of type codes, and then during denotation, we'd check the type of the variable node with the type of the corresponding variable in the context.

\subsubsection{Nested Abstract Syntax} \label{sec:binders:nested-abstract-syntax}
If we want a variant of de Bruijn indices which guarantees well-typed syntax trees, we can use nested abstract syntax.
On mono-typed ASTs, this looks like encoding the size of the context in the type of the expressions.
For example, we could use \mintinline{coq}{option} types:~\cite{Nested2012Hirschowitz}
\begin{minted}{coq}
Notation "^ V" := (option V).
Inductive expr : Type -> Type :=
| NatO {V} : expr V
| NatS {V} : expr V -> expr V
| NatMul {V} : expr V -> expr V -> expr V
| Var {V} : V -> expr V
| LetIn {V} : expr V -> expr (^V) -> expr V.
\end{minted}

This may seem a bit strange to those accustomed to encodings of terms in proof assistants, but it generalizes to a quite familiar intrinsic encoding of dependent type theory using types, contexts, and terms.~\cite{Strongly2012Benton}
Namely, when the expressions are multi-typed, we end up with something like
\begin{minted}{coq}
Inductive context :=
| emp : context
| push : type -> context -> context.
Inductive var : context -> type -> Type :=
| Var0 {t Γ} : var (push t Γ) t
| VarS {t t' Γ} : var Γ t -> var (push t' Γ) t.
Inductive expr : context -> type -> Type :=
| NatO {Γ} : expr Γ Nat
| NatS {Γ} : expr Γ Nat -> expr Γ Nat
| NatMul {Γ} : expr Γ Nat -> expr Γ Nat -> expr Γ Nat
| Var {t Γ} : var Γ t -> expr Γ t
| LetIn {Γ t1 t2} : expr Γ t1 -> expr (push t1 Γ) t2 -> expr Γ t2.
\end{minted}

Note that this generalizes nicely to codes for dependent types if the proof assistant supports induction-induction.
\todoask{Is there a better name for this section?}

Although this representation enjoys both decidable equality of binders (like de Bruijn indices), as well as being well-typed-by-construction (like PHOAS), it's unfortunately unfit for coding algorithms that need to scale without massive assistance from the proof assistant.
In particular, the na\"ive encoding of this inductive datatype incurs a quadratic overhead in representing terms involving binders, because each node stores the entire context.
It is possible in theory to avoid this blowup by dropping the indices of the inductive type from the runtime representation.~\cite{Inductive2003Brady}
One way to simulate this in Coq would be to put \mintinline{coq}{context} in \mintinline{coq}{Prop} and then extract the code to OCaml, which erases the \mintinline{coq}{Prop}s.
Alternatively, if Coq is extended with support for dropping irrelevant subterms~\cite{sprop} from the term representation, then this speedup could be accomplished even inside Coq.

\subsubsection{Nominal} \label{sec:binders:nominal}
Nominal representations use names rather than indices for binders.
These representations have the benefit of being more human-readable, but require reasoning about freshness of names and capture-avoiding substitution.
Additionally, if the representation of names is not sufficiently compact, the overhead of storing names at every binder node can become significant.

\subsubsection{Locally Nameless} \label{sec:binders:locally-nameless}
We mention the locally nameless representation because it is the term representation used by Coq itself.
This representation uses de Bruijn indices for closed terms, and names for variables which are not bound in the current term.

Much like nominal representations, locally nameless representations also incur the overhead of generating and storing names.
Na\"ive algorithms for generating fresh names, such as the algorithm used in Coq, can easily incur overhead that's linear in the size of the context.
Generating $n$ fresh names then incurs $\mathcal \Theta(n^2)$ overhead.
\todo{explain what evar substitutions are somewhere?  reference it here?}
Additionally, using a locally nameless representation requires that evar substitutions be named.
\todo{elaborate more on the performance bottlenecks of named evar substitutions}
See also \autoref{sec:setoid-rewrite-bottlenecks}.

\subsection{Performance of Proving Reflective Well-Formedness of PHOAS} \label{sec:wf:perf}

We saw in \autoref{sec:binders:PHOAS} that in order to prove the soundness theorem, we needed a way to relate two PHOASTs, which generalized to a notion of well-formedness for the \mintinline{coq}{Expr} type.

Unfortunately, the proof that two \mintinline{coq}{expr}s are \mintinline{coq}{related} is quadratic in the size of the expression, for much the same reason that proving conjunctions in \autoref{sec:quadratic-conj-certificate} resulted in a proof term which was quadratic in the number of conjuncts.
We present two ways to encode linearly-sized proofs of well-formedness in PHOAS.

\subsubsection{Iterating Reflection} \label{sec:wf:perf:reflective}

The first method of encoding linearly-sized proofs of \mintinline{coq}{related} is itself a good study in how using proof by reflection can compress proof terms.
Rather than constructing the inductive \mintinline{coq}{related} proof, we can instead write a fixed point:
\begin{minted}{coq}
Fixpoint is_related {var1 var2 : Type} (Γ : list (var1 * var2))
    (e1 : @expr var1) (e2 : @expr var2) : Prop :=
  match e1, e2 with
  | NatO, NatO => True
  | NatS e1, NatS e2 => is_related Γ e1 e2
  | NatMul x1 y1, NatMul x2 y2
    => is_related Γ x1 x2 /\ is_related Γ y1 y2
  | Var v1, Var v2 => List.In (v1, v2) Γ
  | LetIn e1 f1, LetIn e2 f2
    => is_related Γ e1 e2 /\ forall v1 v2, is_related ((v1, v2) :: Γ) (f1 v1) (f2 v2)
  | _, _ => False
  end.
\end{minted}
This unfortunately isn't quite linear in the size of the syntax tree, though it is significantly smaller.
One way to achieve truely linear%
\footnote{%
Actually, the size of the proof term will still be logarithmic in the size of the syntax tree, due to the way we represent list membership proofs.%
}
proofs is to pick a more optimized representation for list membership and to convert the proposition to be an eliminator.
This consists of replacing $A \wedge B$ with $\forall P, A \to B \to P$, and similar.
\begin{minted}{coq}
Fixpoint is_related_elim {var1 var2 : Type} (Γ : list (var1 * var2))
   (e1 : @expr var1) (e2 : @expr var2) : Prop :=
  match e1, e2 with
  | NatO, NatO => True
  | NatS e1, NatS e2 => is_related_elim Γ e1 e2
  | NatMul x1 y1, NatMul x2 y2 => forall P : Prop,
      (is_related_elim Γ x1 x2 -> is_related_elim Γ y1 y2 -> P) -> P
  | Var v1, Var v2 => forall (P : Prop),
      (forall n, List.nth_error Γ (N.to_nat n) = Some (v1, v2) -> P) -> P
  | LetIn e1 f1, LetIn e2 f2 => forall P : Prop,
      (is_related_elim Γ e1 e2
       -> (forall v1 v2, is_related_elim ((v1, v2) :: Γ) (f1 v1) (f2 v2))
       -> P)
      -> P
  | _, _ => False
  end.
\end{minted}
We can now prove that \mintinline{coq}{is_related_elim Γ e1 e2 → is_related Γ e1 e2}.

Note that making use of the fixpoint is significantly more inconvenient than making use of the inductive; the proof of \mintinline{coq}{check_is_even_expr_sound}, for example, proceeds most naturally by induction on the relatedness hypothesis.
We could instead induct on one of the ASTs and destruct the other one, but this becomes quite hairy when the ASTs are indexed over their types.

\todo{Should I benchmark this and insert graphs?}

\subsubsection{Via de Bruijn} \label{sec:wf:perf:de-bruijn}

An alternative, ultimately superior, method of constructing compact proofs of relatedness involves a translation to a de Bruijn representation.
We can define a boolean predicate on de Bruijn syntax representing well-formedness.
\begin{minted}{coq}
Fixpoint is_closed_under (max_idx : nat) (e : expr) : bool :=
  match expr with
  | NatO => true
  | NatS e => is_closed_under max_idx e
  | NatMul x y => is_closed_under max_idx x && is_closed_under max_idx y
  | Var n => n <? max_idx
  | LetIn v f => is_closed_under max_idx v && is_closed_under (S max_idx) f
  end.
Definition is_closed := is_closed_under 0.
\end{minted}
Note that this check generalizes quite nicely to expressions indexed over their types---so long as type codes have decidable equality---where we can pass around a list (or more efficient map structure) of types for each variable, and just check that the types are equal.

Now we can prove that whenever a de Bruijn \mintinline{coq}{expr} is closed, any two PHOAS \mintinline{coq}{expr}s created from that AST will be related in the empty context.
Therefore, if the PHOAS \mintinline{coq}{expr} we start off with is the result of converting some de Bruijn \mintinline{coq}{expr} to PHOAS, we can easily prove that it's well-formed simply by running \mintinline{coq}{vm_compute} on the \mintinline{coq}{is_closed} procedure.
How might we get such a de Bruijn \mintinline{coq}{expr}?
The easiest way is to write a converter from PHOAS to de Bruijn.

Hence we can prove the theorem \texttt{∀ e, is\_closed (PHOAS\_to\_deBruijn e) = true ∧ e = deBruijn\_to\_PHOAS (PHOAS\_to\_deBruijn e) → Wf e}.
The hypothesis of this theorem is quite easy to check; we simply run \mintinline{coq}{vm_compute} and then instantiate it with the proof term \mintinline{coq}{conj (eq_refl true) (eq_refl e)}, which is linear in the size of \mintinline{coq}{e}.

\section{Reification}

The one part of proof by reflection that we've neglected up to this point is reification.
\todo{How much should be covered in this chapter?  How much in reification-by-parametricity?}

\section{Related Work}
\todo{this section}
\todo{mention RTac, etc}

\section{Reflective Syntax Tranformation}
\todo{prep the reader for the upcoming chapter on the rewriter}
\todo{Maybe call this Reflective Syntax Transformation}
\todo{should the content on slowness in non-reflective rewriting go here?}

%\section{related work (incl. RTac)}
%\todo{talk about related work}
%\todo{figure out where this section belongs}

\begin{comment}
\todo{list:}
\begin{itemize}
\item Maybe expand on the trust story of reflective programming at the beginning
\item Expand section on PHOAS, treat HOAS, de Bruijn, named, etc
\item Talk about well-formedness, reflective well-formedness
\item Explain various ways of doing reification, explain where the trust comes from, what's proven and what's not
\item Performance measurements of the various ways of doing reification, forward reference reification by parametricity chapter
\item Related work section on reflective syntax transformation / uses of proof by reflection in general, brief intro to rewriting chapter
\end{itemize}
\end{comment}

\todo{should I talk about parsers at all?}

\begin{comment}
\begin{subappendices}

\section{Reflective Evenness Checking in Coq} \label{sec:evenness-in-coq}
\input{coq-fragments/evenness.raw.tex}
\begin{comment}
    \section{Transcript bits from Adam}
     Okay, and then there'll be another section that is on program transformation and rewriting and this will be the other main section and this is like, I this is another week link in the thesis but this is this well, maybe it's not a week like oh this is the one that's sort of the performance issue is motivated by the grinding criterion that you have this separation between what you need to trust and what you can freely optimize and gives you this tension that makes.

     Like program transformation and rewriting part. To efficiently reject scale. Okay. And so that'll present the rewriter there might be a different chapter that's like here the technical details if you're looking to reproduce. The proctor that's gonna look a useful split won't be that first chapter is more tightly connected to the VMware refrigerant and there's another part that goes into more detail right yeah and the second part is explicitly marked off as like like it's not it's only for people that want to know how it's implemented this is like the chapter that it's on probably and read in depth right and then after that I'm thinking they.


    \section{transcript bits from Rajee}

So the next section is on an sort of the other main thing that you one of the other main things maybe the other main thing that you want to do your structure because you want to do you're recording this right? I'm recording this yeah, but my dearly new structure because of structure that I have is.

Not very structurally, what do you mean? I mean, it's helpful you ask these questions and I'm like, oh, I'll probably need to explain this bit here that bet there okay, yeah. Like I'll probably try to get Google to transcribe this and then edit it a bit and turn it into an outline.

So thank you more words to say no. I get all the words you're saying you have so much stuff maybe um, so the other thing you might want to do in caulk is. A program transformation or rewriting so this is like sometimes you have one. Program and you want to turn it into another program typical example of this is your writing a compiler.

Or you have some things and you know that some other some things are equal and you want to like replace things with other equal things you want to do like equational reasoning.

Right? And all custom built-in tactics for this and they work on small examples and they don't scale. And they're sort of notorious for not scaling. And there are a number. Of ways in which they don't scale some of which are probably just artifacts of how they're designed some of which are issues with how things are set up.

Some of which are potentially fundamental issues to doing this sort of thing. And.

The. Solution that I will be proposing in my thesis. Is that it turns out that you can do all of this program transformation rewriting stuff by shoving it into the part shoving it all into the type level. Serving it all into the part of like this part of the system that's been heavily optimized for a particular type of computation.

So what is it in before you shove it into the type level, right? So I talked before about two different parts of the trusted good base with the kernel and this large bit around it. And previously there in this large bit around it. And so what this looks like is that you're like I have this equation.

Please take my thing, maybe it's an expression, maybe it's a program whatever and transform this bit to this other bet. And caucus like okay, I know that those two bits are the same. Let me generate a proof that you're like program with the first bit is equal to your program with that bit changed.

We had it cloud generate this. Ah, there's a standard way of generating it. Okay, it has one of the built-in tactics tactics or things that generate proofs. It knows how to generate this sort of proof. And that's fine when you're doing one step. But maybe you have a program that's a couple hundred lines long.

Or thousand lines long and maybe you wanted to like one transformation in each of the lines and so you're doing like a thousand transformations, which is like should be reasonable. I will as an aside the like main project that I've worked on uses needs code sharing and student program transformations with code sharing.

What does that mean? So you're like let x be a plus a and then let y be x plus x then let's z by plus y and you have a bunch of these and if you inline to all of them then your program blows up in size. And so we want to do maybe you had like an extra plus zero on all of these.

You want to get rid of the plus zeroes without inlining everything. And like caulk has ways of generating fruits like this. But their quadratic and how many are like each individual transformation is linear in the number of variables, you've allocated above it. Which makes it quadratic in the leg or like makes it a product of how many lines you have and how many transformations you're doing, right?

Which and the like quadratic factor is you're like okay, it's quadratic, but like how bad is the quadratic? And adding up each of them, what is it like one plus two and then the next one is three or is it the next one's four oh because you're doing everything that the second one did and it's it's like one plus two plus three plus okay, oh but like the quantity like if the scaling factor is like a nanosecond yeah and you're dealing with a couple thousand you're like couple thousand nanoseconds, whatever.

If the skeleton factor is a minute, you're like couple thousand minutes or like thousand squared minutes that's pretty terrible yeah and so the scaling factors work out so that like, 50 to 100 lines, you hit like about a minute. And the programs that we worked with went from about 90 lines up to about 900 lines see so if you hit a minute at like 90 lines, you're not gonna be able to handle 900.

So that's that's sort of what the section is about. Oh what we see outside of the specific example of like inlining this thing where you referring it's oh well, so you need to not inline it but you're doing like arbitrary code transformations, okay this program. And the tool that I have let's you do like there's somewhere restrictions on what kind of programs you can handle and it's a research prototype so there's a bunch of restrictions there mostly engineering work.

I think to lift. But it puts all of the work in the in the like fast part of caulk the running time is constant sorry the not to run it the proof size is constant in the number of steps. It's like linear and how many lines of code you have hmm right so before we had a quadratically sized proof because every step had its own proof that encoded the entire program right and so you chain all these together and you'd be sad.

It's unclear whether or not this is actually the quadratic bottleneck in. The existing parts of the system like it's a real hard to diagnose. Like what exactly the issue is for musings that I will hopefully mention in the initial section where I was like palliative things that are hard like another thing that makes it hard is that caulk is written as like a big mutual recursive block which is like you keep jumping around between functions and so looking at like where am I spending my time is real hard to pin down because you're like well all the functions and like even.

So even if you know what function you're spending time and it's not necessarily. Easy to tie to like user level things because maybe you're spending a lot of time type checking this quadratically sized proof and like making sure that things light up. Or maybe the. Like could transformation equational rewriting bit generated a proof term that used more power than it needed to like use the more type level computation and so you're spending a lot of time exploring bad roots.

Unclear.

Or maybe you're spending a lot of time in a completely different part of the stuff like there's some other parts of the system that are quadratic or cubic in like how many lines of yeah like how many variables you have for stupid reasons. That are nevertheless very hard to fix.

Ah.

And like so like here's another example that will probably go into the first section. Sometimes you want to introduce you want to like do things underneath a bunch of variables. And.

Every time you do a thing you need to create a new goal state. And you need to relate it to the old goal state. And this has cost that's something like linear in how many variables exist in the goal state. And. So then if you want to introduce a bunch of things if you do have one out of time, you'll be creating one new goal state for each variable and each one will have cost linear and how many variables were above it and now oops it's quadratic to introduce and variables and do something underneath them that's sad there might actually be another linear factor here.

I think it might end up being cubic. Yeah and so like being modular in a bunch of places like runs real hard against being fast.

Yeah. So anyway back to this this part of the system of doing program transformation, so I'm going to present the like new tool that I have. I probably will have a section that's like if you're looking to implement your own purposes, then that is like incorporating this here the details of all the things that went into like building this tool.

If your user that's looking to use this you don't the section is not for you. Another bit that will show up in this section. Is sorry I just remembered another bit for the previous section maybe I'll say that first the previous section the one about how to design your API is about conversion one thing that I will probably mention is a neat trick where you can convince caulk that two different theorems are actually the same and can have the same proof.

What do you mean convince call? Ah, if you. Design your theorem very carefully then you can make it so that caulk the like bit where you're like conquer these two things the same are the types the same. If you design things very carefully, you can get cock to say yes for two different theorems and then one cox is yes, you can say look.

I have the same proof for both them. So this is not true no it is true right so so this the sort of thing you do this with um, so in category theory you have objects and you have errors between them. And there's a common thing that you do in category theory where you're like if I flip all the arrows around.

Nothing changes. And so if you're very careful about how you set things up in caulk you can flip all the arrows arrows around and say cock did anything change cock will say like nope nothing changed.

Um, but if you're not careful then it doesn't work.

Uh, yeah, so that was an extra bit for the previous section extra bit and the second section the program transformation one. Is that I'm hung up in a probably not useful but you can change one of the errors or like it's set up in such a way like there's like one exception or something what do you mean like there's a break it like if there's like one function that you can this is mathematics there aren't exceptions there's like one function like okay, so you're taking something that has in verses all around.

It's not something that has inverses it's your. Your putting on it's like like when you put on upside-down classes. You're putting on arrow flippy glasses. How are you flipping the arrows okay, so here's a definition of what it means to be the empty set okay to be the empty set is to have exactly one function from you to any other set.

Here's a okay, so now let's flip all the iris. What does it mean if there's exactly one function to you from every other set? Now you're the one element so.

Any theorem that is a sufficiently abstract to be cast in terms of arbitrary categories. If it applies to the one element set there is another version of the theorem that applies to the empty set.

Okay. That sounds a little bit better but I have made full sense of it but that's on me go on it's a pretty cool trick yeah and it's more cool that I can convince talk to do all the work in a that. I don't have to do any of it so back to the program transformation section.

Sorry. I was just considering that I had told zoom to record separate audio for each of us for this call hearing to see if I'm sad about that. Why would you be sad about that if I want to transcribe it and it's missing things. I guess I can combine the audio files in audacity, it'll be fine.

Anyway, so the section about program transformation. One of the steps that you have to do is you want to take your meaning of the program and turn it into a syntax tree. What does that mean so the meaning of the program might be something like take these two numbers and add them and here's how addition is defined the syntax tree might be?

You have a function node called plus and it is applied to two arguments and one of them is the variable with this name and the other one is the variable with that name yeah isn't that what the whole thing is anyway what oh. So. Are you writing the first thing like are you writing the meaning of the program are you asking called to generate you you write the meaning and you ask our quote the syntax tree is?

How does clock generate that free you have to tell clock out a generator okay, um and the default the like obvious way of doing this is slow. For disturbingly similar reasons to how Python is slow. Because if you write it in the language of proof, script automation, you end up doing lots of things that you don't actually need to be doing.

Like you end up. Making lots of intermediate goals and you end up spending a lot of time checking that things are the same over and over and over again. It's just hard to avoid in this language. You could spend more time writing in a better language. Or. For most of these kinds of problems.

The part of the system that lets you do proof by induction. Can be repurposed to turn meanings into syntax trees. So one key component of doing induction is you're like caulk I want to do induction on this thing and here is the theorem that I'm trying to prove and cock needs to find all the instances of that thing in the theorem and be like in all of these places the induction variable shows up.

And this is kind of similar. You're like, please find all the places with addition and replace it with the addition node. Please find all the places with subtraction and replace it with the subtraction node. And so you can leverage this facility that was there for doing things like induction.

To do to give you abstract syntax trees out of programs. I see. It's pretty cute and it's also blazingly fast. Me.

Ah. So the tactic is called pattern. Okay, but the I've named the method of doing this reification by parametricity. And reification is the you turn a per you turn the meaning of a program into the abstract syntax tree. And parametricity is the. It's about how.

When you have very little information about things there's sort of only one thing you can do with them. I'm like, I have a function and it takes two types. A and B. And it takes in a thing of type A and it's the thing of type B. They returns the thing of type A.

And you're like well, obviously it's just returning first one. That's sort of the only thing I can do. And I'm like well.

If. You're in Haskell. Done or so if you're in Python then you can do something sneaky like say well if they're the same type then return the second one and otherwise return the first one but if you're in a nice language like caulk that's what's called parametric then you're not allowed to do that and really the only thing you can do is return the first one.

Why does it matter? Oh we're sort of taking advantage of the fact that what you do is it's allowed to depend on. These sorts of. Details that you're not supposed to be able to look at okay, oh that if you claim to do something for all types, you have to do it the same for all types, you're not allowed to do it differently for some types then for other ones, this is part of the.

This is part of the mathematics of caulk.

What's this what? Um, is it the way coffee setup versus to call custom code okay and you trust that it implements the mathematics and then you also trust or prove that you hope that the mathematics is correct? And people write papers about the mathematics, they don't write papers about the code.

You you ideally you should prove that they could match us the math right with some people working on that.

But yeah like like one of the bits of the the the trusted part is like, how do you check that two things are the same? And like in particular how do you do this quickly and like what how do you just if you're like checking that two things are the same and you want to know whether to do computation on the left or on the right, how do you decide which one to do first and like this is not a thing that shows up in the mathematics.

But it is a thing that shows up in the trusted code base. Yeah.

Okay, so I think to finish up what I was saying were. Relying on the fact that you don't get to do different things depending on different types to let us like swap all of the things out for different things. That you can notice are different.

\todo{this chapter}
\section{related work (incl. RTac)}

\section{reification variants (including reification by parametricity)}

\section{reflective well-formedness?}

\section{description of constructed tool (rewriter)}

\section{case study: fiat-crypto}

\section{case study: parsers?}
\end{comment}
\end{subappendices}
\end{comment}
