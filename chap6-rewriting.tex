\chapter{Engineering Challenges in the Rewriter}\label{ch:rewriting-more}
\setboolean{zerowidthscripts}{true}%

\begin{quote}
  premature optimization is the root of all evil
\end{quote}
\begin{flushright}
  --- Donald Knuth
\end{flushright}

\todo{Better chapter title?}
%\section{Introduction}\label{sec:rewriting-more:intro}
\autoref{ch:rewriting} discussed in detail our framework for building verified partial evaluators, going into the context, motivation, and the techniques used to put the framework together.
However, there was a great deal of engineering effort that went into building this tool which we glossed over.
Much of the engineering effort was mundane, and we elide the details entirely.
However, we believe some of the engineering effort serves as a good case-study for the difficulties of building proof-based systems at scale.
This chapter is about exposing the details relevant to understanding how the bottlenecks and principles identified elsewhere in this thesis played out in designing and implementing this tool.

%\section{A Brief Survey of the Engineering Challenges}\label{sec:rewriting-more:challenges-overview}

While the core rewriting engine of the framework is about 1\,300 lines of code, and early simplified versions of the core engine were only about 150 lines of code%
\footnote{%
See \href{https://web.archive.org/web/20200716002534/https://github.com/JasonGross/fiat-crypto/blob/3b3e926e4186caa1a4003c81c65dad0a1c04b43d/src/Experiments/RewriteRulesSimpleNat.v}{\texttt{https://github.com/JasonGross/fiat-crypto/blob/3b3e926e4186caa1a4003c81c65dad0a1c04b43d/src/Experiments/RewriteRulesSimpleNat.v}} for the file \texttt{src/Experiments/RewriteRulesSimpleNat.v} from \href{https://github.com/JasonGross/fiat-crypto/tree/experiments-small-rewrite-rule-compilation}{the branch \texttt{experiments-small-rewrite-rule-compilation} on \texttt{JasonGross/fiat-crypto}} on GitHub.%
}%
, the correctness proofs take nearly another 8\,000 lines of code!
% git ls-files "src/Rewriter/Rewriter/*.v" | grep -o 'src/Rewriter/Rewriter/[^/]*\.v' | xargs coqwc  | sort -h | less
% add up totals, subtract off the lines in Rewriter.v
As such, this tool, developed to solve performance scaling issues in verified syntax transformation, itself serves as a good case study of some of the pain that arises when scaling proof-based engineering projects.

Our discussion in this section is organized by the conceptual structure of the normalization and pattern matching compilation engine;
we hope that organizing the discussion in this way will make the examples more understandable, motivated, and incremental.
We note, however, that many of the challenges fall into the same broad categories that we've identified earlier in this thesis:
issues arising from the power and (mis)use of dependent types, as introduced in \fullref{sec:why-how-dependent-types};
and issues arising arising from API mismatches, as described in \fullref{ch:api-design}.

\subsection{Pre-Reduction}\label{sec:rewriting-more:pre-reduction}
The the two biggest underlying causes of engineering challenges are expression API mismatch, which we'll discuss in \fullref{sec:rewriting-more:AST:choices}, and our desire to reduce away known computations in the rewriting engine once and for all when compiling rewriting rules, rather than again and again every time we perform a rewrite.
In pratice, performing this early reduction nets us an approximately $2\times$ speed-up.

\subsubsection{What does this reduction consist of?}\label{sec:rewriting-more:pre-reduction:what-reduction}
Recall from \autoref{sec:pattern-matching-compilation-and-evaluation} that the core of our rewriting engine consists of three steps:
\begin{enumerate}
\item
  The first step is pattern-matching compilation: we must compile the lefthand sides of the rewrite rules to a decision tree that describes how and in what order to decompose the expression, as well as describing which rewrite rules to try at which steps of decomposition.
\item
  The second step is decision-tree evaluation, during which we decompose the expression as per the decision tree, selecting which rewrite rules to attempt.
\item
  The third and final step is to actually rewrite with the chosen rule.
\end{enumerate}
The first step is performed once and for all; it depends only on the rewrite rules, and not on the expression we are rewriting in.
The second and third steps do, in fact, depend on the expression being rewritten, and it is in these steps that we seek to eliminate needless work early.

The key insight, which allows us to perform this precompilation at all, is that the most of the decisions we seek to eliminate depend only on the \emph{head identifier} of any application.%
\footnote{%
  In order to make this simplification, we need to restrict the rewrite rules we support a little bit.
  In particular, we only support rewrite rules operating on $\eta$-long applications of concrete identifiers to arguments.
  This means that we cannot support identifiers with variable arrow structure (e.g., a variadic \mintinline{coq}{curry} function) nor do we support rewriting things like \mintinline{coq}{List.map f} to \mintinline{coq}{List.map g}---we only support rewriting \mintinline{coq}{List.map f xs} to \mintinline{coq}{List.map g ys}.%
}
We thus augment the $\text{reduce}(c)$ constant case of \autoref{fig:nbe} in \autoref{sec:thunk-eval-subst-term} by first $\eta$-expanding the identifier, before proceeding to $\eta$-expand the identifier application and perform rewriting with \text{rewrite-head} once we have an $\eta$-long form.

Now that we know what the reduction consists of, we can now discuss what goes in to making the reduction possible, and the engineering challenges that arise.

\subsubsection{CPS}\label{sec:rewriting-more:pre-reduction:cps}
Due to the pervasive use of Gallina \mintinline{coq}{match} statements on terms which are not known during this compilation phase, we need to write essentially all of the decision-tree-evaluation code in continuation-passing style.
This causes a moderate amount of pain, distributed over the entire rewriter.
We will come back to this in \autoref{sec:rewriting-more:pre-reduction-again:cps}, after we have introduced the constructions necessary to understand the code.

\subsubsection{Type Codes}\label{sec:rewriting-more:pre-reduction:type-codes}
The pattern-matching compilation algorithm of \textcite{Aehlig} does not deal with types.
In general, unification of types is somewhat more complicated than unification of terms, because terms are indexed over types.
We have two options, here:
\begin{enumerate}
\item
  We can treat terms and types as independent and untyped, simply collecting a map of unification variables to types, checking non-linear occurences (such as the types in \mintinline{coq}{@fst ?A ?B (@pair ?A ?B ?x ?y)}) for equality, and run a typechecking pass afterwards to reconstruct well-typedness.
  In this case, we would consider the rewriting to have failed if the replacement is not well-typed.
\item
  We can perform matching on types first, taking care to preserve typing information, and then perform matching on terms afterwards, taking care to preserve typing information.
\end{enumerate}

The obvious tradeoff between these options is that the former option requires doing more work at runtime, because we end up doing needless comparisons that we could know in advance will always turn out a particular way.
Importantly, note that Coq's reduction will not be able to reduce away these runtime comparisons; reduction alone is not enough to deduce that a boolean equality function defined by recursion will return true when passed identical arguments, if the arguments are not also concrete terms.
%
%\todo{mention a trade-off here: note that we can't eliminate equality tests/casts early if we introduce them in the wrong place, c.f.~\texttt{app\_transport\_with\_unification\_resultT'\_cps}}
%
%\todo{talk about the general tradeoff between runtime checks and static proofs} % ``However, the proofs are much simpler if we simply do a wholesale check at the very end

Following standard practice in dependently-typed languages, we chose the second option.
We now believe that this was a mistake, as it's fiendishly hard to deconstruct the expressions in a way that preserves enough typing information to completely avoid the need to compare type codes for equality and cast across proofs.
For example, to preserve typing information when matching for \mintinline{coq}{@fst ?A ?B (@pair ?A ?B ?x ?y)}, we would have to end up with the following \mintinline{coq}{match} statement.
Note that the reader is not expected to understand this statement, and the author was only able to construct it with some help from Coq's typechecker.
\begin{minted}{coq}
| App f v =>
 let f :=
  match f in expr t return option (ident t) with
  | Ident idc => Some idc
  | _ => None
  end in
 match f with
 | Some maybe_fst =>
   match v in expr s return ident (s -> _) -> _ with
   | App f y =>
     match f in expr _s
      return
       match _s with arrow b _ => expr b | _ => unit end
       -> match _s with arrow _ ab => ident (ab -> _) | _ => unit end
       -> _
     with
     | App f x =>
       let f :=
        match f in expr t return option (ident t) with
        | Ident idc => Some idc
        | _ => None
        end in
       match f with
       | Some maybe_pair =>
         match maybe_pair in ident t
          return
           match t with arrow a _ => expr a | _ => unit end
           -> match t with arrow a (arrow b _) => expr b | _ => unit end
           -> match t with arrow a (arrow b ab) => ident (ab -> _) | _ => unit end
           -> _
         with
         | @pair a b =>
           fun (x : expr a) (y : expr b) (maybe_fst : ident _) =>
            let is_fst := match maybe_fst with fst => true | _ => false end in
            if is_fst
            then … (* now we can finally do something with a, b, x, and y *)
            else …
         | _ => …
         end x
       | None => …
       end
     | _ => …
     end y
   | _ => …
   end maybe_fst
 | None => …
 end
\end{minted}
\begin{comment}
\begin{minted}{coq}
Require Import Coq.Program.Program.
Inductive type := arrow (a b : type) | prod (a b : type).
Delimit Scope etype_scope with etype.
Bind Scope etype_scope with type.
Infix "->" := arrow : etype_scope.
Infix "*" := prod : etype_scope.
Inductive ident : type -> Set :=
| fst {a b : type} : ident (a * b -> a)
| pair {a b : type} : ident (a -> b -> a * b)
.
Inductive expr {var : type -> Type} : type -> Type :=
| App {s d} (f : expr (s -> d)) (x : expr s) : expr d
| Ident {t} (idc : ident t) : expr t
.

Definition foo : forall var t (e : @expr var t),
    match e : @expr var _ in expr t return option (@expr var t) with
    | App f v
      => let f
             := (match f in expr t return option (ident t) with
                 | Ident idc => Some idc
                 | _ => None
                 end) in
         match f with
         | Some maybe_fst
           => match v in expr s return ident (s -> _) -> _ with
              | App f y
                => match f in expr _s
                         return match _s with arrow b _ => expr b | _ => unit end
                                -> match _s with arrow _ ab => ident (ab -> _) | _ => unit end
                                -> _
                   with
                   | App f x
                     => let f := (match f in expr t return option (ident t) with
                                  | Ident idc => Some idc
                                  | _ => None
                                  end) in
                        match f with
                        | Some maybe_pair
                          => match maybe_pair in ident t
                                   return
                                   match t with arrow a _ => expr a | _ => unit end
                                   -> match t with arrow a (arrow b _) => expr b | _ => unit end
                                   -> match t with arrow a (arrow b ab) => ident (ab -> _) | _ => unit end
                                   -> _
                             with
                             | @pair a b
                               => fun (x : expr a) (y : expr b) (maybe_fst : ident _)
                                  => let is_fst := match maybe_fst with fst => true | _ => false end in
                                     if is_fst
                                     then None
                                     else None
                             | _ => fun _ _ _ => None
                             end x
                        | None => fun _ _ => None
                        end
                   | _ => fun _ _ => None
                   end y
              | _ => fun _ => None
              end maybe_fst
         | None => None
         end
    | _ => None
    end
    = None.
repeat first [ progress cbv beta iota zeta
             | progress intros
             | reflexivity
             | match goal with
               | [ |- context[match ?e with _ => _ end] ] => is_var e; destruct e || dependent destruction e
               end ].
Defined.
\end{minted}
\end{comment}
This is quite the mouthful.

Furthermore, there are two additional complications.
First, this sort of match expression must be generated \emph{automatically}.
Since pattern-matching evaluation happens on \emph{lists} of expressions, we'd need to know exactly what each match reveals about the types of all other expressions in the list.
Additionally, in order to allow reduction to happen where it should, we need to make sure to match the head identifier \emph{first}, without convoying it across matches on unknown variables.
\todo{should I cite CPDT about the convoy pattern?}
Note that in the code above, we did not follow this requirement, as it would complicate the \mintinline{coq}{return} clauses even more (presuming we wanted to propagate typing information as we'd have to in the general case rather than cutting corners).

Second, trying to prove anything about functions written like this is an enormous pain.
Because of the intricate dependencies in typing information involved in the convoy pattern, Coq's \mintinline{coq}{destruct} tactic is useless.
The \mintinline{coq}{dependent destruction} tactic is sometimes able to handle such goals, but even when it can, it often introduces a dependency on the axiom \mintinline{coq}{JMeq_eq}, which is equivalent to assuming \emph{uniqueness of identity proofs} (UIP), that all proofs of equality are equal---note that this contradicts, for example, the popular univalence axiom of homotopy type theory~\cite{HoTTBook}.
In order to prove anything about such functions without assuming UIP, the proof effectively needs to replicate the complicated \mintinline{coq}{return} clauses of the function definition.
However, since they are not to be replicated exactly, but merely be generated from the same insights, such proof terms often have to be written almost entirely by hand.
These proofs are furthermore quite hard to maintain, as even small changes in the structure of the function often require intricate changes in the proof script.

Due to a lack of foresight and an unfortunate reluctance to take the design back to the drawing board after we already had working code, we ended up mixing these two approaches, getting, not quite the worst of both worlds, but definitely a significant fraction of the pain of both worlds:
We must deal with both the pain of indexing our term unification information over our type unification information, and we must still insert typecasts in places where we have lost the information that the types will line up.
% \todo{talk about the cost of inconsistent decisions spreading pain elsewhere} % ``Here we pay the price of an imperfect abstraction barrier (that we have types lying around, and we rely in some places on types lining up, but do not track everywhere that types line up).''

\subsubsection{How Do We Know What We Can Unfold?}\label{sec:rewriting-more:pre-reduction:tracking-unfolding}
Coq's built-in reduction is somewhat limited, especially when we want it to have reasonable performance.
This is, after all, a large part of the problem this tool is intended to solve.

In practice, we make use of three reduction passes; that we cannot interleave them is a limitation of the built-in reduction:
\begin{enumerate}
\item First, we unfold everything except for a specific list of constants; these constants are the ones that contain computations on information not fully known at pre-evaluation time.
\item Next, we unfold all instances of a particular set of constants; these constants are the ones that we make sure to only use when we know that inlining them won't incur extra overhead.
\item Finally, we use \mintinline{coq}{cbn} to simplify a small set of constants in only the locations that these constants are applied to constructors.
\end{enumerate}

Ideally, we'd either be able to do the entire simplification in the third step, or we'd be able to avoid the third step entirely.
Unfortunately, Coq's reduction is not fast enough to do the former, and the latter requires a significant amount of effort.
In particular, the strategy that we'd need to follow is to have two versions of every function which sometimes computes on known data and sometimes computes on unknown data, and we'd need to track in all locations which data is known and which data is unknown.

We already track known and unknown data to some extent (see, for example, the \mintinline{coq}{known} argument to the \mintinline{coq}{rIdent} constructor discussed below).
Additionally, we have two versions of a couple of functions, such as the bind function of the option monad, where we decide which to use based on, e.g., whether or not the option value that we're binding will definitely be known at pre-reduction time.

Note that tracking this sort of information is non-trivial, as there's no help from the typechecker.

We'll come back to this in \autoref{sec:rewriting-more:pre-reduction-again:tracking-unfolding}.

\subsection{NbE vs.~Pattern Matching Compilation: Mismatched Expression APIs and Leaky Abstraction Barriers}\label{sec:rewriting-more:AST:choices}
\todo{should I be consistent about naming syntax that's well-typed by construction?  I call it ``intrinsically-typed syntax'', ``syntax that is well-typed by construction'', ``intrinsically-well-typed syntax'', and ``type-indexed sytnax''\ldots}
We introduced normalization by evaluation (NbE)~\cite{NbE} in \autoref{sec:our-solution} and expanded on it in \autoref{sec:thunk-eval-subst-term} as a way to support higher-order reduction of $\lambda$-terms.
The termination argument for NbE proceeds by recursion on the type of the term we're reducing.
In particular, the most natural way to define these functions in a proof assistant is to proceed by structural recursion on the type of the term being reduced.
This feature suggests that using intrinsically-typed syntax is more natural for NbE, and we saw in \autoref{sec:binders:de-bruijn} that denotation functions are also simpler on syntax that is well-typed by construction.

However, the pattern-matching compilation algorithm of \textcite{maranget2008compiling} inherently operates on untyped syntax.
We thus have four options:
\begin{enumerate}[(1)]
\item
  use intrinsically-well-typed syntax everywhere, paying the cost in the pattern-matching compilation and evaluation algorithm;
\item
  use untyped syntax in both NbE and rewriting, paying the associated costs in NbE, denotation, and in our proofs;
\item
  use intrinsically-well-typed syntax in most passes, and untyped syntax for pattern matching compilation;
\item
  invent a pattern-matching compilation algorithm that is well-suited to type-indexed sytnax.
\end{enumerate}
We ultimately chose option (3).
I was not clever enough to follow through on option (4), and while options (1) and (2) are both interesting, option (3) seemed to follow the well-established convention of using whichever datatype is best-suited to the task at hand.
As we'll shortly see, all of these options come with significant costs, and (3) is not as obviously a good choice as it might seem at first glance.

\subsubsection{Pattern-Matching Evaluation on Type-Indexed Terms}\label{sec:rewriting-more:AST:type-indexed-pattern-matching}
While the cost of performing pattern-matching compilation on type-indexed terms is noticable, it's relatively insignificant compared to the cost of evaluating decisions trees directly on type-indexed terms.
In particular, pattern-matching compilation effectively throws away the type information whenever it encounters it; whether we do this early or late does not matter much, and we only perform this compilation once for any given set of rewrite rules.

By contrast, evaluation of the decision tree needs to produce \emph{term ASTs} that are used in rewriting, and hence we need to preserve type information in the input.
Recall from \autoref{sec:pattern-matching-compilation-and-evaluation} that decision-tree evaluation operates on lists of terms.
Here already we hit our first snag: if we want to operate on well-typed terms, we must index our lists over a list of types.
This is not so bad, but recall also from \autoref{sec:pattern-matching-compilation-and-evaluation} that decision trees contain four constructors:
\begin{itemize}
  \item \texttt{TryLeaf k onfailure}: Try the $k^\text{th}$ rewrite rule; if it fails, keep going with \texttt{onfailure}.
  \item \texttt{Failure}: Abort; nothing left to try.
  \item \texttt{Switch icases app\_case default}:
    With the first element of the vector, match on its kind; if it is an identifier matching something in \texttt{icases}, which is a list of pairs of identifiers and decision trees, remove the first element of the vector and run that decision tree; if it is an application and \texttt{app\_case} is not \texttt{None}, try the \texttt{app\_case} decision tree, replacing the first element of each vector with the two elements of the function and the argument it is applied to; otherwise, do not modify the vectors and use the \texttt{default} decision tree.
  \item \texttt{Swap i cont}: Swap the first element of the vector with the $i^\texttt{th}$ element (0-indexed) and keep going with \texttt{cont}.
\end{itemize}
The first two constructors are not very interesting, as far as overhead goes, but the third and fourth constructors are quite painful.

Note that the type of \mintinline{coq}{eval_decision_tree} would be something like \mintinline{coq}{∀ {T : Type} (d : decision_tree) (ts : list type) (es : exprlist ts) (K : exprlist ts → option T), option T}.


We cover the \mintinline{coq}{Swap} case first, because it is simpler.
To perform a \mintinline{coq}{Swap}, we must exchange two elements of the type-indexed list.
Hence we need both two swap the elements of the list of types, and then to have a separate, dependently-typed swap function for the vector of expressions.
Moreover, since we need to undo the swapping inside the continutation \todo{does this need more explanation or code?}, we must have an \emph{separate} unswap function on expression vectors which goes from a swapped type list to the original one.
We could instead elide the swap node, but then we could no longer use matching, \mintinline{coq}{hd}, and \mintinline{coq}{tl} to operate on the expressions, and would instead need special operations to do surgery in the middle of the list, in a way that preserves type-indexing.

To perform a \mintinline{coq}{Switch}, we must break apart the first element of our type-indexed list, determining whether it is an application, and identifier, or other.
Note that even with dependent types, we cannot avoid needing a failure case for when the type-indexed list is empty, even though such a case should never occur.
This mismatch---the need to include failure cases that one might expect to be eliminated by dependent typing information---is a sign that the amount of dependency in the types is wrong.
It may be too little, whence the developer should see if there is a way to incorporate the lack-of-error into the typing information (which in this case would require indexing the type of the decision tree over the length of the vector).
It may alternatively be to much dependent typing, and the developer might be well-served by removing more dependency from the types and letting more things fall into the error case.

After breaking apart the first element, we must convoy \todo{should I cite CPDT or somewhere in this thesis for the convoy pattern?} the continuation across the \mintinline{coq}{match} statement so that we can pass an expression vector of the correct type to the continuation \mintinline{coq}{K}.
In code, this branch might look something like
\begin{minted}{coq}
…
| Switch icases app_case default
  => match es in exprlist ts
       return (exprlist ts → option T) → option T
     with
     | [] => λ _, None
     | e :: es
       => match e in expr t
            return (exprlist (t :: ts) → option T) → option T
          with
          | App s d f x => λ K,
              let K' : exprlist ((s → d) :: s :: ts)
                 (* new continuation to pass on recursively *)
                := λ es', K (App (hd es') (hd (tl es')) :: tl (tl es')) in
              … (* do something with app_case *)
          | Ident t idc => λ K,
              let K' : exprlist ts
                 (* new continuation to pass on recursively *)
                := λ es', K (Ident idc :: es') in
              … (* do something with icases *)
          | _ => λ K, … (* do something with default *)
          end
     end K
…
\end{minted}
Note that \mintinline{coq}{hd} and \mintinline{coq}{tl} \emph{must} be type-indexed, and we \emph{cannot} simply match on \mintinline{coq}{es'} in the \mintinline{coq}{App} case;
there is no way to preserve the connection between the types of the first two elements of \mintinline{coq}{es'} inside such a \mintinline{coq}{match} statement.

This may not look too bad, but it gets worse.
Since the \mintinline{coq}{match} on \mintinline{coq}{e} will not be known until we are actually doing the rewriting on a concrete expression, and the continuation is convoyed across this \mintinline{coq}{match}, there is no way to evaluate the continuation during compilation of rewrite rules.
If we don't want to evaluate the continutation early, we'd have to be very careful not to duplicate it across all of the decision tree evaluation cases, as we might otherwise incur a super-linear runtime factor in the number of rewrite rules.
As noted in \autoref{sec:rewriting-more:pre-reduction}, our early reduction nets us a $2\times$ speedup in runtime of rewriting, and is therefore relatively important to be able to do.
\todo{redo performance experiments here, maybe insert a plot}

Here we see something interesting, which does not appear to be as much of a concern in other programming languages:
the representation of our data forces our hand about how much efficiency can be gained from precomputation, even when the representation choices are relatively minor.

\subsubsection{Untyped Syntax in NbE}\label{sec:rewriting-more:AST:untyped-nbe}
There is no good way around the fact that NbE requires typing information to argue termination.
Since NbE will be called on subterms of the overall term, even if we use syntax that is not guaranteed to be type-correct, we must still store the type information in the nodes of the AST.

Furthermore, as we say in \fullref{sec:binders:de-bruijn}, converting from untyped syntax to intrinsically-typed syntax, as well as writing a denotation function, requires either that all types be non-empty, or that we carry around a proof of well-typedness to use during recursion.
\todoask{Is there a good reference for these sorts of issues?  Are they well-known?  Well-studied?}
As discussed in \autoref{ch:design} and specifically in \fullref{sec:when-how-dependent-types}, needing to mix proofs with programs is often a big warning flag, unless the mixing can be hidden behind a well-designed API.
However, if we are going to be hiding the syntax behind an API of being well-typed, it seems like we might as well just use intrinsically well-typed syntax, which natrually inhabits that API.
Furthermore, unlike in many cases where the API is best treated as opaque everywhere, here the API mixing proofs and programs needs to have adequate behavior under reduction, and ought to have good behavior even under partial reduction.
This severely complicates the task of building a good abstraction barrier, as we not only need to ensure that the abstraction barrier does not need to be broken in the course of term-building and typechecking, but we must also ensure that the abstraction barrier can be broken in a principled way via reduction without introducing significant overhead.

%Despite all of these problems, in retrospect, it seems that this option might in fact be the least-costly option to choose.
%Admittedly, we have not actually implemented it, so there might remain hidden engineering challenges.

\subsubsection{Mixing Typed and Untyped Syntax}\label{sec:rewriting-more:AST:both}
The third option is to use whichever datatype is most naturally suited for each pass, and to convert between them as necessary.
This is the option that we ultimately chose, and the one, we believe, that would be most natural to choose to engineers and developers coming from non-dependently-typed languages.

There are a number of considerations that arose when fleshing out this design, and a number of engineering-pain-points that we encountered.
The theme to all of these, as in \autoref{ch:api-design}, is that imperfectly opaque abstraction barriers cause headaches in a non-local manner.

We got lucky, in some sense, that the rewriting pass \emph{always} has a well-typed default option: do no rewriting.
Hence we do not need to worry about carrying around proofs of well-typedness, and this avoids some of the biggest issues described in \nameref{sec:rewriting-more:AST:untyped-nbe}.

The biggest constraint driving our design decisions is that we need conversion between the two representations to be $\mathcal{O}(1)$; if we need to walk the entire syntax tree to convert between typed and untyped representations at every rewriting location, we'll incur quadratic overhead in the size of the term being rewritten.
We can actually relax this constraint a little bit: by designing the untyped representation to be completely evaluated away during the compilation of rewrite rules, we can allow conversion from the untyped syntax to the typed syntax to walk any part of the term that already needed to be revealed for rewriting, giving us amortized constant time rather than truely constant time.
\todo{is this last sentence understandable?}
As such, we need to be able to embed well-typed syntax directly into the non-type-indexed representation at cost $\mathcal{O}(1)$.

As the entire purpose of the untyped syntax is to (a) allow us to perform matching on the AST to determine which rewrite rule to use, and furthermore (b) allow us to reuse the decomposition work so as to avoid needing to decompose the term multiple times, we need an inductive type which can embed PHOAS expressions, and has separate nodes for the structure that we need, namely application and identifiers:

\begin{minted}{coq}
Inductive rawexpr : Type :=
| rIdent (known : bool) {t} (idc : ident t) {t'} (alt : expr t')
| rApp (f x : rawexpr) {t} (alt : expr t)
| rExpr {t} (e : expr t)
| rValue {t} (e : NbEₜ t).
\end{minted}
\label{sec:rewriting-more:rawexpr-def}%
There are three perhaps-unexpected things to note about this inductive type, which we will discuss in later subsections:
\begin{enumerate}
\item
  The constructor \mintinline{coq}{rValue} holds an NbE-value of the type \mintinline{coq}{NbEₜ} introduced in \autoref{sec:thunk-eval-subst-term}.
  We will discuss this in \fullref{sec:rewriting-more:delayed-rewriting}.
\item
  The constructors \mintinline{coq}{rIdent} and \mintinline{coq}{rExpr} hold ``alternate'' PHOAS expressions.
  We will discuss this in \fullref{sec:rewriting-more:revealing-enough-structure}.
\item
  The constructor \mintinline{coq}{rIdent} has an extra boolean \mintinline{coq}{known}.
  We will discuss this in \fullref{sec:rewriting-more:rIdent-known}.
\end{enumerate}

With this inductive type in hand, it's easy to see how \mintinline{coq}{rExpr} allows us $\mathcal{O}(1)$ embedding of intrinsically typed \mintinline{coq}{expr}s into untyped \mintinline{coq}{rawexpr}s.

While it's likely that sufficiently good abstraction barriers around this datatype would allow us to use it with relatively little pain, we did not succeed in designing good enough abstraction barriers.
The bright side of this failure is that we now have a number of examples for this thesis of ways in which inadequate abstraction barriers cause pain.

We will discuss the many issues that arise from leaks in this abstraction barrier in the upcoming subsections.
%While some of these issues will be discussed in \Autoref{sec:rewriting-more:delayed-rewriting,sec:rewriting-more:revealing-enough-structure,sec:rewriting-more:rIdent-known} where we discuss the perhaps-unexpected additions to the \mintinline{coq}{rawexpr} constructors, we can discuss a few complications here.

%\paragraph{Equivalence Relations}
%Correctness conditions on PHOAS transformations are frequently stated in terms of two (partial) equivalence relations: the well-formedness relation \mintinline{coq}{Wf} defined in \autoref{sec:PHOAS:Wf-def}; and pointwise equality---that is, equality up to function extensionality---of the interpretations of the expressions.
%\todo{make sure I'm being consistent about ``interpretation'' vs ``denotation'' across the thesis}

\subsubsection{Pattern Matching Compilation Made For Intrinsically-Typed Syntax}\label{sec:rewriting-more:AST:better-pattern-matching}
The cost of this fourth option is the cleverness required to come up with a version of the pattern matching compilation which, rather than being hindered by types in its syntax, instead puts them to good use.
Lacking this cleverness, we were unable to pay the requisite cost, and hence have not much to say in this section.

\subsection{Patterns with Type Variables -- The Three Kinds of Identifiers}\label{sec:rewriting-more:three-identifier-inductives}
We have one final bit of infrastructure to explain and motivate before we have enough of the structure sketched out to give all of the rest of the engineering challenges: representing the identifiers.
Recall from \fullref{sec:nine-steps} that we automatically emit an inductive type describing all available primitive functions.

When deciding how to represent identifiers, there are roughly three options we have to choose from:
\begin{enumerate}
\item
  We could use an untyped representation of identifiers, such as Coq strings (as in \textcite{TemplateCoq}, for example), or integers indexing into some finite map.
\item
  We could index the expression type over a finite map of valid identifiers, and use dependent typing to ensure that we only have well-typed identifiers.
\item
  We could have a fixed set of valid identifiers, using types to ensure that we have only valid expressions.
\end{enumerate}

The first option results in expressions that are not always well-typed.
As discussed in \autoref{ch:design} and seen in the preceeding sections, having leaky abstraction barriers is often worse than having none at all, and we expect that having partially-well-typed expressions would be no exception.

The second option is probably the way to go if we want truly extensible identifier-sets.
There are two issues.
First, this adds a linear overhead in the number of identifiers---or more precisely, in the total size of the types of the identifiers---because every AST node will store a copy of the entire finite map.
Second, because our expression syntax is simply typed, polymorphic identifiers pose a problem.
To support identifiers like \mintinline{coq}{fst} and \mintinline{coq}{snd}, which have types \mintinline{coq}{∀ A B, A * B → A} and \mintinline{coq}{∀ A B, A * B → B} respectively, we must either replicate the identifiers with all of the ways they might be applied, or else we must add support in our language for dependent types or for explicit type polymorphism.

Instead, we chose to go with the third option, which we believe is the simplest.
The inductive type of identifiers is indexed over the type of the identifier, and type polymorphism is expressed via meta-level arguments to the constructor.
So, for example, the identifier code for \mintinline{coq}{fst} takes two type-code arguments \mintinline{coq}{A} and \mintinline{coq}{B}, and has type \mintinline{coq}{ident (A * B → A)}.
Hence all fully-applied identifier codes have simple types (such as \mintinline{coq}{A * B → A}), and our inductive type still supports polymorphic constants.
An additional benefit of this approach is that unification of identifiers is just pattern matching in Gallina, and hence we can rely on the pattern-matching compilation schemes of Coq's fast reduction machines, or the OCaml compiler itself, to further speed up our rewriting.

\paragraph{Aside: Why Use Pattern Matching Compilation At All?}
Given the fact that, after pre-reduction, there is no trace of the decision tree remaining, one might ask why we use pattern matching compilation at all, rather than just leaving it to the pattern-matching compiler of Coq or OCaml to be performant.
We have three answers to this question.

The first, perhaps most honest answer, is that it is a historical accident; we prematurely optimized this part of the rewriting engine when writing it.

The second answer is that it allows us to easily prune useless work.
The pattern matching compilation algorithm naturally prunes away patterns that can be known to not work, given the structure that we've revealed.
By contrast, if we just record what information we've already revealed as we're performing pattern unification, it's quite tricky to avoid decomposition which can be known to be useless based on only the structure that's been revealed already.
\todo{Does this need more explanation?  A code example?  Something else?}

\todo{be consistent about ``pattern matching compilation'' vs ``pattern-matching compilation''?}
The third and final answer is that pattern matching compilation is a good abstraction barrier for factoring out the work of revealing enough structure from the work of unifying a pattern with an expression.
Said another way, even though we reduce away the decision tree and its evaluation, there is basically no wasted work; removing pattern matching compilation while preserving all the benefits would effectively just be inlining all of the functions, and there would be no dead code revealed by this inlining.
\todo{does this need more or better explanation?  or a code example?}

\paragraph{Pattern Matching For Rewriting}
We now arrive at the question of how to do pattern matching for rewriting with identifiers.
We want to be able to support type variables, for example to rewrite \mintinline{coq}{@fst ?A ?B (@pair ?A ?B ?x ?y)} to \mintinline{coq}{x}.
While it would arguably be more elegant to treat term and type variables identically, doing this would require a language supporting dependent types, and we are not aware of any extension of PHOAS to dependent types.
Extensions of HOAS to dependent types are known~\cite{Outrageous2010McBride}, but the obvious modifications of such syntax that in the simply-typed case turn HOAS into PHOAS result in infinite self-referential types in the dependently-typed case.

As such, insofar as we are using intrinsically well-typed syntax at all, we need to treat type variables separately from term variables.
We need three different sorts of identifiers:
\begin{itemize}
\item
  identifiers whose types contain no type variables, for use in external-facing expressions and the denotation function,
\item
  identifiers whose types are permitted to contain type variables, for use in patterns, and
\item
  identifers with no type information, for use in pattern-matching compilation.
\end{itemize}
The first two are relatively self-explanatory.
The third of these is required because pattern-matching compilation proceeds in an untyped way; there's no obvious place to keep the typing information associated to identifiers in the decision tree, which must be computed before we do any unification, type variables or otherwise.
\todo{is this sufficiently understandable?}

We could, in theory, use a single inductive type of type codes for all three of these.
We could parameterize the inductive of type codes over the set of free type variables (or even just over a boolean declaring whether or not type variables are allowed), and conventionally use the type code for unit in all type-code arguments when building decision trees.

This sort of reuse, however, is likely to introduce more problems than it solves.

The identifier codes used in pattern-matching compilation must be untyped, to match the decision we made for expressions in \autoref{sec:rewriting-more:AST:choices}.
Having them conventionally be typed pattern codes instantiated with unit types is, in some sense, just more opportunity to mess up and try to inspect the types when we really shouldn't.
There is a clear abstraction barrier here, of having these identifier codes not carry types, and we might as well take advantage of that and codify the abstraction barrier in our code.

The question of type variables is more nuanced.
If we are only tracking whether or not a type is allowed to have type variables, then we might as well use two different inductive types; there is not much benefit to indexing the type codes over a boolean rather than having two copies of the inductive, for there's not much that can be done generically in whether or not type variables are allowed.
Note also that we must track at least this much information, for identifiers in expressions passed to the denotation function must not have uninstantiated type variables, and identifiers in patterns must be permitted to have uninstantiated type variables.

However, there is some potential benefit to indexing over the set of uninstantiated type variables.
This might allow us to write type signatures for functions that guarantee some invariants, possibly allowing for easier proofs.
However, it's not clear to us where this would actually be useful; most functions already care only about whether or not we permit type variables at all.
Our current code in fact performs a poor approximation of this strategy in some places: we index over the entire pattern where indexing over the free variables of the pattern would suffice.

This unneeded indexing causes an enormous amount of pain, and is yet another example of how poorly designed abstraction barriers incur outsized overhead.
Rewrite rule replacements are expressed as dependently-typed towers indexed first over the type variables of a pattern, and then again over the term variables.
This design is a historical artifact, from when we expected to be writing rewrite rule ASTs by hand rather than reifying them from Gallina, and found the curried towers more convenient to write.
\todo{should I elaborate more on the pain?}
%``\ldots\space both conceptually simple but in practice complicated by dependent types.
%We must unify a pattern with an expression, gathering binding data for the replacement rule as we go; and we must apply the replacement rule to the binding data (which is non-trivial because the rewrite rules are expressed as curried dependently-typed towers indexed over the rewrite rule pattern).
%In order to state the correctness conditions for gathering binding data, we must first talk about applying replacement rules to binding data.''
%``We can define a transformation that takes in a \texttt{PositiveMap.t} of pattern type variables to types, together with a \texttt{PositiveSet.t} of type variables that we care about, and re-creates a new \texttt{PositiveMap.t} in accordance with the \texttt{PositiveSet.t}.
%This is required to get some theorem types to line up, and is possibly an indication of a leaky abstraction barrier.''
This design, however, is absolutely a mistake, especially given the concession we make in \fullref{sec:rewriting-more:pre-reduction:type-codes} to not track enough typing information to avoid all typechecking.

While indexing over only the set of permitted type variables would simplify proofs significantly, we'd benefit even more by indexing only over whether or not we permit type variables at all.
None of our proofs are made simpler by tracking the set of permitted type variables.
%
%\todo{talk about ``There are two steps to rewriting with a rule \ldots''}
%
%\todo{talk about how lifting causes pain due to dependent types, also linear traversal of term}
%
%\todo{talk about how we can unify types at all (c.f.~\texttt{preunify\_types})}

\subsection{Pre-evaluation}\label{sec:rewriting-more:pre-reduction-again}
$\left.\right.$
\subsubsection{CPS}\label{sec:rewriting-more:pre-reduction-again:cps}
\todo{insert code examples}
\subsubsection{How Do We Know What We Can Unfold?}\label{sec:rewriting-more:pre-reduction-again:tracking-unfolding}
\begin{itemize}
\item \todo{talk about the ``known'' parameter of rIdent} \label{sec:rewriting-more:rIdent-known}
\item \todo{talk about ``Note that here we are jumping through some extra hoops to get the right reduction behavior at rewrite-rule-compilation time.'' for \texttt{eval\_decision\_tree}}
%\item \todo{foward-reference pain with casts?}
\item \todo{talk about whether or not to eliminate PositiveMap.t?} % ``In a possibly-gratuitous use of dependent typing to ensure that''
\end{itemize}
\subsubsection{Revealing ``Enough'' Structure}\label{sec:rewriting-more:pre-reduction-again:revealing-enough-structure}
\begin{itemize}
\item \todo{talk also about tracking both the revealed and unrevealed structure}
\item \todo{talk about $\eta$-expanding identifier matches, reference \autoref{sec:rewriting-more:pre-reduction}}
\item \todo{talk about rawexpr\_types\_ok}
\end{itemize}

\subsection{The Let-In Monad: Missing Abstraction Barriers at the Type Level}\label{sec:rewriting-more:let-in-monad}
\todo{Talk also about the pain of wf statements for, e.g., \texttt{wf\_normalize\_deep\_rewrite\_rule}, having to go underneath multiple monads}

\subsection{Rewriting Again in the Output of a Rewrite Rule}\label{sec:rewriting-more:do-again}
$\left.\right.$

\subsection{Delayed Rewriting in Variable Nodes}\label{sec:rewriting-more:delayed-rewriting}
\todo{rValue vs rExpr}

\subsubsection{Relating Expressions and Values}\label{sec:rewriting-more:values-and-expressions}
\todo{figure out where this goes, how to explain it, where it arises from:}
``In general, the stored values are only interp-related to the same things that the ``unrevealed structure'' expressions are interp-related to. There is no other relation (that we've found) between the values and the expressions, and this caused a great deal of pain when trying to specify the interpretation correctness properties.''

\subsubsection{Which Equivalence Relation?}\label{sec:rewriting-more:which-equivalence}
\todo{talk about \texttt{rawexpr\_equiv}, 4-place \texttt{wf\_rawexpr}, nuance of revealing structure in CPS'd \texttt{eval\_decision\_tree}, non-obvious \texttt{wf\_value} binding list, \texttt{interp\_related\_gen} unsuccessfully avoiding funext (and also needing to be instantiated with \texttt{value\_interp\_related} sometimes), \texttt{rawexpr\_interp\_related} and separating (or not) goodness from relatedness, \texttt{rawexpr\_types\_ok}?, \texttt{unification\_resultT'\_interp\_related}?, \texttt{interp\_unify\_pattern'}?, interpretation-correctness related to rewriting again}

\subsection{What's the Ground Truth: Patterns Or Expressions?}\label{sec:rewriting-more:patterns-vs-expressions}
\todo{default interpretation of a pattern --- complicated, but perhaps needed for phrasing correctness of unification}

\clearpage

\todo{this chapter}
\todo{mention frowned-upon Perl scripts previously in BoringSSL(?) OpenSSL?; (ask Andres for reference?)} Perl scripts were complicated, a number of steps removed from actual running code, hard to maintain and verify.
\todo{Refer back to representation changes (good abstraction barriers / equivalences) being important in fiat-crypto, and being cheap only because we have a rewriter}
\setlistdepth{20}
\renewlist{itemize}{itemize}{20}%
\setlist[itemize,1]{label=\textbullet}%
\setlist[itemize,2]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,3]{label=\textasteriskcentered}%
\setlist[itemize,4]{label=\textperiodcentered}%
\setlist[itemize,5]{label=\textbullet}%
\setlist[itemize,6]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,7]{label=\textasteriskcentered}%
\setlist[itemize,8]{label=\textperiodcentered}%
\setlist[itemize,9]{label=\textbullet}%
\setlist[itemize,10]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,11]{label=\textasteriskcentered}%
\setlist[itemize,12]{label=\textperiodcentered}%
\setlist[itemize,13]{label=\textbullet}%
\setlist[itemize,14]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,15]{label=\textasteriskcentered}%
\setlist[itemize,16]{label=\textperiodcentered}%
\setlist[itemize,17]{label=\textbullet}%
\setlist[itemize,18]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,19]{label=\textasteriskcentered}%
\setlist[itemize,20]{label=\textperiodcentered}%

\input{rewriting/rewriting.md.tex}

\setboolean{zerowidthscripts}{false}%
