\chapter{Engineering Challenges in the Rewriter} \label{ch:rewriting-more}

\begin{quote}
  premature optimization is the root of all evil
\end{quote}
\begin{flushright}
  --- Donald Knuth
\end{flushright}

\todo{Better chapter title?}
\section{Introduction} \label{sec:rewriting-more:intro}
\autoref{ch:rewriting} discussed in detail our framework for building verified partial evaluators, going into the context, motivation, and the techniques used to put the framework together.
However, there was a great deal of engineering effort that went into building this tool which we glossed over.
Much of the engineering effort was mundane, and we elide the details entirely.
However, we believe some of the engineering effort serves as a good case-study for the difficulties of building proof-based systems at scale.
This chapter is about exposing the details relevant to understanding how the bottlenecks and principles identified elsewhere in this thesis played out in designing and implementing this tool.

\section{A Brief Survey of the Engineering Challenges} \label{sec:rewriting-more:challenges-overview}

While the core rewriting engine of the framework is about 1\,300 lines of code, and early simplified versions of the core engine were only about 150 lines of code%
\footnote{%
See \href{https://web.archive.org/web/20200716002534/https://github.com/JasonGross/fiat-crypto/blob/3b3e926e4186caa1a4003c81c65dad0a1c04b43d/src/Experiments/RewriteRulesSimpleNat.v}{\texttt{https://github.com/JasonGross/fiat-crypto/blob/3b3e926e4186caa1a4003c81c65dad0a1c04b43d/src/Experiments/RewriteRulesSimpleNat.v}} for the file \texttt{src/Experiments/RewriteRulesSimpleNat.v} from \href{https://github.com/JasonGross/fiat-crypto/tree/experiments-small-rewrite-rule-compilation}{the branch \texttt{experiments-small-rewrite-rule-compilation} on \texttt{JasonGross/fiat-crypto}} on GitHub.%
}%
, the correctness proofs take nearly another 8\,000 lines of code!
% git ls-files "src/Rewriter/Rewriter/*.v" | grep -o 'src/Rewriter/Rewriter/[^/]*\.v' | xargs coqwc  | sort -h | less
% add up totals, subtract off the lines in Rewriter.v
As such, this tool, developed to solve performance scaling issues in verified syntax transformation, itself serves as a good case study of some of the pain that arises when scaling proof-based engineering projects.

Our discussion in this section is organized by the conceptual structure of the normalization and pattern matching compilation engine;
we hope that organizing the discussion in this way will make the examples more understandable, motivated, and incremental.
We note, however, that many of the challenges fall into the same broad categories that we've identified earlier in this thesis:
issues arising from the power and (mis)use of dependent types, as introduced in \fullref{sec:why-how-dependent-types};
and issues arising arising from API mismatches, as described in \fullref{ch:api-design}.

\subsection{Pre-Reduction} \label{sec:rewriting-more:pre-reduction}

\subsection{NbE vs.~Pattern Matching Compilation: Mismatched Expression APIs}
\todo{should I be consistent about naming syntax that's well-typed by construction?  I call it ``intrinsically-typed syntax'', ``syntax that is well-typed by construction'', ``intrinsically-well-typed syntax'', and ``type-indexed sytnax''\ldots}
We introduced normalization by evaluation (NbE)~\cite{NbE} in \autoref{sec:our-solution} and expanded on it in \autoref{sec:thunk-eval-subst-term} as a way to support higher-order reduction of $\lambda$-terms.
The termination argument for NbE proceeds by recursion on the type of the term we're reducing.
In particular, the most natural way to define these functions in a proof assistant is to proceed by structural recursion on the type of the term being reduced.
This feature suggests that using intrinsically-typed syntax is more natural for NbE, and we saw in \autoref{sec:binders:de-bruijn} that denotation functions are also simpler on syntax that is well-typed by construction.

However, the pattern-matching compilation algorithm of \textcite{maranget2008compiling} inherently operates on untyped syntax.
We thus have four options:
(1) use intrinsically-well-typed syntax everywhere, paying the cost in the pattern-matching compilation and evaluation algorithm;
(2) use untyped syntax in both NbE and rewriting, paying the associated costs in NbE, denotation, and in our proofs;
(3) use intrinsically-well-typed syntax in most passes, and untyped syntax for pattern matching compilation;
(4) invent a pattern-matching compilation algorithm that is well-suited to type-indexed sytnax.
We ultimately chose option (3).
I was not clever enough to follow through on option (4), and while options (1) and (2) are both interesting, option (3) seemed to follow the well-established convention of using whichever datatype is best-suited to the task at hand.
As we'll shortly see, all of these options come with significant costs, and (3) is not as obviously a good choice as it might seem at first glance.

\subsubsection{Pattern-Matching Evaluation on Type-Indexed Terms}
While the cost of performing pattern-matching compilation on type-indexed terms is noticable, it's relatively insignificant compared to the cost of evaluating decisions trees directly on type-indexed terms.
In particular, pattern-matching compilation effectively throws away the type information whenever it encounters it; whether we do this early or late does not matter much, and we only perform this compilation once for any given set of rewrite rules.

By contrast, evaluation of the decision tree needs to produce \emph{term ASTs} that are used in rewriting, and hence we need to preserve type information in the input.
Recall from \autoref{sec:pattern-matching-compilation-and-evaluation} that decision-tree evaluation operates on lists of terms.
Here already we hit our first snag: if we want to operate on well-typed terms, we must index our lists over a list of types.
This is not so bad, but recall also from \autoref{sec:pattern-matching-compilation-and-evaluation} that decision trees contain four constructors:
\begin{itemize}
  \item \texttt{TryLeaf k onfailure}: Try the $k^\text{th}$ rewrite rule; if it fails, keep going with \texttt{onfailure}.
  \item \texttt{Failure}: Abort; nothing left to try.
  \item \texttt{Switch icases app\_case default}:
    With the first element of the vector, match on its kind; if it is an identifier matching something in \texttt{icases}, which is a list of pairs of identifiers and decision trees, remove the first element of the vector and run that decision tree; if it is an application and \texttt{app\_case} is not \texttt{None}, try the \texttt{app\_case} decision tree, replacing the first element of each vector with the two elements of the function and the argument it is applied to; otherwise, do not modify the vectors and use the \texttt{default} decision tree.
  \item \texttt{Swap i cont}: Swap the first element of the vector with the $i^\texttt{th}$ element (0-indexed) and keep going with \texttt{cont}.
\end{itemize}
The first two constructors are not very interesting, as far as overhead goes, but the third and fourth constructors are quite painful.

Note that the type of \mintinline{coq}{eval_decision_tree} would be something like \mintinline{coq}{∀ {T : Type} (d : decision_tree) (ts : list type) (es : exprlist ts) (K : exprlist ts → option T), option T}.


We cover the \mintinline{coq}{Swap} case first, because it is simpler.
To perform a \mintinline{coq}{Swap}, we must exchange two elements of the type-indexed list.
Hence we need both two swap the elements of the list of types, and then to have a separate, dependently-typed swap function for the vector of expressions.
Moreover, since we need to undo the swapping inside the continutation \todo{does this need more explanation or code?}, we must have an \emph{separate} unswap function on expression vectors which goes from a swapped type list to the original one.
We could instead elide the swap node, but then we could no longer use matching, \mintinline{coq}{hd}, and \mintinline{coq}{tl} to operate on the expressions, and would instead need special operations to do surgery in the middle of the list, in a way that preserves type-indexing.

To perform a \mintinline{coq}{Switch}, we must break apart the first element of our type-indexed list, determining whether it is an application, and identifier, or other.
Note that even with dependent types, we cannot avoid needing a failure case for when the type-indexed list is empty, even though such a case should never occur.
This mismatch---the need to include failure cases that one might expect to be eliminated by dependent typing information---is a sign that the amount of dependency in the types is wrong.
It may be too little, whence the developer should see if there is a way to incorporate the lack-of-error into the typing information (which in this case would require indexing the type of the decision tree over the length of the vector).
It may alternatively be to much dependent typing, and the developer might be well-served by removing more dependency from the types and letting more things fall into the error case.

After breaking apart the first element, we must convoy \todo{should I cite CPDT or somewhere in this thesis for the convoy pattern?} the continuation across the \mintinline{coq}{match} statement so that we can pass an expression vector of the correct type to the continuation \mintinline{coq}{K}.
In code, this branch might look something like
\begin{minted}{coq}
…
| Switch icases app_case default
  => match es in exprlist ts return (exprlist ts → option T) → option T with
     | [] => λ _, None
     | e :: es
       => match e in expr t return (exprlist (t :: ts) → option T) → option T with
          | App s d f x => λ K,
              let K' : exprlist ((s → d) :: s :: ts) (* new continuation to pass on recursively *)
                := λ es', K (App (hd es') (hd (tl es')) :: tl (tl es')) in
              … (* do something with app_case *)
          | Ident t idc => λ K, … (* do something with icases *)
          | _ => λ K,
              let K' : exprlist ts (* new continuation to pass on recursively *)
                := λ es', K (Ident idc :: es') in
              … (* do something with default, eventually calling  *)
          end
     end K
…
\end{minted}
Note that \mintinline{coq}{hd} and \mintinline{coq}{tl} \emph{must} be type-indexed, and we \emph{cannot} simply match on \mintinline{coq}{es'} in the \mintinline{coq}{App} case;
there is no way to preserve the connection between the types of the first two elements of \mintinline{coq}{es'} inside such a \mintinline{coq}{match} statement.

This may not look too bad, but it gets worse.
Since the \mintinline{coq}{match} on \mintinline{coq}{e} will not be known until we are actually doing the rewriting on a concrete expression, and the continuation is convoyed across this \mintinline{coq}{match}, there is no way to evaluate the continuation during compilation of rewrite rules.
If we don't want to evaluate the continutation early, we'd have to be very careful not to duplicate it across all of the decision tree evaluation cases, as we might otherwise incur a super-linear runtime factor in the number of rewrite rules.
As noted in \autoref{sec:rewriting-more:pre-reduction}, our early reduction nets us a $2\times$ speedup in runtime of rewriting, and is therefore relatively important to be able to do.

Here we see something interesting, which does not appear to be as much of a concern in other programming languages:
the representation of our data forces our hand about how much efficiency can be gained from precomputation, even when the representation choices are relatively minor.

\subsubsection{Untyped Syntax in NbE}
There is no good way around the fact that NbE requires typing information to argue termination.
Since NbE will be called on subterms of the overall term, even if we use syntax that is not guaranteed to be type-correct, we must still store the type information in the nodes of the AST.




\begin{itemize}
\item \todo{talk about rawexpr\_types\_ok}
\item \todo{talk about how we can unify types at all (c.f.~\texttt{preunify\_types})}
\end{itemize}

\subsection{Patterns with Type Variables -- The Three Kinds of Identifiers}
\begin{itemize}
\item \todo{talk about the default of having syntax indexed over the set of available type variables}
\item \todo{talk about how lifting causes pain due to dependent types, also linear traversal of term}
\end{itemize}

\subsection{Pre-evaluation}
$\left.\right.$
\subsubsection{CPS}
$\left.\right.$
\subsubsection{Type Codes}
$\left.\right.$
\subsubsection{What Can We Unfold?}
\begin{itemize}
\item \todo{talk about the ``known'' parameter of rIdent}
\item \todo{talk about multiple aliases like \texttt{option\_bind'}}
\item \todo{talk about lack of help from the compiler / type-checker}
\item \todo{talk about ``Note that here we are jumping through some extra hoops to get the right reduction behavior at rewrite-rule-compilation time.'' for \texttt{eval\_decision\_tree}}
\item \todo{foward-reference pain with casts?}
\item \todo{talk about whether or not to eliminate PositiveMap.t?} % ``In a possibly-gratuitous use of dependent typing to ensure that''
\item \todo{talk about the general tradeoff between runtime checks and static proofs} % ``However, the proofs are much simpler if we simply do a wholesale check at the very end
\item \todo{talk about the cost of inconsistent decisions spreading pain elsewhere} % ``Here we pay the price of an imperfect abstraction barrier (that we have types lying around, and we rely in some places on types lining up, but do not track everywhere that types line up).''
\end{itemize}
\subsubsection{Revealing ``Enough'' Structure}
\begin{itemize}
\item \todo{talk also about tracking both the revealed and unrevealed structure}
\item \todo{talk about $\eta$-expanding identifier matches}
\end{itemize}

\subsection{The Let-In Monad: Missing Abstraction Barriers at the Type Level}
\todo{Talk also about the pain of wf statements for, e.g., \texttt{wf\_normalize\_deep\_rewrite\_rule}, having to go underneath multiple monads}

\subsection{Rewriting Again in the Output of a Rewrite Rule}
$\left.\right.$

\subsection{Delayed Rewriting in Variable Nodes}
\todo{rValue vs rExpr}

\subsection{Relating Expressions and Values}
\todo{figure out where this goes, how to explain it, where it arises from:}
``In general, the stored values are only interp-related to the same things that the ``unrevealed structure'' expressions are interp-related to. There is no other relation (that we've found) between the values and the expressions, and this caused a great deal of pain when trying to specify the interpretation correctness properties.''

\subsection{Which Equivalence Relation?}
\todo{talk about \texttt{rawexpr\_equiv}, 4-place \texttt{wf\_rawexpr}, nuance of revealing structure in CPS'd \texttt{eval\_decision\_tree}, non-obvious \texttt{wf\_value} binding list, \texttt{interp\_related\_gen} unsuccessfully avoiding funext (and also needing to be instantiated with \texttt{value\_interp\_related} sometimes), \texttt{rawexpr\_interp\_related} and separating (or not) goodness from relatedness, \texttt{rawexpr\_types\_ok}?, \texttt{unification\_resultT'\_interp\_related}?, \texttt{interp\_unify\_pattern'}?, interpretation-correctness related to rewriting again}

\subsection{Dependently Typed Pain in Applying Rewrite Rules}
\begin{itemize}
\item \todo{talk about ``There are two steps to rewriting with a rule \ldots''}
``\ldots\space both conceptually simple but in practice complicated by dependent types.
We must unify a pattern with an expression, gathering binding data for the replacement rule as we go; and we must apply the replacement rule to the binding data (which is non-trivial because the rewrite rules are expressed as curried dependently-typed towers indexed over the rewrite rule pattern).
In order to state the correctness conditions for gathering binding data, we must first talk about applying replacement rules to binding data.''
\item \todo{mention a trade-off here: note that we can't eliminate equality tests/casts early if we introduce them in the wrong place, c.f.~\texttt{app\_transport\_with\_unification\_resultT'\_cps}}
\end{itemize}

\subsection{Dependently Typed Pain in Indexing Over Types}
\todo{this subsection}
``We can define a transformation that takes in a \texttt{PositiveMap.t} of pattern type variables to types, together with a \texttt{PositiveSet.t} of type variables that we care about, and re-creates a new \texttt{PositiveMap.t} in accordance with the \texttt{PositiveSet.t}.
This is required to get some theorem types to line up, and is possibly an indication of a leaky abstraction barrier.''

\subsection{What's the Ground Truth: Patterns Or Expressions?}
\todo{default interpretation of a pattern --- complicated, but perhaps needed for phrasing correctness of unification}

\section{Leaky Abstraction Barriers: NbE vs.~Pattern Matching Compilation}

\clearpage

\todo{this chapter}
\todo{mention frowned-upon Perl scripts previously in BoringSSL(?) OpenSSL?; (ask Andres for reference?)} Perl scripts were complicated, a number of steps removed from actual running code, hard to maintain and verify.
\todo{Refer back to representation changes (good abstraction barriers / equivalences) being important in fiat-crypto, and being cheap only because we have a rewriter}
\setlistdepth{20}
\renewlist{itemize}{itemize}{20}%
\setlist[itemize,1]{label=\textbullet}%
\setlist[itemize,2]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,3]{label=\textasteriskcentered}%
\setlist[itemize,4]{label=\textperiodcentered}%
\setlist[itemize,5]{label=\textbullet}%
\setlist[itemize,6]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,7]{label=\textasteriskcentered}%
\setlist[itemize,8]{label=\textperiodcentered}%
\setlist[itemize,9]{label=\textbullet}%
\setlist[itemize,10]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,11]{label=\textasteriskcentered}%
\setlist[itemize,12]{label=\textperiodcentered}%
\setlist[itemize,13]{label=\textbullet}%
\setlist[itemize,14]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,15]{label=\textasteriskcentered}%
\setlist[itemize,16]{label=\textperiodcentered}%
\setlist[itemize,17]{label=\textbullet}%
\setlist[itemize,18]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,19]{label=\textasteriskcentered}%
\setlist[itemize,20]{label=\textperiodcentered}%

\input{rewriting/rewriting.md.tex}
