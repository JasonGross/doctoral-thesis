\chapter{Engineering Challenges in the Rewriter}\label{ch:rewriting-more}
\setboolean{zerowidthscripts}{true}%

\begin{quote}
  premature optimization is the root of all evil
\end{quote}
\begin{flushright}
  --- Donald Knuth
\end{flushright}

\todo{Better chapter title?}
\section{Introduction}\label{sec:rewriting-more:intro}
\autoref{ch:rewriting} discussed in detail our framework for building verified partial evaluators, going into the context, motivation, and the techniques used to put the framework together.
However, there was a great deal of engineering effort that went into building this tool which we glossed over.
Much of the engineering effort was mundane, and we elide the details entirely.
However, we believe some of the engineering effort serves as a good case-study for the difficulties of building proof-based systems at scale.
This chapter is about exposing the details relevant to understanding how the bottlenecks and principles identified elsewhere in this thesis played out in designing and implementing this tool.

\section{A Brief Survey of the Engineering Challenges}\label{sec:rewriting-more:challenges-overview}

While the core rewriting engine of the framework is about 1\,300 lines of code, and early simplified versions of the core engine were only about 150 lines of code%
\footnote{%
See \href{https://web.archive.org/web/20200716002534/https://github.com/JasonGross/fiat-crypto/blob/3b3e926e4186caa1a4003c81c65dad0a1c04b43d/src/Experiments/RewriteRulesSimpleNat.v}{\texttt{https://github.com/JasonGross/fiat-crypto/blob/3b3e926e4186caa1a4003c81c65dad0a1c04b43d/src/Experiments/RewriteRulesSimpleNat.v}} for the file \texttt{src/Experiments/RewriteRulesSimpleNat.v} from \href{https://github.com/JasonGross/fiat-crypto/tree/experiments-small-rewrite-rule-compilation}{the branch \texttt{experiments-small-rewrite-rule-compilation} on \texttt{JasonGross/fiat-crypto}} on GitHub.%
}%
, the correctness proofs take nearly another 8\,000 lines of code!
% git ls-files "src/Rewriter/Rewriter/*.v" | grep -o 'src/Rewriter/Rewriter/[^/]*\.v' | xargs coqwc  | sort -h | less
% add up totals, subtract off the lines in Rewriter.v
As such, this tool, developed to solve performance scaling issues in verified syntax transformation, itself serves as a good case study of some of the pain that arises when scaling proof-based engineering projects.

Our discussion in this section is organized by the conceptual structure of the normalization and pattern matching compilation engine;
we hope that organizing the discussion in this way will make the examples more understandable, motivated, and incremental.
We note, however, that many of the challenges fall into the same broad categories that we've identified earlier in this thesis:
issues arising from the power and (mis)use of dependent types, as introduced in \fullref{sec:why-how-dependent-types};
and issues arising arising from API mismatches, as described in \fullref{ch:api-design}.

\subsection{Pre-Reduction}\label{sec:rewriting-more:pre-reduction}
The the two biggest underlying causes of engineering challenges are expression API mismatch, which we'll discuss in \fullref{sec:rewriting-more:AST:choices}, and our desire to reduce away known computations in the rewriting engine once and for all when compiling rewriting rules, rather than again and again every time we perform a rewrite.
In pratice, performing this early reduction nets us an approximately $2\times$ speed-up.

\subsubsection{What does this reduction consist of?}\label{sec:rewriting-more:pre-reduction:what-reduction}
Recall from \autoref{sec:pattern-matching-compilation-and-evaluation} that the core of our rewriting engine consists of three steps:
\begin{enumerate}
\item
  The first step is pattern-matching compilation: we must compile the lefthand sides of the rewrite rules to a decision tree that describes how and in what order to decompose the expression, as well as describing which rewrite rules to try at which steps of decomposition.
\item
  The second step is decision-tree evaluation, during which we decompose the expression as per the decision tree, selecting which rewrite rules to attempt.
\item
  The third and final step is to actually rewrite with the chosen rule.
\end{enumerate}
The first step is performed once and for all; it depends only on the rewrite rules, and not on the expression we are rewriting in.
The second and third steps do, in fact, depend on the expression being rewritten, and it is in these steps that we seek to eliminate needless work early.

The key insight, which allows us to perform this precompilation at all, is that the most of the decisions we seek to eliminate depend only on the \emph{head identifier} of any application.%
\footnote{%
  In order to make this simplification, we need to restrict the rewrite rules we support a little bit.
  In particular, we only support rewrite rules operating on $\eta$-long applications of concrete identifiers to arguments.
  This means that we cannot support identifiers with variable arrow structure (e.g., a variadic \mintinline{coq}{curry} function) nor do we support rewriting things like \mintinline{coq}{List.map f} to \mintinline{coq}{List.map g}---we only support rewriting \mintinline{coq}{List.map f xs} to \mintinline{coq}{List.map g ys}.%
}
We thus augment the $\text{reduce}(c)$ constant case of \autoref{fig:nbe} in \autoref{sec:thunk-eval-subst-term} by first $\eta$-expanding the identifier, before proceeding to $\eta$-expand the identifier application and perform rewriting with \text{rewrite-head} once we have an $\eta$-long form.

Now that we know what the reduction consists of, we can
{sec:rewriting-more:revealing-enough-structure}




\subsection{NbE vs.~Pattern Matching Compilation: Mismatched Expression APIs}\label{sec:rewriting-more:AST:choices}
\todo{should I be consistent about naming syntax that's well-typed by construction?  I call it ``intrinsically-typed syntax'', ``syntax that is well-typed by construction'', ``intrinsically-well-typed syntax'', and ``type-indexed sytnax''\ldots}
We introduced normalization by evaluation (NbE)~\cite{NbE} in \autoref{sec:our-solution} and expanded on it in \autoref{sec:thunk-eval-subst-term} as a way to support higher-order reduction of $\lambda$-terms.
The termination argument for NbE proceeds by recursion on the type of the term we're reducing.
In particular, the most natural way to define these functions in a proof assistant is to proceed by structural recursion on the type of the term being reduced.
This feature suggests that using intrinsically-typed syntax is more natural for NbE, and we saw in \autoref{sec:binders:de-bruijn} that denotation functions are also simpler on syntax that is well-typed by construction.

However, the pattern-matching compilation algorithm of \textcite{maranget2008compiling} inherently operates on untyped syntax.
We thus have four options:
(1) use intrinsically-well-typed syntax everywhere, paying the cost in the pattern-matching compilation and evaluation algorithm;
(2) use untyped syntax in both NbE and rewriting, paying the associated costs in NbE, denotation, and in our proofs;
(3) use intrinsically-well-typed syntax in most passes, and untyped syntax for pattern matching compilation;
(4) invent a pattern-matching compilation algorithm that is well-suited to type-indexed sytnax.
We ultimately chose option (3).
I was not clever enough to follow through on option (4), and while options (1) and (2) are both interesting, option (3) seemed to follow the well-established convention of using whichever datatype is best-suited to the task at hand.
As we'll shortly see, all of these options come with significant costs, and (3) is not as obviously a good choice as it might seem at first glance.

\subsubsection{Pattern-Matching Evaluation on Type-Indexed Terms}\label{sec:rewriting-more:AST:type-indexed-pattern-matching}
While the cost of performing pattern-matching compilation on type-indexed terms is noticable, it's relatively insignificant compared to the cost of evaluating decisions trees directly on type-indexed terms.
In particular, pattern-matching compilation effectively throws away the type information whenever it encounters it; whether we do this early or late does not matter much, and we only perform this compilation once for any given set of rewrite rules.

By contrast, evaluation of the decision tree needs to produce \emph{term ASTs} that are used in rewriting, and hence we need to preserve type information in the input.
Recall from \autoref{sec:pattern-matching-compilation-and-evaluation} that decision-tree evaluation operates on lists of terms.
Here already we hit our first snag: if we want to operate on well-typed terms, we must index our lists over a list of types.
This is not so bad, but recall also from \autoref{sec:pattern-matching-compilation-and-evaluation} that decision trees contain four constructors:
\begin{itemize}
  \item \texttt{TryLeaf k onfailure}: Try the $k^\text{th}$ rewrite rule; if it fails, keep going with \texttt{onfailure}.
  \item \texttt{Failure}: Abort; nothing left to try.
  \item \texttt{Switch icases app\_case default}:
    With the first element of the vector, match on its kind; if it is an identifier matching something in \texttt{icases}, which is a list of pairs of identifiers and decision trees, remove the first element of the vector and run that decision tree; if it is an application and \texttt{app\_case} is not \texttt{None}, try the \texttt{app\_case} decision tree, replacing the first element of each vector with the two elements of the function and the argument it is applied to; otherwise, do not modify the vectors and use the \texttt{default} decision tree.
  \item \texttt{Swap i cont}: Swap the first element of the vector with the $i^\texttt{th}$ element (0-indexed) and keep going with \texttt{cont}.
\end{itemize}
The first two constructors are not very interesting, as far as overhead goes, but the third and fourth constructors are quite painful.

Note that the type of \mintinline{coq}{eval_decision_tree} would be something like \mintinline{coq}{∀ {T : Type} (d : decision_tree) (ts : list type) (es : exprlist ts) (K : exprlist ts → option T), option T}.


We cover the \mintinline{coq}{Swap} case first, because it is simpler.
To perform a \mintinline{coq}{Swap}, we must exchange two elements of the type-indexed list.
Hence we need both two swap the elements of the list of types, and then to have a separate, dependently-typed swap function for the vector of expressions.
Moreover, since we need to undo the swapping inside the continutation \todo{does this need more explanation or code?}, we must have an \emph{separate} unswap function on expression vectors which goes from a swapped type list to the original one.
We could instead elide the swap node, but then we could no longer use matching, \mintinline{coq}{hd}, and \mintinline{coq}{tl} to operate on the expressions, and would instead need special operations to do surgery in the middle of the list, in a way that preserves type-indexing.

To perform a \mintinline{coq}{Switch}, we must break apart the first element of our type-indexed list, determining whether it is an application, and identifier, or other.
Note that even with dependent types, we cannot avoid needing a failure case for when the type-indexed list is empty, even though such a case should never occur.
This mismatch---the need to include failure cases that one might expect to be eliminated by dependent typing information---is a sign that the amount of dependency in the types is wrong.
It may be too little, whence the developer should see if there is a way to incorporate the lack-of-error into the typing information (which in this case would require indexing the type of the decision tree over the length of the vector).
It may alternatively be to much dependent typing, and the developer might be well-served by removing more dependency from the types and letting more things fall into the error case.

After breaking apart the first element, we must convoy \todo{should I cite CPDT or somewhere in this thesis for the convoy pattern?} the continuation across the \mintinline{coq}{match} statement so that we can pass an expression vector of the correct type to the continuation \mintinline{coq}{K}.
In code, this branch might look something like
\begin{minted}{coq}
…
| Switch icases app_case default
  => match es in exprlist ts
       return (exprlist ts → option T) → option T
     with
     | [] => λ _, None
     | e :: es
       => match e in expr t
            return (exprlist (t :: ts) → option T) → option T
          with
          | App s d f x => λ K,
              let K' : exprlist ((s → d) :: s :: ts)
                 (* new continuation to pass on recursively *)
                := λ es', K (App (hd es') (hd (tl es')) :: tl (tl es')) in
              … (* do something with app_case *)
          | Ident t idc => λ K,
              let K' : exprlist ts
                 (* new continuation to pass on recursively *)
                := λ es', K (Ident idc :: es') in
              … (* do something with icases *)
          | _ => λ K, … (* do something with default *)
          end
     end K
…
\end{minted}
Note that \mintinline{coq}{hd} and \mintinline{coq}{tl} \emph{must} be type-indexed, and we \emph{cannot} simply match on \mintinline{coq}{es'} in the \mintinline{coq}{App} case;
there is no way to preserve the connection between the types of the first two elements of \mintinline{coq}{es'} inside such a \mintinline{coq}{match} statement.

This may not look too bad, but it gets worse.
Since the \mintinline{coq}{match} on \mintinline{coq}{e} will not be known until we are actually doing the rewriting on a concrete expression, and the continuation is convoyed across this \mintinline{coq}{match}, there is no way to evaluate the continuation during compilation of rewrite rules.
If we don't want to evaluate the continutation early, we'd have to be very careful not to duplicate it across all of the decision tree evaluation cases, as we might otherwise incur a super-linear runtime factor in the number of rewrite rules.
As noted in \autoref{sec:rewriting-more:pre-reduction}, our early reduction nets us a $2\times$ speedup in runtime of rewriting, and is therefore relatively important to be able to do.

Here we see something interesting, which does not appear to be as much of a concern in other programming languages:
the representation of our data forces our hand about how much efficiency can be gained from precomputation, even when the representation choices are relatively minor.

\subsubsection{Untyped Syntax in NbE}\label{sec:rewriting-more:AST:untyped-nbe}
There is no good way around the fact that NbE requires typing information to argue termination.
Since NbE will be called on subterms of the overall term, even if we use syntax that is not guaranteed to be type-correct, we must still store the type information in the nodes of the AST.

Furthermore, as we say in \fullref{sec:binders:de-bruijn}, converting from untyped syntax to intrinsically-typed syntax, as well as writing a denotation function, requires either that all types be non-empty, or that we carry around a proof of well-typedness to use during recursion.
\todoask{Is there a good reference for these sorts of issues?  Are they well-known?  Well-studied?}
As discussed in \autoref{ch:design} and specifically in \fullref{ch:when-how-dependent-types}, needing to mix proofs with programs is often a big warning flag, unless the mixing can be hidden behind a well-designed API.
However, if we are going to be hiding the syntax behind an API of being well-typed, it seems like we might as well just use intrinsically well-typed syntax, which natrually inhabits that API.
Furthermore, unlike in many cases where the API is best treated as opaque everywhere, here the API mixing proofs and programs needs to have adequate behavior under reduction, and ought to have good behavior even under partial reduction.
This severely complicates the task of building a good abstraction barrier, as we not only need to ensure that the abstraction barrier does not need to be broken in the course of term-building and typechecking, but we must also ensure that the abstraction barrier can be broken in a principled way via reduction without introducing significant overhead.

%Despite all of these problems, in retrospect, it seems that this option might in fact be the least-costly option to choose.
%Admittedly, we have not actually implemented it, so there might remain hidden engineering challenges.

\subsubsection{Mixing Typed and Untyped Syntax}\label{sec:rewriting-more:AST:both}
The third option is to use whichever datatype is most naturally suited for each pass, and to convert between them as necessary.
This is the option that we ultimately chose, and the one, we believe, that would be most natural to choose to engineers and developers coming from non-dependently-typed languages.

There are a number of considerations that arose when fleshing out this design, and a number of engineering-pain-points that we encountered.
The theme to all of these, as in \autoref{ch:api-design}, is that imperfectly opaque abstraction barriers cause headaches in a non-local manner.

We got lucky, in some sense, that the rewriting pass \emph{always} has a well-typed default option: do no rewriting.
Hence we do not need to worry about carrying around proofs of well-typedness, and this avoids some of the biggest issues described in \nameref{sec:rewriting-more:AST:untyped-nbe}.

The biggest constraint driving our design decisions is that we need conversion between the two representations to be $\mathcal{O}(1)$; if we need to walk the entire syntax tree to convert between typed and untyped representations at every rewriting location, we'll incur quadratic overhead in the size of the term being rewritten.
We can actually relax this constraint a little bit: by designing the untyped representation to be completely evaluated away during the compilation of rewrite rules, we can allow conversion from the untyped syntax to the typed syntax to walk any part of the term that already needed to be revealed for rewriting, giving us amortized constant time rather than truely constant time.
\todo{is this last sentence understandable?}
As such, we need to be able to embed well-typed syntax directly into the non-type-indexed representation at cost $\mathcal{O}(1)$.

As the entire purpose of the untyped syntax is to (a) allow us to perform matching on the AST to determine which rewrite rule to use, and furthermore (b) allow us to reuse the decomposition work so as to avoid needing to decompose the term multiple times, we need an inductive type which can embed PHOAS expressions, and has separate nodes for the structure that we need, namely application and identifiers:

\begin{minted}{coq}
Inductive rawexpr : Type :=
| rIdent (known : bool) {t} (idc : ident t) {t'} (alt : expr t')
| rApp (f x : rawexpr) {t} (alt : expr t)
| rExpr {t} (e : expr t)
| rValue {t} (e : NbEₜ t).
\end{minted}
\label{sec:rewriting-more:rawexpr-def}%
There are three perhaps-unexpected things to note about this inductive type, which we will discuss in later subsections:
\begin{enumerate}
\item
  The constructor \mintinline{coq}{rValue} holds an NbE-value of the type \mintinline{coq}{NbEₜ} introduced in \autoref{sec:thunk-eval-subst-term}.
  We will discuss this in \fullref{sec:rewriting-more:delayed-rewriting}.
\item
  The constructors \mintinline{coq}{rIdent} and \mintinline{coq}{rExpr} hold ``alternate'' PHOAS expressions.
  We will discuss this in \fullref{sec:rewriting-more:revealing-enough-structure}.
\item
  The constructor \mintinline{coq}{rIdent} has an extra boolean \mintinline{coq}{known}.
  We will discuss this in \fullref{sec:rewriting-more:rIdent-known}.
\end{enumerate}

With this inductive type in hand, it's easy to see how \mintinline{coq}{rExpr} allows us $\mathcal{O}(1)$ embedding of intrinsically typed \mintinline{coq}{expr}s into untyped \mintinline{coq}{rawexpr}s.

While it's likely that sufficiently good abstraction barriers around this datatype would allow us to use it with relatively little pain, we did not succeed in designing good enough abstraction barriers.
The bright side of this failure is that we now have a number of examples for this thesis of ways in which inadequate abstraction barriers cause pain.

We will discuss the many issues that arise from leaks in this abstraction barrier in the upcoming subsections.
%While some of these issues will be discussed in \Autoref{sec:rewriting-more:delayed-rewriting,sec:rewriting-more:revealing-enough-structure,sec:rewriting-more:rIdent-known} where we discuss the perhaps-unexpected additions to the \mintinline{coq}{rawexpr} constructors, we can discuss a few complications here.

%\paragraph{Equivalence Relations}
%Correctness conditions on PHOAS transformations are frequently stated in terms of two (partial) equivalence relations: the well-formedness relation \mintinline{coq}{Wf} defined in \autoref{sec:PHOAS:Wf-def}; and pointwise equality---that is, equality up to function extensionality---of the interpretations of the expressions.
%\todo{make sure I'm being consistent about ``interpretation'' vs ``denotation'' across the thesis}

\subsubsection{Pattern Matching Compilation Made For Intrinsically-Typed Syntax}\label{sec:rewriting-more:AST:better-pattern-matching}
The cost of this fourth option is the cleverness required to come up with a version of the pattern matching compilation which, rather than being hindered by types in its syntax, instead puts them to good use.
Lacking this cleverness, we were unable to pay the requisite cost, and hence have not much to say in this section.

\subsection{Patterns with Type Variables -- The Three Kinds of Identifiers}
\begin{itemize}
\item \todo{talk about the default of having syntax indexed over the set of available type variables}
\item \todo{talk about how lifting causes pain due to dependent types, also linear traversal of term}
\item \todo{talk about how we can unify types at all (c.f.~\texttt{preunify\_types})}
\end{itemize}

\subsection{Pre-evaluation}
$\left.\right.$
\subsubsection{CPS}
$\left.\right.$
\subsubsection{Type Codes}
$\left.\right.$
\subsubsection{What Can We Unfold?}
\begin{itemize}
\item \todo{talk about the ``known'' parameter of rIdent} \label{sec:rewriting-more:rIdent-known}
\item \todo{talk about multiple aliases like \texttt{option\_bind'}}
\item \todo{talk about lack of help from the compiler / type-checker}
\item \todo{talk about ``Note that here we are jumping through some extra hoops to get the right reduction behavior at rewrite-rule-compilation time.'' for \texttt{eval\_decision\_tree}}
\item \todo{foward-reference pain with casts?}
\item \todo{talk about whether or not to eliminate PositiveMap.t?} % ``In a possibly-gratuitous use of dependent typing to ensure that''
\item \todo{talk about the general tradeoff between runtime checks and static proofs} % ``However, the proofs are much simpler if we simply do a wholesale check at the very end
\item \todo{talk about the cost of inconsistent decisions spreading pain elsewhere} % ``Here we pay the price of an imperfect abstraction barrier (that we have types lying around, and we rely in some places on types lining up, but do not track everywhere that types line up).''
\end{itemize}
\subsubsection{Revealing ``Enough'' Structure}\label{sec:rewriting-more:revealing-enough-structure}
\begin{itemize}
\item \todo{talk also about tracking both the revealed and unrevealed structure}
\item \todo{talk about $\eta$-expanding identifier matches, reference \autoref{sec:rewriting-more:pre-reduction}}
\item \todo{talk about rawexpr\_types\_ok}
\end{itemize}

\subsection{The Let-In Monad: Missing Abstraction Barriers at the Type Level}
\todo{Talk also about the pain of wf statements for, e.g., \texttt{wf\_normalize\_deep\_rewrite\_rule}, having to go underneath multiple monads}

\subsection{Rewriting Again in the Output of a Rewrite Rule}
$\left.\right.$

\subsection{Delayed Rewriting in Variable Nodes}\label{sec:rewriting-more:delayed-rewriting}
\todo{rValue vs rExpr}

\subsection{Relating Expressions and Values}
\todo{figure out where this goes, how to explain it, where it arises from:}
``In general, the stored values are only interp-related to the same things that the ``unrevealed structure'' expressions are interp-related to. There is no other relation (that we've found) between the values and the expressions, and this caused a great deal of pain when trying to specify the interpretation correctness properties.''

\subsection{Which Equivalence Relation?}\label{sec:rewriting-more:which-equivalence}
\todo{talk about \texttt{rawexpr\_equiv}, 4-place \texttt{wf\_rawexpr}, nuance of revealing structure in CPS'd \texttt{eval\_decision\_tree}, non-obvious \texttt{wf\_value} binding list, \texttt{interp\_related\_gen} unsuccessfully avoiding funext (and also needing to be instantiated with \texttt{value\_interp\_related} sometimes), \texttt{rawexpr\_interp\_related} and separating (or not) goodness from relatedness, \texttt{rawexpr\_types\_ok}?, \texttt{unification\_resultT'\_interp\_related}?, \texttt{interp\_unify\_pattern'}?, interpretation-correctness related to rewriting again}

\subsection{Dependently Typed Pain in Applying Rewrite Rules}
\begin{itemize}
\item \todo{talk about ``There are two steps to rewriting with a rule \ldots''}
``\ldots\space both conceptually simple but in practice complicated by dependent types.
We must unify a pattern with an expression, gathering binding data for the replacement rule as we go; and we must apply the replacement rule to the binding data (which is non-trivial because the rewrite rules are expressed as curried dependently-typed towers indexed over the rewrite rule pattern).
In order to state the correctness conditions for gathering binding data, we must first talk about applying replacement rules to binding data.''
\item \todo{mention a trade-off here: note that we can't eliminate equality tests/casts early if we introduce them in the wrong place, c.f.~\texttt{app\_transport\_with\_unification\_resultT'\_cps}}
\end{itemize}

\subsection{Dependently Typed Pain in Indexing Over Types}
\todo{this subsection}
``We can define a transformation that takes in a \texttt{PositiveMap.t} of pattern type variables to types, together with a \texttt{PositiveSet.t} of type variables that we care about, and re-creates a new \texttt{PositiveMap.t} in accordance with the \texttt{PositiveSet.t}.
This is required to get some theorem types to line up, and is possibly an indication of a leaky abstraction barrier.''

\subsection{What's the Ground Truth: Patterns Or Expressions?}
\todo{default interpretation of a pattern --- complicated, but perhaps needed for phrasing correctness of unification}

\section{Leaky Abstraction Barriers: NbE vs.~Pattern Matching Compilation}

\clearpage

\todo{this chapter}
\todo{mention frowned-upon Perl scripts previously in BoringSSL(?) OpenSSL?; (ask Andres for reference?)} Perl scripts were complicated, a number of steps removed from actual running code, hard to maintain and verify.
\todo{Refer back to representation changes (good abstraction barriers / equivalences) being important in fiat-crypto, and being cheap only because we have a rewriter}
\setlistdepth{20}
\renewlist{itemize}{itemize}{20}%
\setlist[itemize,1]{label=\textbullet}%
\setlist[itemize,2]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,3]{label=\textasteriskcentered}%
\setlist[itemize,4]{label=\textperiodcentered}%
\setlist[itemize,5]{label=\textbullet}%
\setlist[itemize,6]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,7]{label=\textasteriskcentered}%
\setlist[itemize,8]{label=\textperiodcentered}%
\setlist[itemize,9]{label=\textbullet}%
\setlist[itemize,10]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,11]{label=\textasteriskcentered}%
\setlist[itemize,12]{label=\textperiodcentered}%
\setlist[itemize,13]{label=\textbullet}%
\setlist[itemize,14]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,15]{label=\textasteriskcentered}%
\setlist[itemize,16]{label=\textperiodcentered}%
\setlist[itemize,17]{label=\textbullet}%
\setlist[itemize,18]{label=\normalfont \bfseries \textendash}%
\setlist[itemize,19]{label=\textasteriskcentered}%
\setlist[itemize,20]{label=\textperiodcentered}%

\input{rewriting/rewriting.md.tex}

\setboolean{zerowidthscripts}{false}%
