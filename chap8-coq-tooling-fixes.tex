\chapter{A Retrospective on Performance Improvements} \label{ch:coq-tooling-fixes}
Throughout this thesis, we've looked at the problem of performance in proof assistants, especially those based on dependent type theory, with Coq as our primary tool under investigation.
\autoref{part:introduction} aimed to convince the reader that this problem is interesting, important, challenging, and understudied, as it differs in non-trivial ways from performance bottlenecks in non-dependently-typed languages.
\autoref{part:design} discussed design principles to avoid performance pitfalls, and \autoref{part:rewriting} took a deep dive into a particular set of performance bottlenecks and presented a tool, and, we hope, exposed the underlying design methodology, that allows eliminating asymptotic bottlenecks in one important part of proof assistant systems.

In this chapter, we will look instead at the successes of the past decade\footnote{%
  Actually, the time span we're considering is the course of the author's experience with Coq, which is a bit less than a decade.%
}%
, ways in which performance has improved in major ways.
\autoref{sec:fixes:minutiae} will discuss specific improvements in the implementation of Coq which resulted in performance gains, paying special attention to the underlying bottleneck being addressed.
Those without special interest in the low-level details of proof assistant implementation may want to skip to \autoref{sec:fixes:coq-theory}, which will discuss changes to the underlying type theory of Coq which make possible drastic performance improvements.
While we will again have our eye on Coq in \autoref{sec:fixes:coq-theory}, we will broaden our perspective in \autoref{sec:fixes:theory} to discuss new discoveries of the past decade or so in dependent type theory which enable performance improvements but have not yet made their way into Coq.

\section{Concrete Performance Advancements in Coq}\label{sec:fixes:minutiae}
In this section, we dive into the minutiae: concrete changes to Coq that have measurably increased performance.
\subsection{Removing Pervasive Evar Normalization}\label{sec:econstr}
Back when I started using Coq, in version 8.4, almost every single tactic was at least linear in performance in the size of the goal.
This included tactics like ``add new hypothesis to the context of type \mintinline{coq}{True}'' (\mintinline{coq}{pose proof I}) and tactics like ``give me the type of the most recently added hypothesis'' (\mintinline{coq}{match goal with H : ?T |- _ => T end}).
The reason for this was pervasive \emph{evar normalization}.

Let us review some details of the way Coq handles proof scripts.
\todo{if this is described earlier, refer back to it}
Coq a partial proof term, where not-yet-given subterms are \emph{existential variables}, or evars, which may show up as goals.
For example, when proving the goal \mintinline{coq}{True ∧ True}, after running \mintinline{coq}{split}, the proof term would be \mintinline{coq}{conj ?Goal1 ?Goal2}, where \mintinline{coq}{?Goal1} and \mintinline{coq}{?Goal2} are evars.
There are two subtleties:
\begin{enumerate}
\item
  Evars may be under binders.
  Coq uses a locally nameless representation of terms (c.f.~\autoref{sec:binders:locally-nameless}), where closed terms use de Bruijn indices, but open terms, i.e., evars, refer to the variables in their context by name.
  Hence each evar carries with it a named context, which causes a great deal of trouble as described in \fullref{sec:perf:quadratic-evar-subst}.
\item
  Coq supports backtracking, so we must remember the history of partial proof terms.
  In particular, we cannot simply mutate partial proof terms to instantiate the evars, and copying the entire partial proof term just to update a small part of it would also incur a great deal of overhead.
  Instead, Coq never mutates the terms, and instead simply keeps a map of which evars have been instantiated with which terms, called the \emph{evar map}.
\end{enumerate}

There is an issue with the straightforward implementation of evars and evar maps.
When walking terms, care must be taken with the evar case, to check whether or not the evar has in fact been instantiated or not.
Subtle bugs in unification and other areas of Coq resulted from some functions being incorrectly sensitive to whether or not a term had been built via evar instantiation or given directly.%
\footnote{%
  See the discussion at \textcite{coq-pr-econstr} for more details.%
}
The fast-and-easy solution used in older versions of Coq was to simply evar-normalize the goal before walking it.
That is, every tactic that had to walk the goal for any reason whatsoever would create a copy of the type of the goal---and sometimes the proof context as well---replacing all instantiated evars with their instantiation.
Needless to say, this was very expensive when the size of the goal was large.

\todo{include performance graphs?}

As of Coq 8.7, most tactics no longer perform useless evar normalization, and instead walk terms using a dedicated API which does on-the-fly normalization as necessary.~\cite{coq-pr-econstr}
This brought speedups of over 10\% to some developments, and improved asymptotic performance of some tactic scripts and interactive proof development.

\subsection{Delaying the Externalization of Application Arguments}\label{sec:delayed-externalization}
Coq has many representations of terms.
There is \texttt{constr\_expr}, the AST produced by Coq's parser.
Interalization turns \texttt{constr\_expr} into the untyped \texttt{glob\_constr} representation of terms by performing name resolution, bound variable checks, notation desugaring, and implicit argument insertion.~\cite{Constrintern}
Type inference fills in the holes in untyped \texttt{glob\_constr}s to turn them into typed \texttt{constr}s, possibly with remaining existential variables.~\cite{Pretyping}
In order to display proof goals, this process must be reversed.
The internal representation of \texttt{constr} must be ``detyped'' into \texttt{glob\_constr}s, which involves primarily just turning de Bruijn indices into names.~\cite{Detyping}
Finally, implicit arguments must be erased and notations must be re-sugared when externalizing \texttt{glob\_constr}s into \texttt{constr\_expr}s, which can be printed relatively straightforwardly.~\cite{Constrextern,Ppconstr}

In old versions, Coq would externalize the entire goal, including subterms that were never printed due to being hidden by notations and implicit arguments.
Starting in version 8.5pl2, lazy externalization of function arguments was implemented.~\cite{coq-commit-delayed-externalization}
This resulted in massive speed-ups to interactive development involving large goals whose biggest subterms were mostly hidden.

Changes like this one can be a game-changer for interactive proof development.
The kind of development that can happen when it takes a tenth of a second to see the goal after executing a tactic is vastly different from the kind of development that can happen when it takes a full second or two.
In the former case, the proof engine can almost feel like an extension of the coder's mind, responding to thoughts about strategies to try almost as fast as they can be typed.
In the latter case, development is significantly more clunky and involves much more friction.

In the same vein, bugs such as \coqbug{3691} and \coqbug{4819}, where Coq crawled the entire evar map in \texttt{-emacs} mode (used for ProofGeneral/Emacs) looking at all instantiated evars, resulted in interactive proof times of up to half-a-second for every goal display, even when the goal was small and there was nothing in the context.
Fixed in Coq 8.6, these bugs, too, got in the way of seamless proof development.

\subsection{The \Ltac\space Profiler}\label{sec:ltac-prof}
\begin{quote}
  The most common mistake in performance engineering is to blindly optimize without profiling; you most often spend your time optimizing parts that aren't actually bottlenecks.
\end{quote}
\begin{flushright}
  --- Charles Leiserson (heavily paraphrased, reconstructed from memory of 6.172)
\end{flushright}
\todo{Ask Charles about a better/accurate/refined quote}

In old versions of Coq, there was no good way to profile tactic execution.
Users could wrap some invocations in \mintinline{coq}{time} to see how long a given tactic took, or could regularly print some output to see where execution hung.
Both of these are very low-tech methods of performance deubugging, and work well enough for small tactics.
For debugging hundreds or thousands of lines of \Ltac\space code, though, these methods are insufficient.

A genuine profiler for \Ltac\space was developed in 2015 and integrated into Coq itself in version 8.6.~\cite{coqpl-15-ltac-profiler}
\todo{mention the story of how it was created and mention my part in it}

For those interested in amusing quirks of implementation details, the profiler itself was relatively easy to implement.
Since \Ltac\space already records backtraces for error reporting, it was a relatively simple matter to hook into the stack-trace-recorder and track how much time was spent in each call-stack.

\todo{should I include more text here, extracting from the abstract submitted to CoqPL?}

\subsection{Compilation to Native Code}\label{sec:native-compiler}
Starting in version 8.5, Coq allows users to compile their functional Gallina programs to native code and fully reduce them to determine their output.~\cite{nativecompute,coq-commit-native-compiler}
In some cases, the native compiler is almost $10\times$ faster%
\footnote{\url{https://github.com/coq/coq/pull/12405\#issuecomment-633612308}}
than the optimized call-by-value evaluation bytecode-based virtual machine described in \textcite{Gregoire-Leroy-02}.

The native compiler shines most at optimizing algorithmic and computational bottlenecks.
For example, computing the number of primes less than $n$ via the Sieve of Eratosthenes is about $2\times$ to $5\times$ faster in the native compiler than in the VM.
\todo{should I include code and/or plots?}
By contrast, when the input term is very large compared to the amount of computation, the compilation time can dwarf the running time, eating up any gains that the native compiler has over the VM.
This can be seen by comparing the times it takes to get the head of the explicit list of all unary-encoded natural numbers less than, say, 3000, on which the native compiler (1.7s) is about 5\% slower than the VM (1.6s) which itself is about $2\times$ slower than built-in call-by-value reduction machine (0.79s) which requires no translation.
Furthermore, when the output is large, both the VM and the native compiler suffer from inefficiencies in the readback code.
\todo{should I add an example or plots?}
%Fixpoint walk {A} (ls : list A) : unit := match ls with nil => tt | cons _ xs => walk xs end.
%Time Check ltac:(restart_timer;
%                let v := (eval vm_compute in (walk (seq 0 (Z.to_nat 2000)))) in
%                finish_timing ("Tactic call 1(vm)");
%                restart_timer;
%                let v := (eval native_compute in (walk (seq 0 (Z.to_nat 2000)))) in
%                finish_timing ("Tactic call 1(native)");
%                let v := (eval vm_compute in (seq 0 3000)) in
%                restart_timer;
%                let __ := (eval lazy in (List.hd 0%nat v)) in
%                finish_timing ("Tactic call 2(lazy)");
%                restart_timer;
%                let __ := (eval cbv in (List.hd 0%nat v)) in
%                finish_timing ("Tactic call 2(cbv)");
%                restart_timer;
%                let __ := (eval native_compute in (List.hd 0%nat v)) in
%                finish_timing ("Tactic call 2(native)");
%                restart_timer;
%                let __ := (eval vm_compute in (List.hd 0%nat v)) in
%                finish_timing ("Tactic call 2(vm)");
%                exact 0).


\todo{Should I add more to this section?  What should I say?}

\subsection{Primitive Integers and Arrays}\label{sec:prim-ints-arrays}
Primitive 31-bit integer arithmetic operations were added to Coq in 2007.~\cite{coq-commit-int31,Extending2010Armand}
Although most of Coq merely used an inductive representation of 31-bit integers, the VM included code for compiling these constants to native machine integers.%
\footnote{%
  The integer arithmetic is 31-bit rather than 32-bit because OCaml reserves the lowest bit for tagging whether a value is a pointer address to a tagged value or an integer.%
}
After hitting memory limits in storing the inductive representations in proofs involving proof traces from SMT solvers, work was started to allow the use of primitive datatypes that would be stored efficiently in proof terms.~\cite{denes2013prim-ints-arrays}

Some of this work has since been merged into Coq, including IEEE 754-2008 binary64 floating point numbers merged in Coq 8.11~\cite{coq-pr-floats}, 63-bit integers merged in Coq 8.10~\cite{coq-pr-int63}, and persistent arrays~\cite{Persistent2007Conchon} merged into Coq 8.13~\cite{coq-pr-parray}.
Work enabling primitive recursion over these native datatypes is still underway,~\cite{coq-pr-parray-prim-recursion} and the actual use of these primitive datatypes to reap the performance benefits is still to come as of the writing of this thesis.

\subsection{Primitive Projections for Record Types}\label{sec:fixes:theory:primitive-projections}\label{sec:primitive-projections}
Since version 8.5, Coq has had the ability to define record types with projections whose arguments are not stored in the term representation.~\cite{coq-commit-polyproj}
This allows asymptotic speedups, as discussed in \fullref{sec:prim-record-proj}.
\todo{what should be said in this section?}

Note that this is a specific instance of a more general theory of implicit arguments~\cite{logical2001implicit,barras2008implicit}, and there has been other work on how to eliminate useless arguments from term representations.~\cite{Inductive2003Brady}

\subsection{Fast Typing of Application Nodes}
In \fullref{sec:perf:quadratic-application}, we discussed how the typing rule for function application resulted in quadratic performance behavior when there was in fact only linear work that needed to be done.
As of Coq 8.10, when typechecking applications in the kernel, substitution is delayed so as to achieve linear performance.~\cite{coq-pr-fast-application-typing}
\todo{does this need more of a recap?}
Unfortunately, the pretyping and type inference algorithm is still quadratic, due to the type theory rules used for type inference.

\todo{should I talk about https://github.com/coq/coq/pull/263 - Fast lookup in named contexts?}
\todo{should I talk about https://github.com/coq/coq/pull/6506 - Fast rel lookup?}

\section{Performance-Enhancing Advancements in the Type Theory of Coq}\label{sec:fixes:coq-theory}
While some of the above performance enhancements touch the trusted kernel of Coq, they do not fundamentally change the type theory.
Some performance enhancements require significant changes to the type theory.
In this section we will review a couple of particularly important changes of this kind.

\subsection{Universe Polymorphism}\label{sec:fixes:theory:univ-poly}\label{sec:univ-poly}

Recall that the main case study of \autoref{ch:design} was our implementation of a category theory library.
Recall also from \nameref{sec:abstraction-barriers:packed-records} how the choice of whether to use packed or unpacked records impacts performance; while unpacked records are more friendly for developing algebraic hierarchies, packed records achieve significantly better performance when large towers of dependent concepts (such as categories, functors between categories, and natural transformations between functors) are formalized.

This section addresses a particular feature which allows an entire-library $2\times$ speed-up when using fully-packed records.
How is such a large performance gain achievable?
Without this feature, called \emph{universe polymorphism}, encoding some mathematical objects requires \emph{duplicating} the entire library!
Removing this duplication of code will halve the compile-time.

\subsubsection{What are universes?}\label{sec:universes-def}
Universes are type theory's answer to Russell's paradox.~\cite{sep-russell-paradox}
Russell's paradox, a famous paradox discovered in 1901, proceeds as follows.
A \emph{set} is an unordered collection of distinct objects.
Since each \emph{set} is an object, we may consider the set of all sets.
Does this set contain itself?
It must, for by definition it contains all sets.

So we see by example that some sets contain themselves, while others (such as the empty set with no objects) do not.
Let us consider now the set consisting of exactly the sets that do not contain themselves.
Does this set contain itself?
If it does not, then it fails to live up to its description as the set of \emph{all} sets that do not contain themselves.
However, if it does contain itself, then it also fails to live up to its description as a set consisting \emph{only} of sets that do not contain themselves.
Paradox!

The resolution to this paradox is to forbid sets from containing themselves.
The collection of all sets is too big to be a set, so let's call it (and collections of its size) a proper class.
We can nest this construction, as type theory does:
We have \mintinline{coq}{Type₀}, the \mintinline{coq}{Type₁} of all small types, and we have \mintinline{coq}{Type₁}, the \mintinline{coq}{Type₂} of all \mintinline{coq}{Type₁}s, etc.
These subscripts are called \emph{universe levels}, and the subscriped \mintinline{coq}{Type}s are sometimes called \emph{universes}.

Most constructions in Coq work just fine if we simply place them in a single, high-enough, universe.
In fact, the entire standard library in Coq effectively uses only three universes.
Most of the standard library in fact only needs one universe.
We need a second universe for the few constructions that talk about equality between types, and a third for the encoding of a variant of Russell's paradox in Coq.

However, one universe is not sufficent for category theory, even if we don't need to talk about equality of types nor prove that \mintinline{coq}{Type : Type} is inconsistent.

The reason is that category theory, much like set theory, talks about itself.
\todo{check if category has been defined before}

\subsubsection{Complications from Categories of Categories}\label{sec:category-def}\label{sec:category-of-categories}

In standard mathematical practice, a category \cat{C} can be defined~\cite{awodey2010category} to consist of:
\begin{itemize}
  \item
    a class \Ob[C] of \emph{objects}
  \item
    for all objects $a, b \in \Ob[C]$, a class $\Hom[C](a, b)$ of \emph{morphisms from $a$ to $b$}
  \item
    for each object $x \in \Ob[C]$, an \emph{identity morphism} $\Id[x] \in \Hom[C](x, x)$
  \item
    for each triple of objects $a, b, c \in \Ob[C]$, a \emph{composition function} $\circ : \Hom[C](b, c) \times \Hom[C](a, b) \to \Hom[C](a, c)$
\end{itemize}
satisfying the following axioms:
\begin{itemize}
  \item
    associativity: for composable morphisms $f$, $g$, $h$, we have $f \circ (g \circ h) = (f \circ g) \circ h$.
  \item
    identity: for any morphism $f \in \Hom[C](a, b)$, we have $\Id[b] \circ f = f = f \circ \Id[a]$
\end{itemize}

Some complications arise in applying the last subsection's definition of categories to the full range of common constructs in category theory.
One particularly prominent example formalizes the structure of a collection of categories, showing that this collection itself may be considered as a category.

The morphisms in such a category are \emph{functors},\label{sec:define-functor} maps between categories consisting of a function on objects, a function on hom-types, and proofs that these functions respect composition and identity~\cite{mac1998categories,awodey2010category,HoTTBook}.

The na\"ive concept of a ``category of all categories'',\label{sec:category-of-categories} which includes even itself, leads into mathematical inconsistencies which manifest as universe inconsistency errors in Coq, much as with the set of all sets discussed above.

The standard resolution, as with sets, is to introduce a hierarchy of categories, where, for instance, most intuitive constructions are considered \emph{small} categories, and then we also have \emph{large} categories, one of which is the category of small categories.
Both definitions wind up with literally the same text in Coq, giving:
\begin{minted}{coq}
Definition SmallCat : LargeCategory :=
  {| Ob := SmallCategory;
     Hom C D := SmallFunctor C D;
     ⋮
  |}.
\end{minted}

It seems a shame to copy-and-paste this definition (and those of \mintinline{coq}{Category}, \mintinline{coq}{Functor}, etc.) $n$ times to define an $n$-level hierarchy.

\emph{Universe polymorphism}\label{sec:universe-polymorphism-def} is a feature that allows definitions to be quantified over their universes.
While Coq 8.4 supports a restricted flavor of universe polymorhism that allows the universe of a definition to vary as a function of the universes of its arguments, Coq 8.5 and later~\cite{coq-commit-polyproj} support an established kind of more general universe polymorphism~\cite{Harper1991107}, previously implemented only in NuPRL~\cite{nuprl}.
In these versions of Coq, any definitions declared polymorphic are parametric over their universes.

While judicious use of universe polymorphism can reduce code duplication, careless use can lead to tens of thousands of universe variables which then become a performance bottleneck in their own right.%
\footnote{%
  See, for example, the commit message of \href{https://github.com/HoTT/HoTT/commit/a445bc38c172a8f2787afe7d9d193596a0f6e1bd}{a445bc38c172a8f2787afe7d9d193596a0f6e1bd in the HoTT/HoTT library on GitHub}, where moving from Coq 8.5$\beta$2 to 8.5$\beta$3 incurred a 4$\times$ slowdown in the file \texttt{hit/V.v}, entirely due to performance regressions in universe handling, which were later fixed.
  This slowdown is likely the one of \coqbug{4537}.

  Coq actually had an implementation of full universe polymorphism between versions 8.3 and 8.4, implemented in \href{https://github.com/coq/coq/commit/d98dfbcae463f8d699765e2d7004becd7714d6cf}{commit d98dfbcae463f8d699765e2d7004becd7714d6cf} and reverted mere minutes later in \href{https://github.com/coq/coq/commit/60bc3cbe72083f4fa1aa759914936e4fa3d6b42e}{commit 60bc3cbe72083f4fa1aa759914936e4fa3d6b42e}.
  In-person discussion, either with Matthieu himself or with Bob Harper, revealed that Matthieu abandoned this initial attempt after finding that universe polymorphism was too slow, and it was only by implementing the algorithm of \textcite{Harper1991107} that universe polymorphism with typical ambiguity~\cite{Universe2012Shulman,Typical1966Specker,Harper1991107}, where users need not write universe variables explicitly, was able to be implemented in a way that was sufficiently performant.%
}

\subsection{Judgmental η for Record Types}\label{sec:fixes:theory:record-eta}\label{sec:record-eta}
The same commit that introduced universe polymorphism in Coq 8.5 also introduced judgmental $\eta$ conversion for records with primitive projections.~\cite{coq-commit-polyproj}
We have already discussed the advantages of primitive projections in \autoref{sec:primitive-projections}, and we have talked a bit about judgmental $\eta$ in \fullref{sec:no-judgmental-eta} and \fullref{sec:duality-conversion}.

\todo{make sure we've talked about judgmental equality and conversion}
The $\eta$ conversion rule for records says that every term $x$ of record type $T$ is convertible with the constructor of $T$ applied to the projections of $T$ applied to $x$.
For example, if \mintinline{coq}{x} has type \mintinline{coq}{A * B}, then the $\eta$ rule equates \mintinline{coq}{x} with \mintinline{coq}{(fst x, snd x)}.

As discussed in \autoref{sec:duality-conversion}, having records with judgmental $\eta$ conversion allows de-duplicating code that would otherwise have to be duplicated.
\todo{should more be said here?}

\subsection{SProp: The Definitionally Proof Irrelevant Universe}\label{sec:fixes:theory:sprop}\label{sec:sprop}
We discussed in \fullref{sec:term-size} how irrelevant proof arguments tended to cause performance issues just by existing as part of the goal.
While there is as-yet no way to erase these arguments, Coq versions 8.10 and later have the ability to define types as judgmentally irrelevant, paving the way for more aggressive erasure.~\cite{coq-pr-sprop,sprop}
\todo{should more be said here?}

\section{Performance-Enhancing Advancements in Type Theory at Large}\label{sec:fixes:theory}
We come now to discoveries and inventions of the past decade or so which have not yet made it into Coq but which show great promise for significant performance improvements.

\subsection{Higher Inductive Types: Setoids for Free}\label{sec:fixes:theory:HITs}\label{sec:HITs}
\todo{is this sentence redundant?}
\todo{fix the intro to this subsection}
\todo{fix flow}
Recall again that the main case study of \autoref{ch:design} was our implementation of a category theory library.

\subsubsection{Equality}\label{sec:equality}
Equality, which has recently become a very hot topic in type theory~\cite{HoTTBook} and higher category theory~\cite{Leinster2007}, provides another example of a design decision where most usage is independent of the exact implementation details.
Although the question of what it means for objects or morphisms to be equal does not come up much in classical 1-category theory, it is more important when formalizing category theory in a proof assistant, for reasons seemingly unrelated to its importance in higher category theory.
We consider some possible notions of equality.

\paragraph{Setoids}
A setoid~\cite{bishop1967foundations} is a carrier type equipped with an equivalence relation; a map of setoids is a function between the carrier types and a proof that the function respects the equivalence relations of its domain and codomain.
Many authors \cite{copumpkin/categories,MathClasses,megacz-coq-categories,huet2000constructive,benediktahrens/coinductives,ahrens2010categorical,konn/category-agda,Algebra,mattam82-cat,Carvalho1998,wilander2012constructing%
  %,rs-/triangles%
}
choose to use a setoid of morphisms, which allows for the definition of the category of set(oid)s, as well as the category of (small) categories, without assuming functional extensionality, and allows for the definition of categories where the objects are quotient types.
However, there is significant overhead associated with using setoids everywhere, which can lead to slower compile times.
Every type that we talk about needs to come with a relation and a proof that this relation is an equivalence relation.
Every function that we use needs to come with a proof that it sends equivalent elements to equivalent elements.
Even worse, if we need an equivalence relation on the universe of ``types with equivalence relations'', we need to provide a transport function between equivalent types that respects the equivalence relations of those types.

\paragraph{Propositional Equality}
An alternative to setoids is propositional equality, which carries none of the overhead of setoids, but does not allow an easy formulation of quotient types, and requires assuming functional extensionality to construct the category of sets.

\todo{check if we've defined propositional equality before}
Intensional type theories like Coq's have a built-in notion of equality, often called definitional equality or judgmental equality, and denoted as $x \equiv y$.
This notion of equality, which is generally internal to an intensional type theory and therefore cannot be explicitly reasoned about inside of that type theory, is the equality that holds between $\beta\delta\iota\zeta\eta$-convertible terms.

Coq's standard library defines what is called \emph{propositional equality} on top of judgmental equality, denoted $x = y$.
One is allowed to conclude that propositional equality holds between any judgmentally equal terms.

Using propositional equality rather than setoids is convenient because there is already significant machinery made for reasoning about propositional equalities, and there is much less overhead.
However, we ran into significant trouble when attempting to prove that the category of sets has all colimits, which amounts to proving that it is closed under disjoint unions and quotienting; quotient types cannot be encoded without assuming a number of other axioms.

\paragraph{Higher Inductive Types}\label{sec:hit}
The recent emergence of higher inductive types allows the best of both worlds.
 The idea of higher inductive types~\cite{HoTTBook} is to allow inductive types to be equipped with extra proofs of equality between constructors.
They originated as a way to allow homotopy type theorists to construct types with non-trivial higher paths.
A very simple example is the interval type, from which functional extensionality can be proven~\cite{interval-implies-funext}.
The interval type consists of two inhabitants \mintinline{coq}{zero : Interval} and \mintinline{coq}{one : Interval}, and a proof \mintinline{coq}{seg : zero = one}.
In a hypothetical type theory with higher inductive types, the type checker does the work of carrying around an equivalence relation on each type for us, and forbids users from constructing functions that do not respect the equivalence relation of any input type.
For example, we can, hypothetically, prove functional extensionality as follows:
\begin{minted}{coq}
Definition f_equal {A B x y} (f : A → B) : x = y → f x = f y.
Definition functional_extensionality {A B} (f g : A → B)
    : (∀ x, f x = g x) → f = g
  := λ (H : ∀ x, f x = g x)
       ⇒ f_equal (λ (i : Interval) (x : A)
                     ⇒ match i with
                         | zero ⇒ f x
                         | one  ⇒ g x
                         | seg  ⇒ H x
                       end)
                  seg.
\end{minted}
Had we neglected to include the branch for \mintinline{coq}{seg}, the type checker should complain about an incomplete match; the function \mintinline{coq}{λ i : Interval ⇒ match i with zero ⇒ true | one ⇒ false end} of type \mintinline{coq}{Interval → bool} should not typecheck for this reason.

The key insight is that most types do not need any special equivalence relation, and, moreover, if we are not explicitly dealing with a type with a special equivalence relation, then it is impossible (by parametricity) to fail to respect the equivalence relation.
Said another way, the only way to construct a function that might fail to respect the equivalence relation would be by some eliminator like pattern matching, so all we have to do is guarantee that direct invocations of the eliminator result in functions that respect the equivalence relation.

As with the choice involved in defining categories, using propositional equality with higher inductive types rather than setoids derives many of its benefits from not having to deal with all of the overhead of custom equivalence relations in constructions that do not need them.
In this case, we avoid the overhead by making the type checker or the metatheory deal with the parts we usually do not care about.
Most of our definitions do not need custom equivalence relations, so the overhead of using setoids would be very large for very little gain.

\subsection{Univalence and Isomorphism Transport}\label{sec:fixes:theory:univalence}\label{sec:univalence}
When considering higher inductive types, the question ``when are two types equivalent?'' arises naturally.
The standard answer in the past has been ``when they are syntactically equal''.
The result of this is that two inductive types that are defined in the same way, but with different names, will not be equal.
Voevodsky's univalence principle gives a different answer: two types are equal when they are isomorphic.
This principle, encoded formally as the \emph{univalence axiom}, allows reasoning about ismorphic types as easily as if they were equal.

\Citeauthor*{Equivalences2018Tabareau} built a framework on top of the insights of univalence, combined with parametricity~\cite{reynolds1983types,wadler1989theorems}, for automatically porting definitions and theorems to equiavlent types.~\cite{Equivalences2018Tabareau,Tabareau2019TheMO}

What is the application to performance?
As we saw, for example, in \fullref{sec:rewriting-more:AST:choices}, the choice of representation of a datatype can have drastic consequences on how easy it is to encode algorithms and write correctness proofs.
These design choices can also be intricately entwined with both the compile-time and run-time performance characteristics of the code.
One central message of both \autoref{ch:rewriting-more} and \autoref{ch:design} is that picking the right API really matters when writing code with dependent types.
The promise of univalence, still in its infancy, is that we could pick the right API for each algorithmic chunk, prove the APIs isomorphic, and use some version of univalence to compose the APIs and reason about the algorithms as easily as if we had used the same interface everywhere.

\todo{should I talk about the difference between isomorphism and equivalence?}
\todo{what more should be said here?}

\subsection{Cubical Type Theory}\label{sec:fixes:theory:cubical}\label{sec:cubical}
One important detail we elided in the previous subsections is the question of computation.
Higher inductive types and univalence are much less useful if they are opaque to the type checker.
The proof of function extensionality, for example, relies on the elimination rule for the interval having a judgmental computation rule.%
\footnote{%
  We leave it as a fun exercise for the advanced reader to figure out why the Church encoding of the interval, where \mintinline{coq}{Interval := ∀ P (zero : P) (one : P) (seg : zero = one), P}, does not yield a proof a functional extensionality.%
}

\todo{define point vs path constructors}
Higher inductive types whose eliminators compute on the point constructors can be hacked into dependently-typed proof assistants by adding inconsistent axioms and then hiding them behind opaque APIs so that inconsistency cannot be proven.~\cite{Running2011Licata,Bertot2013}
This is unsatisfactory, however, on two counts:
\begin{enumerate}
\item
  The eliminators do not compute on path constructors.
  For example, the interval eliminator would compute on \mintinline{coq}{zero} and \mintinline{coq}{one}, but not on \mintinline{coq}{seg}.
\item
  Adding these axioms compromises the trust story.
\end{enumerate}

Cubical type theory is the solution to both of these problems, for both higher inductive types and univalence.~\cite{Cubical2016Cohen}
Unlike most other type theories, computation in cubical type theory is implemented by appealing to the category thoretic model, and the insights that allow such computation are slowly making their way into more mainstream dependently-typed proof assistants.~\cite{Cubical2019Vezzosi}

\begin{comment}
\begin{subappendices}
    \section{Fragments from the category theory paper}
    For reasons that we present in the course of the paper, we were unsatisfied with the feature set of released Coq version 8.4.  We wound up adopting the Coq version under development by homotopy type theorists~\cite{HoTT/coq}, making critical use of its stronger universe polymorphism (\autoref{sec:category-of-categories}) and higher inductive types (\autoref{sec:hit}).  We hope that our account here provides useful data points for proof assistant designers about which features can have serious impact on proving convenience or performance in very abstract developments.  The two features we mentioned earlier in the paragraph can simplify the Coq user experience dramatically, while a number of other features, at various stages of conception or implementation by Coq team members, can make proving much easier or improve proof script performance by orders of magnitude, generally by reducing term size (\autoref{sec:term-size}): primitive record projections (\autoref{sec:prim-record-proj}), internalized proof irrelevance for equalities (\autoref{sec:equality-reflection}), and $\eta$ rules for records (\autoref{sec:no-judgmental-eta}) and equality proofs (\autoref{sec:compute-match}).

    \hrule


    \section{transcript bits from Adam}
    Ah a sort of like preconclusion chapter that's like let's now look at how cock is evolved performance wise over the years like here places that we've actually improved performance and this will be one that draws a bunch of other examples from the category theory paper, like look universe polymorphism is a thing that was implemented and helps here and like sort of like presenting a bunch of little things.
\section{transcript bits from Rajee}
So those are the two main sections the thesis. And then there's another section of other small. Miscellaneous things that come up better like performance bottlenecks. Through like can or performance concerns, let's say these are things like decide design decisions that can have quadratic impacts. Um, Decisions about like what parts of cop to use for what and like why some bits might be more or less slow than others.

Yeah, that's that's that section and then I think I'm going to have seconds last section. Be a sort of retrospective of like places where cocks performance has gotten better in the past like decade or so of like I started with a bunch of ways that solving performance issues improved systems is heard but here are some successes and things where like we've managed to improve things and you can actually like, Leverage this for faster performance.


\todo{this chapter}
\end{subappendices}
\end{comment}
